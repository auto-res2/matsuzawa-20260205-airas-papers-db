{
  "research_topic": "Automated prompt optimization through iterative refinement and performance feedback",
  "queries": [
    "prompt optimization",
    "iterative prompt refinement",
    "reinforcement learning prompts"
  ],
  "research_study_list": [
    {
      "title": "Pareto Prompt Optimization",
      "full_text": "Published as a conference paper at ICLR 2024 FINE -TUNING ENHANCES EXISTING MECHANISMS : A CASE STUDY ON ENTITY TRACKING Nikhil Prakash1∗ Tamar Rott Shaham2 Tal Haklay3 Yonatan Belinkov3 David Bau1 1Northeastern University 2MIT CSAIL 3Technion – IIT ABSTRACT Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models’ performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language compre- hension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit im- plements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality: Entity track- ing is performed by tracking the position of the correct entity in both the original model and its fine-tuned versions. (iii) Performance boost in the fine-tuned models is primarily attributed to its improved ability to handle the augmented positional information. To uncover these findings, we employ: Patch Patching, DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than funda- mentally alters, the mechanistic operation of the model. 1 I NTRODUCTION The capabilities of models fine-tuned on general reasoning tasks have hinted at nontrivial mecha- nisms underlying task learning. While it has been widely understood that fine-tuning a pretrained model on a specific task can improve task performance on that same task (Howard & Ruder, 2018), studies of fine-tuning on generalized domains (Gururangan et al., 2020) have suggested that fine- tuning on generic problems can improve specific task performance as well. In particular, fine-tuning on coding has been observed to lead to a range of improved capabilities in a model (Madaan et al., 2022; Kim & Schuster, 2023). In this paper, we study the mechanisms underlying one specific ca- pability which is dramatically improved by fine-tuning a standard large language model (LLM) on the generic task of arithmetic-problem solving: the ability of a model to perform in-context entity tracking, where the model can infer properties associated with an entity previously defined in the in- put context. For example, if we say “The apple is in Box C,” a model will later be able to infer “Box C contains the apple”. The ability to track and maintain information associated with various entities within the context is fundamental for complex reasoning (Karttunen, 1976; Heim, 1983; Nieuwland & Van Berkum, 2006; Kamp et al., 2010), thus making entity tracking an intriguing case study. We ask several specific questions about the mechanisms underlying the emergence of improved entity tracking in an arithmetic-tuned model. First, we ask: can the performance gap be explained because the fine-tuned models contain a different circuit for performing entity tracking? Or does it contain the same entity-tracking circuit as the base model? To answer this question, we explicitly identify the entity-tracking circuit in the base Llama-7B model, using the path-patching method from Elhage et al. (2021); Wang et al. (2022), consisting of a sparse set of 72 attention heads in four ∗Correspondence to prakash.nik@northeastern.edu tamarott@mit.edu, tal.ha@campus.technion.ac.il, belinkov@technion.ac.il, d.bau@northeastern.edu 1 arXiv:2402.14811v1  [cs.CL]  22 Feb 2024Published as a conference paper at ICLR 2024 groups, each group active at a specific token location (Fig. 1); acting in isolation, this sparse circuit can reproduce the entire entity-tracking capability of the base model. Then, without altering the graph, we ask if exactly the same set of components constitutes the entity-tracking circuit in the fine- tuned models. We observe that the identical circuit exists in the fine-tuned models, which alone can restore atleast 88% of the overall performance of the entire fine-tuned model. However, achieving the full performance of the fine-tuned models requires incorporation of additional components. Next, we ask: how does this common circuit work? Can we discern the role of each group of at- tention heads? To answer these questions, we use Desiderata-based Component Masking (DCM; Davies et al., 2023), a method for automatically identifying model components responsible for per- forming a specific semantic subtask. That is done by specifying a set of “desiderata,” each consisting of pairs of entity tracking tasks, a base task, and a carefully designed alternation of it. The alternation is done on a specific semantic part of the task (e.g. the entity name) with a known target output (e.g. switch the entity property). Using these sets of tasks, we automatically identify groups of model components that have causal effects that correspond to specific semantics. For example, we could identify whether circuit components are transporting entity name information ( e.g. “Box C” in the previous example), or its associated property (e.g. “contains the apple”), or some other scheme. We test these hypotheses and surprisingly find a third scheme that is used: entity tracking is performed by identifying and transporting theposition of the queried entity in the context, with multiple groups of heads collaborating to pass the position downstream. Furthermore, this scheme and specific role of each group of heads remain the same between models, confirming that fine-tuning preserves the overall mechanism for performing the entity tracking task. The mechanism invariance is observed in both low-rank adaptations (LoRA) (Hu et al., 2021) and fully fine-tuned models. Third, we ask: if the mechanism remains the same after fine-tuning, can we attribute the perfor- mance improvement to a specific step in the mechanism? To study this question, we introduce cross-model activation-patching (CMAP), which allows us to localize the specific sub-mechanism being improved by fine-tuning. Cross-model activation patching shows evidence that (i) the inter- nal representation of both the original model and the fine-tuned models is similar enough so that patching components of the entity-tracking circuit from the fine-tuned models to Llama-7B leads to enhanced performance. (ii) In fine-tuned models the entity tracking circuit has augmented positional information for attending to the correct object and hence fetching its enhanced representation. Taken together, our findings indicate that fine-tuning enhances the existing mechanism of the orig- inal model rather than causing a fundamental shift. Notably, the entity tracking circuit remains consistent across both base and fine-tuned models and maintains the same functionality, with the performance gap mainly attributed to an improved core sub-mechanism. The code, data and fully fine-tuned model can be accessed at https://finetuning.baulab.info. 2 R ELATED WORK Mechanistic interpretabilityaims to elucidate neural network behaviors by comprehending the un- derlying algorithms implemented by models (Olah et al., 2017; Elhage et al., 2022). Recently, no- table progress has been made in identifying circuits performing various tasks within models (Nanda et al., 2023; Wang et al., 2022; Chughtai et al., 2023; Olah et al., 2020; Lieberum et al., 2023), and in methods enabling circuit discoveries (Davies et al., 2023; Conmy et al., 2024; Wu et al., 2024; Meng et al., 2022; Chan et al., 2022). We aim to harness mechanistic interpretability to uncover an explanation for the performance enhancement observed in fine-tuned models. Specifically, our exploration focuses on whether the performance gap results from varying circuit implementations of the same task and if not, we aim to identify the enhanced mechanism within the circuit. Fine-tuning on generic domains such as code, mathematics, and instructions has been shown to enhance language models performance, both in the context of general fine-tuning and when tailored for specific tasks (Christiano et al., 2017; Gururangan et al., 2020; Madaan et al., 2022; Ouyang et al., 2022; Chung et al., 2022; Taori et al., 2023; Chiang et al., 2023; Liu & Low, 2023; Kim & Schuster, 2023; Zheng et al., 2023; Touvron et al., 2023b; Bommarito II & Katz, 2022). Several attempts to understand the effect of such fine-tuning on model operations reveal interesting charac- teristics; instruction fine-tuning can destroy knowledge for OOD input (Kumar et al., 2022), shift the model’s weight to a task-depended sub-domain (Gueta et al., 2023; Ilharco et al., 2022), and enhance existing capabilities rather than introduce new knowledge (Zhou et al., 2023). Fine-tuned models 2Published as a conference paper at ICLR 2024 were shown to have a localized set of components that perform the task (Panigrahi et al., 2023), and modified underlying embedding spaces and attention patterns (Kovaleva et al., 2019; Merchant et al., 2020; Wu et al., 2020; Zhou & Srikumar, 2022). Concurrent to our research, (Jain et al., 2023) delved into the impact of fine-tuning on LLMs from a mechanistic perspective. Although their main finding, suggesting that fine-tuning rarely alters pretrained capabilities, resonates with our result of enhancing existing mechanisms through fine-tuning, their study involved controlled experiments utilizing transformer models created using the tracr library (Lindner et al., 2024). In contrast, our experiments focus on established LLMs such as Llama-7B and their fine-tuned variants, specifically in the context of entity tracking tasks, which we believe better represent real-world language tasks. Entity trackingis a fundamental cognitive ability that enables AI models to recognize and trace entities, including objects, individuals, or concepts, within a given context (Karttunen, 1976; Heim, 1983; Nieuwland & Van Berkum, 2006; Kamp et al., 2010; Marcus, 2018). In the large language models realm, models such as GPT-2 (Radford et al., 2019) have shown some related abilities, such as predicting the next moves in board games (Toshniwal et al., 2022; Li et al., 2022). Utilizing a probing technique, Li et al. (2021) shows that entity state can be recovered from internal activa- tions in BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020). Lately, Kim & Schuster (2023) presented a dataset of entity tracking tasks, showing that models fine-tuned on code data perform entity tracking more accurately. We use entity tracking as a case study to explore how fine-tuning changes the model’s functionality to achieve enhanced performance. Complimentary of our work, (Feng & Steinhardt, 2023) investigated how LLMs keep track of various properties associated with an entity. Their findings indicated that models generate binding ID vectors corresponding to entities and attributes. We find it intriguing to further investigate the interaction between these binding ID vectors and the entity tracking circuit we have identified. 3 E XPERIMENTAL SETUP To explore the internal mechanism that enables entity tracking we adapt the dataset presented in Kim & Schuster (2023), aimed at evaluating the ability of a language model to track state changes of discourse entities. The dataset contains English sentences describing different settings of objects located in different boxes, with different labels, and the task is to discover what is inside a specific box. For example, when the model is presented with “The apple is in box F , the computer is in Box Q, the document is in Box X... Box F contains the”, it should predict the next token as “apple” (see additional task examples in Fig. 2 and in the Appendix J). Each of our tasks involves 7 boxes and no operations (i.e. contents of the boxes are not altered), each box is labeled with a random alphabetic letter. For convenience, we only use single-token objects. In contrast to Kim & Schuster (2023), we reorder the structure of the context segment (where each box information is defined) such that the object is mentioned before the box label ( “The apple is in box F” instead of “Box F contains the apple”). This is to ensure that the context segment and the query segment (where the box is queried) have different structures, and the model needs to infer the box information rather than locating the longest identical context segment in the text. We study four language models: LLaMA-7B (Touvron et al., 2023a), and three fine-tuned versions of it: Vicuna-7B (Chiang et al., 2023) that was fine-tuned on user-shared conversations collected from ShareGPT, Goat-7B (Liu & Low, 2023), fine-tuned on synthetically generated arithmetic ex- pressions using LoRA (Hu et al., 2021), and FLoat-7B (Fine-tuned Llama on arithmetic tasks), fine-tuned on the same data as Goat-7B without LoRA. All these models achieve high performance on the entity tracking task, as shown in Table 1 (first column, evaluation was done over 500 tasks). Although Goat-7B and FLoat-7B were fine-tuned on arithmetic tasks, their ability to perform entity tracking is significantly improved compared to the base Llama-7B model. This aligns with Kim & Schuster (2023), who also found that models trained on structured data are better at performing entity tracking. We seek a mechanistic explanation for this performance gap. 4 I S THE SAME CIRCUIT PRESENT AFTER FINE -TUNING ? In this section we ask whether the circuit that enables entity tracking changes across the different fine-tuned models. Entity tracking might be solved by the same circuit in all four models, or each model may implement a different circuit in the light of fine-tuning data. To answer this, we start with 3Published as a conference paper at ICLR 2024 … Group A (L21 H3)Group B (L14 H27)Group C (L10 H3)Group D (L8 H21) Path-Patching Attention flow QV Attention strength V Attention headsin circuit C ABCD apple C CCthe Figure 1: Entity Tracking circuit in Llama-7B (Cir). The circuit is composed of 4 groups of heads (A,B,C,D) located at the last token (A,B), query label (C), and previous query label (D) token positions. Each group is illustrated by a prominent head in that group. identifying the entity tracking circuit in Llama-7B, and then evaluate the same circuit components in Vicuna-7B, Goat-7B, and FLoat-7B. 4.1 C IRCUIT DISCOVERY IN LLAMA -7B The entity-tracking circuit will be a subgraph of the transformer computational graph, where each node is an attention head at a specific token position, so the whole circuit is a setCir = {(a, t)}. For example, Fig.1 illustrates the entity tracking circuit in Llama-7B consisting of four groups of nodes, each represented by a prominent head; e.g. Group A is characterized with (aL21H3, tlast). Given the nature of the entity tracking task, we are primarily interested in how and what kinds of information are transported between tokens rather than how that information is transformed. We therefore focus our analysis on the attention heads of the circuit, and we consider all MLP layers to be involved in the computation of the final output. To identify the components of the entity tracking circuit, we use Path Patching (Wang et al., 2022; Goldowsky-Dill et al., 2023), using the synthetic box tracking dataset with 300 examples. For each of the original entity tracking tasks xorg we define a corresponding noise task xnoise with a randomized query, box labels, and objects. Then we evaluate each candidate pair of nodes with a score defined as follows. We denote porg as the probability of the correct token predicted by the original run, and we let ppatch be the probability assigned to the correct token when patching a specific path from one specific node to another using activations from the noisy run. The patching score for the candidate pair is defined as (ppatch −porg)/porg. At each iteration we add the paths with the lowest (most negative) scores. In the first step, we identify the group of heads that directly influence the final logit with the lowest patching scores. These attention heads attend mainly to the correct object token: in other words, they look directly at the answer, e.g., ‘apple’ that should be predicted (Fig. 1). We refer to this set of heads as Group A. We then iteratively identify groups of heads that have high direct effects on each other using the path patching score; this leads us to three additional groups of attention heads, (B, C, and D), active at the last, query label, and previous query label token positions, as shown in Fig. 1. We mark the paths between groups with either Q or V to indicate whether the heads of the previous group affect the query or the value vector calculation of the following group correspondingly. Overall, the circuit Cir consists of four groups of heads. Group D at the previous query label token collects information of its segment and passes it on to the heads in Group C at the query box label position via V-composition. The output of Group C is transported to the last token residual stream via the heads of Group B through V-composition, which is used by the heads of Group A via Q- composition to attend to the correct object token. The validity of this information flow channel is further substantiated by the results obtained from the attention knockout technique introduced in Geva et al. (2023), as demonstrated in Appendix A. Interestingly, this circuit suggests that correct object information is fetched directly from its token residual stream, instead of getting it from the query label token residual stream. This result is consistent with the findings of Lieberum et al. 4Published as a conference paper at ICLR 2024 Table 1: Entity-tracking circuitfound in Llama-7B, evaluated on Llama-7B, Vicuna-7B, Goat-7B, and FLoat-7B, without any adjustment of the circuit graph. The circuit achieves high accuracy and faithfulness scores in all models (chance accuracy is 0.14). Accuracy Model Finetuned? Full-Model Circuit Random Circuit Faithfulness Llama-7B – 0.66 0.66 0.00 1.00 Vicuna-7B User conversations 0.67 0.65 0.00 0.97 Goat-7B Arithmetic tasks (LoRA) 0.82 0.73 0.01 0.89 FLoat-7B Arithmetic tasks (w/o LoRA) 0.82 0.72 0.01 0.88 (2023), reporting that heads affecting final logit attend to the correct label, instead of content tokens, to identify the label corresponding to the already-determined correct answer. 4.2 C IRCUIT EVALUATION Although path patching ranks a head based on its relevance via the patching score, it does not provide a clear threshold for the number of heads that should be included in the circuit. In our setting, we include a total of 90 heads in the circuit discovered with path patching (50,10,25,5 heads in Groups A,B,C,D respectively). However, there might be redundancy among the heads in each group. Hence, inspired by Wang et al. (2022), we use a minimality criterion to prune the initial circuit. We then measure the performance of the minimal circuit compared with that of the entire model using the faithfulness metric. We also evaluate it with the completeness metric in the Appendix C. For both criteria, we define the performance metric F to be the accuracy score averaged over 500 examples. That is, for the model M and its circuit Cir, F(M), F(Cir) represent the accuracy of the model and circuit respectively. Specifically, we compute F(Cir) by first mean ablating of all the heads in the model that are not involved in Cir. Minimality. The minimality criterion helps identify heads that do not significantly contribute to the circuit performance found with path patching (90 heads in total). For each head, v ∈ Cir, and a subset of heads K, we measure the relative performance difference of Cir when the heads in K are knockout, with and without v from the circuit. That is, we define the contribution of each head v to Cir as (F(Cir \\ K) − F(Cir \\ (K ∪ {v})))/F(Cir \\ (K ∪ {v})). We filter out the heads with a score lower than 1% (e.g. contribute less than 1% to the performance of the circuit in the absence of the functionality defined by subsetK). Unlike Wang et al. (2022), we use a greedy approach to form the subset for each head in Cir (check Appendix B for more details), and only consider heads that positively contribute to the model performance (e.g. contribute to performing of the task). Using this criterion we prune 20% of the heads of the initial circuit, hence reducing the total number of heads to 72 (see Appendix D for exact distribution and heads in each group). Faithfulness. We next measure how good is the identified circuit compared with the entire model. We use the criterion of faithfulness, which is defined as the percentage of model performance that can be recovered with the circuit, i.e. F(Cir)/F(M). As shown in Table 1, Llama-7B has a faith- fulness score of 1.0, suggesting identified circuit can recover entire model performance. 4.3 C IRCUIT GENERALIZATION ACROSS FINE -TUNED MODELS As described in section 3, fine-tuned models perform the entity tracking task better than the base Llama-7B. Better performance could be attributed to a superior circuit in the fine-tuned models. Hence, in this subsection, we ask the question of whether the fine-tuned models use a different or the same circuit, i.e. with exactly the same group of heads, to perform the entity tracking task. To answer this, we evaluate the circuit identified in Llama-7B, on the fine-tuned models using the faithfulness criterion. Surprisingly, we find that fine-tuned models have good faithfulness scores for the circuit identified in Llama-7B (without any additional optimization or adaptation) as shown in Table 1. Specifically, Vicuna-7B has almost a perfect faithfulness score of 0.97, while Goat-7B and FLoat-7B exhibit slightly lower scores of 0.89 and 0.88, respectively. As a baseline, we calculate 5Published as a conference paper at ICLR 2024 the average accuracy of 10 random circuits with the same total and per-position number of heads; random circuits have virtually zero accuracy. This suggests that Vicuna-7B utilizes roughly the same circuit as that of Llama-7B to perform entity tracking. Whereas, in Goat-7B and FLoat-7B the same circuit is present, but achieving the complete performance of the fine-tuned models requires the incorporation of additional components. To further investigate the overlap between the circuits of fine-tuned models and the base model, we identify the entity tracking circuits of the Goat-7B and FLoat-7B models, using the same pro- cedure as in Section 4.1 (Refer to Appendix E and Appendix F). We found that these circuits are significantly larger, consisting of 175 attenton heads and approximately forming a superset of the Llama-7B circuit (Refer to Appendix E4 and Appendix F4 for more details). This finding suggests that fine-tuning is inserting additional components to the circuitry that performs entity tracking. 5 I S CIRCUIT FUNCTIONALITY THE SAME AFTER FINE -TUNING ? While the same circuit is primarily responsible for performing entity tracking in both the base and fine-tuned models, the specific functionality of different parts of the circuit remain unknown. In other words, to fully comprehend the underlying mechanism through which these models execute the task, it is crucial to understand the functionalities of the circuit components. There are two hypothesis pertaining to circuit functionality in base and fine-tuned models: (i) The same circuit exists in all four models, but the functionalities it implements may vary, accounting for the per- formance difference. (ii) The circuits of all models implement the same mechanism, but with an enhanced functionality in fine-tuned models. To investigate these hypotheses, we use the automatic Desiderata-based Component Masking (DCM) method, introduced in Davies et al. (2023), for iden- tifying groups of model components responsible for specific functionalities. First, we use DCM on the groups of heads in the minimal circuit of Llama-7B to identify subsets of heads with specific functionalities, (e.g. moving positional information or object values). Then, for each model we apply activation patching on those subsets of heads, to quantify their efficacy on various functionalities. 5.1 D ESIDERATA -BASED COMPONENT MASKING The DCM method involves using desiderata for identifying model components responsible for spe- cific functionality. Each desideratum consists of numerous 3-tuple ( original, alternative, target ), where original is an original entity tracking task, alternate is a carefully designed counterfactual task, and target is the desired output, as shown in Fig. 2. If a set of components encodes information regarding the desired semantics, then patching activations from the alternate run into the original run should alter the model output to target. Refer to Davies et al. (2023) for more details. DCM use gradient descent optimization procedures; For each desideratum, we train a sparse binary mask over potential model components to identify the ones that when patched from counterfactual to original run maximize the target value. Hence, compared to brute-force activation patching, DCM is much more efficient. More importantly, it overcomes a major drawback of activation patching, i.e. it can locate the subset of model components that work together to produce the final output. 5.2 C IRCUIT FUNCTIONALITY IN LLAMA -7B To untangle the functionality of groups of heads in the Llama-7B circuit, we define three desiderata, as shown in Fig. 2: (i) Object desideratum, which is used to identify model components encoding the value of correct object, (ii) Label desideratum, used to identify model components encoding the box label value information, and (iii) Position desideratum which can be used to identify model components encoding the positional information of the correct object. Please refer to Fig. 2 caption and Appendix G for additional details about each. We apply DCM to identify the subset of heads that encode these functionalities in Llama-7B circuit. For each group of heads, we train three binary masks, one for each desideratum, that identify the subset of heads encoding specific functionality (check Appendix H for more details). The results are shown in Table A2. All Group A heads encode the value of correct object in their output. While most of the heads in Group B (71.43%) and C (70.0%) encode positional information of the correct object in their output. The heads of Group D are not profoundly involved in any of the three functionalities. 6Published as a conference paper at ICLR 2024 Desiderata OriginalAlternate Target(a) Object cupThedocumentisinBoxX,thepotisinBoxY,thecrossisinBoxZ,…BoxXcontainsthe____ThebookisinBoxA,the cup is in Box B,thecomputerisinBoxC,…Box Bcontainsthe____(b) Label potThedocumentisinBoxX,the pot is in Box Y,thecrossisinBoxZ,…BoxXcontainsthe____ThebookisinBoxA,thecupisinBoxB,thecomputerisinBoxC,…Box Ycontainsthe____ (c) Position crossThedocumentisinBoxX,thepotisinBoxY,thecrossisinBoxZ,…BoxXcontainsthe____ThebookisinBoxA,thecupisinBoxB,thecomputerisinBoxC,…Box Ccontainsthe____ Figure 2: Desiderata for identifying circuit functionality.We define different desiderata (sets of an original sentence and a carefully designed counterfactual alternation of it with a known target output) to evaluate various hypotheses regarding the functionality of a subset of heads within the circuit. (a) when patching heads encoding information about the correct object, the object information from the alternate run (e.g. “cup”) is implanted into the original run. (b) heads sensitive to the query box label will be affected by the alternate query label ( e.g. “Box Y”) causing the output to be the object in that box. (c) Position encoding heads represent positional information of the correct object token. Therefore patching these, the model outputs the object at the location of the correct object token in the alternate sequence (e.g. “cross” which is located at the same position as “computer”.) We next apply activation patching on this subset of heads, using additional ( N = 500) samples from the three desiderata, and compute the accuracy with respect to the target value. In order to incorporate randomness in the generated data, we repeated the evaluation ten times with different samples of the test set and report the mean accuracy and standard deviation. The results are shown in Fig. 3, indicating that heads in Group A are primarily responsible for fetching the value information of the correct object. Hence, we refer to this set of heads as Value Fetcher. Heads in Group B and C are mainly responsible for detecting and transmitting the positional information of the correct object and are therefore referred to as Position Detector and Position Transmitter. Since we were unable to establish the functionality of heads in Group D, we used their attention pattern to annotate them. These heads primarily attend to tokens in their own segment, as shown in Fig. 1, hence we refer to them as Structure Reader heads. Overall, the circuit generates correct output by first detecting the positional information of the correct object with Position Detector heads, using the information collected by the Structure Reader heads. The positional information is transmitted to the Value Fetcher heads, by the Position Transmitter heads, which resolves this information to locate the correct object location and fetches its value, to be generated as final output. This indicates that the model is primarily using positional information to keep track of in-context entities. Additionally, we have some early evidence that the model is encoding positional information relative to the context segment; see Appendix J for more details. 5.3 C IRCUIT FUNCTIONALITY IN FINE -TUNED MODELS Now that we have identified the functionality of the group of heads in the Llama-7B circuit, we can examine whether this circuit, also present in the fine-tuned models, implements the same or different functionalities across different models. To assess this, we employ activation patching on the same subset of heads of Vicuna-7B, Goat-7B, and FLoat-7B that are involved in a specific functionality. As shown in Fig. 3, the functionality of the subset of heads remains the same across fine-tuned models. Position Detector and Position Transmitter heads of Vicuna-7B and Goat-7B achieve per- formance similar to that of Llama-7B, though they demonstrate enhanced accuracy in FLoat-7B. The Value Fetcher heads in fine-tuned models consistently show an improved capability to retrieve the correct object value, e.g. Goat-7B can achieve a performance improvement of 20% compared to Llama-7B. Furthermore, we found that both Goat-7B and FLoat-7B circuits implement precisely the same functionality within each group, as depicted in Fig. A8 and Fig. A9. These findings suggest that neither additional functionality nor a shift in functionality is introduced in fine-tuned models. Overall, the results confirm the hypothesis that circuits in fine-tuned models implement the same functionality with the insight that the Value Fetcher in fine-tuned models has a better ability to resolve positional information for fetching the correct object value information. 7Published as a conference paper at ICLR 2024 Figure 3: Circuit Functionality in Llama-7B, Vicuna-7B, Goat-7B, and FLoat-7B.We use DCM to uncover functionality of each subgroup of Llama-7B circuit. Group A (pink) is mainly sensitive to value desideratum, while groups B, C (purple, turquoise) are responsible for positional information. We find group D insensitive to each of the three desideratum. Error bars indicate standard deviation. Combining the results from previous experiments indicates that not only the circuit from the base model is present in the fine-tuned models, but also its functionality remains the same. Further, addi- tional components in fine-tuned models’ circuits implement the exact same functionality. Hence, we conclude that fine-tuned models implement the same mechanism to perform entity tracking task as the base model. However, the increased performance of fine-tuned models suggests that fine-tuning enhances that existing mechanism. This implies that unraveling the mechanism through which a fine-tuned model accomplishes a task provides valuable insights into how the same task would be executed in the base model. This insight is particularly crucial for tasks that the base model struggles to perform well, making unraveling its mechanism more challenging. 6 W HY DO GOAT-7B AND FLOAT-7B PERFORM BETTER ? In the previous sections, we established that fine-tuned models employ the same mechanism as the base model to perform the entity tracking task, albeit with additional components. In this section, we aim to attribute performance improvement to a specific step in the mechanism. 6.1 C ROSS -MODEL ACTIVATION PATCHING In order to be able to attribute the performance improvement to a specific step in the mechanism, we introduce Cross-Model Activation Patching (CMAP). Unlike naive activation patching, which involves patching activations of the same model on different inputs, CMAP requires patching acti- vations of the same components of different models on the same input, as shown in Fig 4. We use CMAP to patch the output of the subset of heads responsible for dominant functionality in each group of Goat-7B and FLoat-7B circuits. Since we do not fully understand the functionality of Structure Reader heads, we patch all the heads in this group. More specifically, we patch the output of heads in the Goat-7B circuit from the Goat-7B to Llama-7B model, to identify which step in the Goat-7B model mechanism leads to performance improvement. Similarly, we perform the same patching process for heads in the FLoat-7B circuit. 8Published as a conference paper at ICLR 2024 Q K V Q K V Q K V Q K V Q K V Q K V Q K V Q K V !\"#$%&#'()! *() $+,() -\".() %//0,() *() $+,() &'() &#() -\".() 1() !\"#$%&#'()! *() $+,() -\".() %//0,() *() $+,() &'() &#() -\".() 1() !\"#$\"#% $&#'()*+ !\"#$#%&' ()#*%&' Figure 4: Why do Goat-7B and FLoat-7B perform better?We use CMAP to patch activations of the Goat-7B and FLoat-7B circuit components, from Goat-7B and FLoat-7B to Llama-7B model respectively, to attribute the performance improvement to a specific sub-mechanism used to perform entity tracking tasks. We patch the output of the subset of heads in each group that are involved in the primary functionality. We find that patching Value Fetcher heads can solely improve the perfor- mance of Llama-7B to that of Goat-7B and FLoat-7B. Additionally, we also observe a significant performance boost when the output of Position Transmitter heads is patched. 6.2 R ESULTS As shown in Fig. 4, patching the output of the Position Transmitter and Value Fetcher heads from fine-tuned models to Llama-7B improves the performance of Llama-7B beyond its default perfor- mance (red dashed line). It is interesting to observe that the activations of fine-tuned models are compatible with base model, even though they could have been using completely different sub- spaces and/or norms to encode information. We observe the maximal increase in performance when the Value Fetcher heads are patched, recovering the full fine-tuned models’ performance (green dashed line). This indicates that the output of these heads in fine-tuned models encodes an enhanced representation of the correct object, corroborating results from Section 5. Additionally, we also see a substantial increase in performance when the outputs of the Position Transmitter heads are patched, suggesting that fine-tuned models are also transmitting augmented positional information. We spec- ulate that the enhanced encoding in fine-tuned models stem from both additional components in their circuit and the improved ability to encode vital information of shared components with Llama-7B. 7 D ISCUSSION AND CONCLUSION In this work, we investigated the effect of fine-tuning on circuit-level mechanisms in LLMs. We discovered that not only does the circuit from the base model persist in the fine-tuned models, but its functionality also remains unchanged. Further, the circuits in fine-tuned models, augmented with additional components, precisely employ the same functionality. We have introduced Cross-Model Activation Patching (CMAP) to compare mechanisms in two different models, revealing how a fine- tuned model enhances the existing mechanism in a base model to obtain a better performance on entity tracking. In our work we have studied the interaction between a single task and three fine- tuned models. Understanding whether such mechanism invariance is typical will require experience with further tasks on more models. Nevertheless, the methods presented in the paper are generic and could be applied to a variety of settings. Future work may study the training dynamics during the fine-tuning process, to pinpoint exactly when and how the circuit enhancement occurs. 9Published as a conference paper at ICLR 2024 8 E THICS STATEMENT This work investigating the impact of fine-tuning on large language models suggests that fine-tuning primarily enhances existing mechanisms present in the base model. This highlights the importance of training safe and unbiased base models that are openly available. If such models are developed responsibly, then the risks of fine-tuning introducing new biases or dangerous behaviors can be greatly reduced. Hence, indicating that careful stewardship is required in the foundational phases of model development to promote beneficial applications as the capabilities of AI systems advance. 9 A CKNOWLEDGEMENT We would like to thank Open Philanthropy for their generous support through an AI Alignment grant (NP, TH, YB, DB). TH and YB also received support from the Israel Science Foundation (grant No. 448/20) and an Azrieli Foundation Early Career Faculty Fellowship. TRS received partial support from the Zuckerman STEM Leadership Program and the Viterbi Fellowship. We would also like to thank the Center for AI Safety (CAIS) for making computing resources available for this research. REFERENCES Michael Bommarito II and Daniel Martin Katz. Gpt takes the bar exam. arXiv preprint arXiv:2212.14402, 2022. Lawrence Chan, Adri `a Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing: A method for rigorously testing interpretability hypothe- ses. https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/ causal-scrubbing-a-method-for-rigorously-testing , 2022. Accessed: February 14, 2023. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/ . Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing sys- tems, 30, 2017. Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. arXiv preprint arXiv:2302.03025, 2023. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod- els. arXiv preprint arXiv:2210.11416, 2022. Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri `a Garriga- Alonso. Towards automated circuit discovery for mechanistic interpretability.Advances in Neural Information Processing Systems, 36, 2024. Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, and David Bau. Discovering variable binding circuitry with desiderata, 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Com- putational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. 10Published as a conference paper at ICLR 2024 N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposi- tion. arXiv preprint arXiv:2209.10652, 2022. Jiahai Feng and Jacob Steinhardt. How do language models bind entities in context?, 2023. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro- cessing, pp. 12216–12235, Singapore, December 2023. Association for Computational Linguis- tics. doi: 10.18653/v1/2023.emnlp-main.751. URL https://aclanthology.org/2023. emnlp-main.751. Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model behavior with path patching, 2023. Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Knowledge is a region in weight space for fine-tuned language models. arXiv preprint arXiv:2302.04863, 2023. Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 8342–8360, 2020. Irene Heim. File change semantics and the familiarity theory of definiteness. Semantics Critical Concepts in Linguistics, pp. 108–135, 1983. Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328–339, 2018. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. ArXiv, abs/2212.04089, 2022. URL https://api.semanticscholar.org/CorpusID: 254408495. Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefen- stette, Tim Rockt¨aschel, and David Scott Krueger. Mechanistically analyzing the effects of fine- tuning on procedurally defined tasks, 2023. Hans Kamp, Josef Van Genabith, and Uwe Reyle. Discourse representation theory. In Handbook of Philosophical Logic: Volume 15, pp. 125–394. Springer, 2010. Lauri Karttunen. Discourse referents. In Notes from the linguistic underground, pp. 363–385. Brill, 1976. Najoung Kim and Sebastian Schuster. Entity tracking in language models. arXiv preprint arXiv:2305.02363, 2023. Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark se- crets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4365–4374, Hong Kong, China, November 2019. Association for Com- putational Linguistics. doi: 10.18653/v1/D19-1445. URL https://aclanthology.org/ D19-1445. 11Published as a conference paper at ICLR 2024 Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine- tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022. Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 1813–1827, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.143. URL https://aclanthology.org/2021.acl-long.143. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi´egas, Hanspeter Pfister, and Martin Watten- berg. Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022. Tom Lieberum, Matthew Rahtz, J ´anos Kram ´ar, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla, 2023. David Lindner, J ´anos Kram ´ar, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability. Advances in Neural Information Processing Systems, 36, 2024. Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks, 2023. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1384–1403, 2022. Gary Marcus. The deepest problem with deep learning, De- cember 2018. URL https://medium.com/@GaryMarcus/ the-deepest-problem-with-deep-learning-91c5991f5695 . Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36, 2022. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. What happens to BERT em- beddings during fine-tuning? In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of the Third BlackboxNLP Work- shop on Analyzing and Interpreting Neural Networks for NLP , pp. 33–44, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1.4. URL https://aclanthology.org/2020.blackboxnlp-1.4. Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. Mante S Nieuwland and Jos JA Van Berkum. When peanuts fall in love: N400 evidence for the power of discourse. Journal of cognitive neuroscience, 18(7):1098–1111, 2006. Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2(11):e7, 2017. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35: 27730–27744, 2022. Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific skill localiza- tion in fine-tuned language models. arXiv preprint arXiv:2302.06600, 2023. 12Published as a conference paper at ICLR 2024 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel. Chess as a testbed for language model state tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 11385–11393, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems , volume 33, pp. 12388–12401. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/ paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Inter- pretability in the wild: a circuit for indirect object identification in gpt-2 small, 2022. John Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Sim- ilarity analysis of contextual word representation models. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , pp. 4638–4655, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.422. URL https://aclanthology.org/2020.acl-main.422. Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. Interpretabil- ity at scale: Identifying causal mechanisms in alpaca.Advances in Neural Information Processing Systems, 36, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Process- ing Systems, 2023. Yichu Zhou and Vivek Srikumar. A closer look at how fine-tuning changes BERT. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1046–1061, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.75. URL https://aclanthology.org/2022.acl-long.75. 13Published as a conference paper at ICLR 2024 A U NRAVELING CRITICAL INFORMATION FLOW FOR ENTITY TRACKING TASK THROUGH ATTENTION KNOCKOUT Drawing inspiration from the Attention Knockout technique proposed in Geva et al. (2023), which aims to investigate the flow of crucial information from the subject token to the last token position, we adapted this technique to understand how essential information for entity tracking is conveyed within Llama-7B. In our adaptation, all attention heads of a specific layer and position are obstructed from attending to heads in the same layer at a different position, thereby limiting the flow of infor- mation between these positions at the designated layer. Unlike the approach in Geva et al. (2023), where a window of layers around the specified layer was blocked from attending to a previous position, our method initiates by blocking all layers. Subse- quently, at each step, we progressively unblock the next previously blocked layer. More precisely, we block attention heads of all layers at a given position from attending to heads in a different position. Then, in subsequent steps, we systematically unblock each layer, revealing which layer encodes vital information which when unblocked leads to improved performance of the model in conducting entity tracking tasks. 1 2 3 4 5 6 7 8 9 10 1112 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Knockout Starting Layer 0.00 0.20 0.40 0.60 0.80 0.66Accuracy Blocking attention heads at last position Model Accuracy Random Position Query Box Position Correct Object Position Figure A1: Critical Information Flow to Last Token Position In Fig. A1, the results are presented when heads at the last token position are prevented from attend- ing to various previous positions. It is observed that there are two primary sources of information that heads at the last token position utilize: the query box token and the correct object token posi- tion. Specifically, heads in the initial layers focus on and extract crucial information from the query box token residual stream, while heads in the later layers attend to the correct object token, bringing in another essential piece of information. As a baseline comparison, we blocked the heads from attending to a randomly selected previous position, which did not result in a loss of performance. Additionally, Fig. A2 shows the results when heads at the query box token position are blocked from attending to previous positions. We observe that heads in the initial layers transport vital information from the previous query box token position. The findings from the attention knockout methods align with the information flow subgraph iden- tified using path patching in section. 4.1, as shown in Fig. 1. In other words, heads at the previous query box position collect information about their segment, which is then passed on to the query box token residual stream by the initial layer heads at that position. Subsequently, heads in the initial layers at the final token position attend to the query box token position to incorporate it into their 14Published as a conference paper at ICLR 2024 1 2 3 4 5 6 7 8 9 10 1112 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Knockout Starting Layer 0.00 0.20 0.40 0.60 0.80 0.66Accuracy Blocking attention heads at query box position Model Accuracy Random Position Prev Query Box Position Figure A2: Critical Information Flow to Query Box Token Position residual stream. This information is utilized by the heads in the later layers to attend to the correct object token position and convey it to the final logit. B M INIMALITY We have utilized the minimality to identify heads that do not contribute significantly to the circuit performance. For each head v ∈ Cir, we seek a subset of heads K ⊆ Cir \\ {v}, such that when heads in K are knocked out of Cir, v can still recover the performance of Cir considerably. Unlike in Wang et al. (2022) which primarily defined K as the heads in the same class G ⊆ Cir that v belongs to, we use a greedy approach to compute the subset. We cannot use the entire class as K, since some of the classes have a large number of heads, e.g. Value Fetcher Heads, that when knocked out would result in a colossal decrease in performance, which cannot be recovered via a single head. To determine the subset K associated with a head v ∈ G, we rank all the other heads in G based on the difference in circuit performance. This difference is computed by comparing the circuit’s performance F(Cir), when only the other head vj (where vj is any head in G excluding v) is removed, and when both v and vj are removed from the circuit. More specifically, we use {F(Cir \\ {vj }) − F(Cir \\ {v, vj }) | vj ∈ G, vj ̸= v} to rank all heads in G \\ v and then consider upto top 30% of the heads to form the subset K. C E VALUATING LLAMA -7B C IRCUIT WITH COMPLETENESS CRITERION To evaluate the integrity of the Llama-7B entity tracking circuit, we use an additional criterion; completeness, which compares the performance of the circuit and the model under knockouts. The circuit is considered complete if eliminating any subset of its components, denoted as K ⊆ Cir, results in a comparable performance impact to removing the same subset K from the entire model, as described in Wang et al. (2022). Specifically, for every subsetK we calculate the performance of the full model M in the absence of K and compare it to the circuit performance Cir in the absence of K, i.e. |F(Cir \\K) −F(M \\K)|. We define this as the incompleteness score. If the circuit Cir is complete, it should have a low incompleteness score. Since there are exponentially many K, it is computationally intractable to evaluate the incompleteness score of every possible subsetK. Hence, Wang et al. (2022) proposed a few sampling methods to compute K and we use the following two: 1. Random: We uniformly sample a group of circuit components. 2. Circuit groups: We define K to be one of four groups of the circuit, i.e. A, B, C, and D as shown in Fig. 1. 15Published as a conference paper at ICLR 2024 The results in Fig. A3 and Table A1 indicate that the Llama-7B minimal circuit is not perfectly complete, i.e. there are additional heads in the model that are involved in the entity tracking task. Higher incompleteness score can also be attributed to backup heads, which become active only when other relevant heads are excluded during forward propagation, as identified by Wang et al. (2022). 0.0 0.2 0.4 0.6 0.8 1.0 F(C\\K) 0.0 0.2 0.4 0.6 0.8 1.0F(M\\K) Llama-7B circuit completeness tests Random set Group A Group B Group C Group D y = x Figure A3: Completeness of Llama-7B circuit.We plot the full model accuracy in the absence of a subgroup vs. the circuit accuracy in the absence of the same subgroup. In a complete circuit, this trend should represent equality (x = y). Our circuit is not perfectly complete and additional heads might be included. Table A1: Completeness evaluation of Llama-7B circuit.For the random setting we report the mean and standard over 10 random subsets K. Accuracy Full-Model Circuit Completeness Random 0.11 ± 0.06 0 .0 ± 0.01 0 .1 ± 0.06 Group A 0.19 0.0 0.19 Group B 0.23 0.02 0.21 Group C 0.04 0.02 0.02 Group D 0.58 0.46 0.12 D E NUMERATE LLAMA -7B CIRCUIT HEADS IN EACH GROUP Table A2: Groups of heads in the Llama-7B circuit. Group # Heads Functionality Name # DCM A 40 Value Value Fetcher 40 B 7 Position Position Transmitter 5 C 20 Position Position Detector 14 D 5 - Structure Reader - A: Value Fetcher L15 H13, L21 H3, L24 H5, L20 H14, L18 H8, L29 H7, L18 H3, L15 H18, L17 H28, L21 H4, L21 H25, L23 H15, L18 H28, L23 H19, L23 H20, L19 H30, L23 H5, L17 H27, L15 H5, L21 H0, L23 H17, L15 H2, L17 H3, L19 H20, L19 H11, L19 H8, L15 H6, L20 H29, L16 H23, L24 H0, L25 H14, L14 H13, L21 H26, L24 H8, L18 H6, L19 H26, L23 H16, L16 H27, L18 H20, L18 H25. B: Position Transmitter 16Published as a conference paper at ICLR 2024 L14 H27, L11 H23, L12 H23, L19 H12, L13 H0, L16 H2, L13 H14 C: Position Detector L10 H3, L13 H14, L9 H2, L9 H7, L11 H23, L9 H10, L1 H9, L7 H17, L13 H0, L6 H10, L4 H4, L7 H26, L9 H21, L8 H1, L12 H0, L8 H22, L10 H4, L11 H7, L7 H9, L10 H15 D: Structure Reader L8 H21, L12 H23, L11 H9, L8 H12, L11 H23 E C IRCUIT DISCOVERY IN GOAT-7B To better understand the impact of fine-tuning on underlying mechanism for performing entity track- ing task, we also identify the circuit in Goat-7B model responsible for performing this task, using the same procedure as described in Section 4.1. The Goat-7B circuit consists of four groups of heads positioned at the same token positions and similar layers, connected through the same type of com- position as observed in the Llama-7B circuit. We include a total of 200 attention heads (80,30,50,40 heads in Groups A,B,C,D respectively) in the identified circuit. To eliminate redundant heads, we employ the minimality criterion, mirroring the approach used for the Llama-7B circuit, resulting in a circuit with 175 heads. The distribution of heads among groups is detailed in Table A3. The minimal circuit is evaluated using completeness and faithfulness metrics in the following subsections. E1 E NUMERATE GOAT-7B C IRCUIT HEADS IN EACH GROUP Table A3: Groups of heads in the Goat-7B circuit. Group # Heads Functionality Name # DCM A 68 Value Value Fetcher 56 B 28 Position Position Transmitter 15 C 40 Position Position Detector 18 D 39 - Structure Reader - A: Value Fetcher L24 H5, L17 H28, L20 H14, L21 H4, L21 H3, L18 H8, L23 H19, L19 H30, L30 H4, L23 H17, L29 H7, L19 H20, L23 H15, L31 H6, L28 H16, L30 H8, L28 H17, L17 H8, L25 H14, L24 H8, L19 H8, L23 H16, L21 H25, L31 H26, L19 H11, L31 H25, L18 H20, L31 H23, L15 H12, L31 H1, L23 H27, L19 H26, L21 H19, L20 H0, L19 H1, L17 H27, L20 H29, L20 H7, L17 H5, L18 H21, L31 H14, L15 H2, L18 H6, L16 H23, L15 H31, L19 H23, L21 H11, L23 H31, L31 H0, L17 H24, L11 H5, L22 H17, L13 H10, L14 H9, L18 H23, L15 H24, L21 H17, L16 H27, L19 H2, L17 H23, L24 H0, L15 H9, L31 H24, L19 H15, L24 H4, L25 H19, L14 H3, L31 H30 B: Position Transmitter L14 H27, L12 H23, L11 H23, L19 H12, L17 H26, L13 H1, L16 H16, L13 H0, L16 H2, L15 H4, L15 H13, L16 H28, L15 H18, L13 H25, L14 H11, L14 H13, L10 H6, L13 H27, L12 H25, L12 H8, L12 H0, L11 H9, L18 H3, L18 H28, L14 H10, L11 H26, L11 H24, L13 H3 C: Position Detector L10 H3, L13 H14, L6 H10, L11 H23, L11 H24, L9 H7, L1 H9, L9 H10, L10 H7, L7 H17, L13 H0, L5 H7, L12 H0, L12 H8, L12 H23, L12 H20, L13 H25, L13 H12, L4 H4, L13 H4, L12 H16, L11 H2, L11 H7, L7 H26, L10 H4, L4 H27, L9 H21, L8 H1, L11 H28, L12 H30, L8 H12, L9 H30, L15 H26, L13 H23, L6 H17, L13 H1, L8 H29, L8 H25, L13 H6, L12 H17 D: Structure Reader L8 H21, L11 H9, L12 H23, L11 H23, L12 H13, L9 H14, L9 H21, L10 H6, L7 H2, L9 H29, L12 H29, L8 H13, L8 H12, L11 H28, L12 H30, L12 H25, L8 H10, L8 H26, L8 H7, L10 H24, L6 H23, L9 H30, L11 H24, L9 H18, L9 H12, L12 H0, L8 H19, L11 H11, L8 H9, L11 H20, L11 H26, L9 H6, L8 H25, L9 H7, L9 H26, L10 H3, L9 H9, L12 H11, L7 H0 17Published as a conference paper at ICLR 2024 E2 E VALUATING GOAT-7B C IRCUIT WITH COMPLETENESS CRITERION To assess whether the identified Goat-7B circuit encompasses all the components engaged in ex- ecuting the entity tracking task within the Goat-7B model, we utilize the completeness metric, as detailed in Section C. We use two sampling methods for computingK: 1) Random sampling and 2) Circuit groups. Fig. A4 and Table A4 presents the completeness score of the Goat-7B circuit, revealing that the circuit is nearly complete, except for heads in Group A. This suggests the possibility of additional heads contributing to the entity tracking task. The higher incompleteness in Group A could also be attributed to backup heads, which become active only when other relevant heads are excluded during forward propagation, as identified by (Wang et al., 2022). Table A4: Completeness evaluation of Goat-7B circuit.For the random setting we report the mean and standard over 20 random subsets K. Accuracy Full-Model Circuit Completeness Random 0.13 ± 0.06 0 .16 ± 0.11 0 .06 ± 0.06 Group A 0.41 0.21 0.2 Group B 0.13 0.11 0.02 Group C 0.06 0.11 0.05 Group D 0.62 0.63 0.01 0.0 0.2 0.4 0.6 0.8 1.0 F(C\\K) 0.0 0.2 0.4 0.6 0.8 1.0F(M\\K) Goat-7B circuit completeness tests Random set Group A Group B Group C Group D y = x Figure A4: Completeness of Goat-7B circuit.We plot the full model accuracy in the absence of a subgroup vs. the circuit accuracy in the absence of the same subgroup. In a complete circuit, this trend should represent equality (x = y). E3 E VALUATION OF GOAT-7B C IRCUIT WITH FAITHFULNESS CRITERION For a more thorough evaluation of the Goat-7B circuit in comparison to the entire Goat-7B model, we apply the faithfulness criterion as defined in Section 4.2. As presented in Table A5, Goat-7B attains a faithfulness score of 0.99, indicating that the identified Goat-7B circuit can recover almost the entire model performance. We also observe that the Goat-7B circuit has a similar faithfulness score on FLoat-7B, suggesting a high overlap between the components that perform entity tracking task in these models. We also observe that the performance of the Goat-7B circuit on Llama-7B and Vicuna-7B is higher than the entire model. Essentially, this implies that when the heads not present in the Goat-7B circuit are mean-ablated in Llama-7B and Vicuna-7B, their performance shows improvement. This phenomenon aligns with observations made in Vig et al. (2020),i.e. a small proportion of heads can surpass the gender bias effect of the entire model. Our speculation revolves around the presence of 18Published as a conference paper at ICLR 2024 Table A5: Entity-tracking circuit found in Goat-7B, evaluated in Llama-7B, Vicuna-7B, Goat-7B, and FLoat-7B, without any adjustment of the circuit graph. The circuit achieves high accuracy and faithfulness scores in all models (chance accuracy is 0.14). Accuracy Model Finetuned? Full-Model Circuit Random Circuit Faithfulness Llama-7B – 0.66 0.77 0.00 1.17 Vicuna-7B User conversations 0.67 0.76 0.00 1.13 Goat-7B Arithmetic tasks (LoRA) 0.82 0.81 0.01 0.99 FLoat-7B Arithmetic tasks (w/o LoRA) 0.82 0.79 0.02 0.96 negative heads, i.e., attention heads that work against predicting the correct object, in these models, contributing to a decrease in their overall performance, as also reported in Wang et al. (2022). E4 C OMPARISON OF LLAMA -7B AND GOAT-7B CIRCUITS We observe that, while the faithfulness scores of the Llama-7B and Goat-7B circuits in their re- spective models are comparable (i.e., 1.0 and 0.99), there is a substantial disparity in their sizes. Specifically, the Goat-7B circuit comprises 175 heads, whereas the Llama-7B circuit has only 72 heads. This notable difference in the number of attention heads in the circuits implies that fine- tuning introduces additional components to the circuitry dedicated to solving the entity tracking task. Indeed, Table A10 and Fig. A5 suggest that the Goat-7B circuit is approximately a superset of the Llama-7B circuit. Additionally, most of the highly causal heads in the Llama-7B circuit remain to be highly influencial in the Goat-7B circuit, suggesting that a minimal alteration in the base model circuit. Table A6: Intersection of Attention Heads in Llama-7B and Goat-7B Circuits:Comparison of the number of heads in each group of Llama-7B and Goat-7B circuits as well as their intersection. Head Group Number of heads Number of heads Intersection Precision Recall in Llama-7B circuit in Goat-7B circuit A 40 68 27 0.68 0.4 B 7 28 6 0.86 0.21 C 20 40 16 0.8 0.4 D 5 39 5 1.0 0.13 F C IRCUIT DISCOVERY IN FLOAT-7B In addition to examining the circuits of Llama-7B and Goat-7B, we also identify the circuit in the FLoat-7B model responsible for performing entity tracking tasks. The primary objective of ana- lyzing the FLoat-7B model and its associated circuit is to establish whether the results generalize to models fine-tuned without LoRA. Similar to other circuit identification process, we followed the procedure described in Section 4.1, to discover the Float-7B circuit which also consists of four groups of heads located at the same token positions and similar layers to that in Llama-7B and Goat- 7B circuits. We initially included a total of 200 attention heads (80, 30, 50, 40 heads in Groups A, B, C, D, respectively), which were subsequently reduced to 175 heads after applying the minimality criterion. The distribution of heads in the minimal circuit is detailed in Section F1. Notably, we found that the FLoat-7B circuit closely resembles the Goat-7B circuit, suggesting that the impact of fine-tuning with and without LoRA on the underlying mechanism for performing entity tracking tasks is similar. In the following subsections, we evaluate the FLoat-7B circuit using completeness and faithfulness criterion. 19Published as a conference paper at ICLR 2024 0.1 0.2 0.3 Causal Impact of Heads in Llama and Goat Circuit Heads in Goat Circuit Heads NOT in Goat Circuit 0 25 50 75 100 125 150 175 Heads 0.1 0.2 0.3 Heads in Llama Circuit Heads NOT in Llama Circuit Absolute Patching Score Figure A5: Causal Impact of Llama and Goat Circuit Heads: The first subplot shows the attention heads that are included in the Goat circuit (red), sorted based on their causal impact, i.e. absolute patching score. For comparison, the second subplot shows the attention heads that are included in the Llama circuit (green) in the same order as the Goat component in the upper subplot (e.g. the first bar in both subplots represents the same head that is present in both the circuits, hence colored red and green respectively). Table A7: Groups of heads in the FLoat-7B circuit. Group # Heads Functionality Name # DCM A 68 Value Value Fetcher 60 B 29 Position Position Transmitter 13 C 40 Position Position Detector 22 D 38 - Structure Reader - F1 E NUMERATE FLOAT-7B C IRCUIT HEADS IN EACH GROUP A: Value Fetcher L18 H3, L24 H5, L18 H8, L21 H4, L20 H14, L17 H28, L30 H4, L18 H28, L23 H17, L19 H30, L23 H19, L28 H16, L15 H5, L19 H20, L31 H26, L15 H18, L25 H14, L19 H11, L15 H12, L29 H7, L17 H3, L30 H8, L19 H1, L31 H6, L16 H23, L24 H0, L23 H27, L31 H29, L20 H29, L15 H22, L23 H16, L15 H6, L18 H23, L17 H24, L15 H15, L12 H16, L23 H31, L23 H30, L29 H22, L18 H6, L20 H0, L27 H19, L21 H10, L25 H19, L21 H23, L13 H23, L22 H17, L18 H20, L17 H27, L19 H15, L14 H9, L21 H11, L13 H10, L17 H5, L14 H13, L20 H26, L18 H25, L19 H24, L18 H21, L24 H11, L12 H9, L31 H23, L26 H16, L16 H15, L19 H17, L17 H15, L19 H16, L22 H5 B: Position Transmitter L14 H27, L11 H23, L12 H23, L13 H0, L13 H14, L13 H1, L19 H12, L16 H2, L17 H26, L16 H16, L14 H0, L15 H13, L12 H8, L15 H4, L14 H11, L16 H28, L13 H25, L10 H6, L13 H27, L11 H26, L12 H30, L12 H21, L12 H5, L14 H17, L11 H5, L16 H17, L15 H26, L10 H12, L15 H1 C: Position Detector L10 H3, L13 H14, L7 H17, L11 H23, L9 H10, L9 H7, L6 H10, L11 H24, L13 H0, L4 H4, L7 H3, L12 H9, L10 H4, L8 H1, L9 H21, L12 H23, L11 H2, L12 H8, L10 H7, L12 H0, L10 H21, L9 H15, L13 H4, L13 H12, L12 H5, L7 H9, L11 H7, L8 H29, L12 H20, L15 H26, L12 H17, L9 H1, L12 H30, L11 H28, L5 H5, L13 H1, L10 H18, L13 H26, L10 H6, L11 H19 D: Structure Reader L8 H21, L12 H23, L11 H9, L11 H23, L9 H21, L8 H12, L12 H13, L9 H14, L8 H11, L10 H6, L11 H28, L9 H30, L12 H29, L6 H17, L9 H12, L8 H7, L12 H30, L12 H25, L12 H15, L8 H26, L7 H30, L10 H24, L9 H9, L6 H25, 20Published as a conference paper at ICLR 2024 L8 H13, L11 H24, L5 H4, L9 H26, L12 H11, L10 H12, L9 H6, L11 H11, L12 H5, L7 H2, L9 H28, L10 H7, L7 H0, L6 H31 F2 E VALUATING FLOAT-7B C IRCUIT WITH COMPLETENESS CRITERION We assess the entirety for the FLoat-7B circuit using the completeness criterion, as described in Section C. Similar to other circuits’ evaluation, we utilize two sampling methods for computing K: 1) Random sampling and 2) Circuit groups. Fig. A6 and Table A8 presents the completeness score of the FLoat-7B circuit. As with Goat-7B circuit, FLaot-7B circuit is almost complete, except for the heads in Group A, suggesting either the presence of additional heads that are fetching the value of correct object or backup heads, that get activated when other relevant heads are ablated during forward propagation. Table A8: Completeness evaluation of FLoat-7B circuit.For the random setting we report the mean and standard over 20 random subsets K. Accuracy Full-Model Circuit Completeness Random 0.2 ± 0.07 0 .12 ± 0.1 0 .09 ± 0.04 Group A 0.51 0.08 0.43 Group B 0.15 0.11 0.04 Group C 0.19 0.07 0.12 Group D 0.68 0.57 0.11 0.0 0.2 0.4 0.6 0.8 1.0 F(C\\K) 0.0 0.2 0.4 0.6 0.8 1.0F(M\\K) FLoat-7B circuit completeness tests Random set Group A Group B Group C Group D y = x Figure A6: Completeness of FLoat-7B circuit.We plot the full model accuracy in the absence of a subgroup vs. the circuit accuracy in the absence of the same subgroup. In a complete circuit, this trend should represent equality (x = y). F3 E VALUATING FLOAT-7B C IRCUIT WITH FAITHFULNESS CRITERION In addition to the completeness score, we assess the FLoat-7B circuit using the faithfulness criterion, which evaluates the performance of the identified circuit compared to the entire model. We observe that the FLoat-7B circuit attains a faithfulness score of 1.0 on the FLoat-7B model, indicating its capability to recover the entire model performance. Furthermore, we note that its faithfulness on the Goat-7B circuit is 0.93, suggesting a high degree of overlap between the circuits of the FLoat-7B and Goat-7B models for performing entity tracking. 21Published as a conference paper at ICLR 2024 Table A9: Entity-tracking circuit found in FLoat-7B, evaluated in Llama-7B, Vicuna-7B, Goat- 7B, and FLoat-7B without any adjustment of the circuit graph . The circuit achieves high accuracy and faithfulness scores in all models (chance accuracy is 0.14). Accuracy Model Finetuned? Full-Model Circuit Random Circuit Faithfulness Llama-7B – 0.66 0.69 0.0 1.05 Vicuna-7B User conversations 0.67 0.7 0.0 1.04 Goat-7B Arithmetic tasks (LoRA) 0.82 0.76 0.01 0.93 FLoat-7B Arithmetic tasks (w/o LoRA) 0.82 0.82 0.02 1.0 F4 C OMPARISON OF LLAMA -7B AND FLOAT-7B CIRCUITS To gain insights into how fine-tuning impacts the underlying mechanism for performing entity track- ing in Llama-7B, we compare the identified circuits of Llama-7B and FLoat-7B models. Similar to the observation with the Goat-7B model, we note a substantial difference in the sizes of the Llama- 7B and FLoat-7B circuits (72 and 175, respectively). This difference in circuit sizes suggests that fine-tuning introduces additional components dedicated to the task of entity tracking. This is further supported by the results presented in Table A10 and Fig. A7, demonstrating that the FLoat-7B circuit indeed forms a superset of the Llama-7B circuit. Additionally, most of the highly causal heads in the Llama-7B circuit remain highly influential in the Goat-7B circuit, suggesting minimal alteration in the base model circuit. Table A10: Intersection of Attention Heads in Llama-7B and FLoat-7B Circuits:Comparison of the number of heads in each group of Llama-7B and FLoat-7B circuits as well as their intersection. Head Group Number of heads Number of heads Intersection Precision Recall in Llama-7B circuit in FLoat-7B circuit A 40 68 27 0.68 0.4 B 7 29 7 1.0 0.24 C 20 40 15 0.75 0.38 D 5 38 5 1.0 0.13 0.1 0.2 0.3 Causal Impact of Heads in Llama and FLoat Circuit Heads in FLoat Circuit Heads NOT in FLoat Circuit 0 25 50 75 100 125 150 175 Heads 0.1 0.2 0.3 Heads in Llama Circuit Heads NOT in Llama Circuit Absolute Patching Score Figure A7: Causal Impact of Llama and FLoat Circuit Heads: The first subplot shows the at- tention heads that are included in the FLoat circuit (red), sorted based on their causal impact, i.e. absolute patching score. For comparison, the second subplot shows the attention heads that are in- cluded in the Llama circuit (green) in the same order as the FLoat component in the upper subplot (e.g. the first bar in both subplots represents the same head that is present in both the circuits, hence colored red and green respectively). 22Published as a conference paper at ICLR 2024 G D ESIDERATA FOR DCM To identify the functionality of various circuit components, we conceptualized and defined three desiderata: (1) Object desideratum is used to identify attention heads that encode the value of the correct object in their output. Consequently, when the output of these components is patched from the counterfac- tual run (which contains a different correct object value, associated with a completely different box label) to the original run, the final output changes to the value of the correct object of the counter- factual example, as shown in Fig. 2(a). This occurs even though that object was not included in the original statement. (2) Label desideratum is used to identify circuit components that are encoding the query box label value. Hence, when their output is patched from the counterfactual run (that queried a different box label) to the original run, the final output of the original run changes to the object from the original run that is associated with the query box label of thecounterfactual statement, as shown in Fig. 2(b). Notably, this occurs even though that object was not initially associated with the query box in the original statement but rather with the query box of the counterfactual one, and it is not present in the counterfactual statement. (3) Position desideratum is used to identify circuit components that encode the positional informa- tion of the correct object, i.e. when they are patched from counterfactual run to the original run, the final output of the original run changes to the object in the original statement located at the same po- sition as the correct object of the counterfactual statement, as shown in Fig. 2(c). This happens even though this object is not the correct object of the original run and is not present in the counterfactual statement. For each of the three desiderata, we train a binary mask over the model components to identify the circuit components encoding the corresponding vital information to accomplish the task. H DCM E XPERIMENT DETAILS As mentioned in section 5.2, we restricted the model component search space to the heads in each group. More specifically, for each group in the circuit, we identify the subset of heads that are involved in three functionalities: 1) encoding object value, 2) box label value, and 3) correct object position. We synthetically generated training ( N = 1000) and eval datasets (N = 500), according to the desiderata. To train a binary mask consisting of learnable parameters ( W), we minimize the following loss function: L = −logittarget + λ X 1 − W (1) with λ = 0.01. We trained it for two epochs, with ADAM optimizer and a batch size of 32. I C IRCUIT FUNCTIONALITY IN GOAT-7B AND FLOAT-7B To identify the functionality of the head groups within the Goat-7B and FLoat-7B circuits, we adopted a procedure similar to the one detailed in section 5. Initially, we employed DCM to lo- calize the subset of heads within each group encoding various functionalities. Subsequently, we conducted activation patching on these subsets of heads. As depicted in Table A3 as well as Fig. A8 for Goat-7B circuit, and in Table A7 as well as Fig. A9 for FLoat-7B circuit, the functionality of each head group mirrors that of the Llama-7B circuit. Specifically, Group A heads predominantly encode the value of the correct object, while Groups B and C are tasked with encoding the positional information of the correct object. Nevertheless, the functionality of heads in Group D continues to remain a mystery. J A DDITIONAL DESIDERATA FOR POSITIONAL INFORMATION In section 5, we found that the models are using the positional information of the correct object for entity tracking. However, we could not precisely characterize the positional information. Therefore, 23Published as a conference paper at ICLR 2024 Label Object Position 0.0 0.2 0.4 0.6Accuracy (A) Value fetcher Label Object Position (B) Position transmitter Label Object Position (C) Position detector Label Object Position (D) Structure reader Llama-7B Vicuna-7B FLoat-7B Goat-7B Chance (position) Figure A8: Activation patching results of the functionality heads in the Goat-7B circuit, identified using DCM. Error bars indicate standard deviation. Label Object Position 0.0 0.2 0.4 0.6Accuracy (A) Value fetcher Label Object Position (B) Position transmitter Label Object Position (C) Position detector Label Object Position (D) Structure reader Llama-7B Vicuna-7B FLoat-7B Goat-7B Chance (position) Figure A9: Activation patching results of the functionality heads in the Float-7B circuit, identified using DCM. Error bars indicate standard deviation. we devised a few additional positional desiderata, as shown in Tables A12 and A13. Similar to our analysis on previously defined desiderata, we applied activation patching on the subset of heads in the Position Transmitter group that encodes positional information. In other words, we were interested in understanding what kind of positional information is being used by Value Fetcher heads to locate the correct object in the context. Results are summarized in Table A11. Although redundant text at the start, end, or between the object and the box does not impact the positional information encoded in those heads, an additional segment at the start does seem to affect it. Further, mentions of boxes with labels before the correct segment (the segment containing the correct object) also interfere with the positional information. In addition to relative positional information, the semantics of the association between the object and the box are also influential. Combining these results, we speculate that the model is enumerating the association of the boxes and their corresponding object from the start token as well as keeping track of the semantics of the association. However, more work is needed to completely characterize the positional information. Table A11: Positional desideratato characterize the positional information transported by the Posi- tion Transmitter heads to the Value Fetcher heads. Please refer to Table A12 and A13 for examples. Position hyp. Acc. Var. 1 Random text at the start 0.35 0.02 2 Random text at the end 0.34 0.01 3 Additional tokens between object and box 0.38 0.02 4 Additional segment at the start 0.25 0.02 5 Additional segment at the end 0.31 0.02 6 Additional boxes before correct segment 0.2 0.02 7 Incorrect box segment 0.16 0.01 8 Altered box object order 0.16 0.01 9 Altered association btw box and object 0.18 0.02 10 No comma to separate segments 0.38 0.02 11 Additional comma after the object 0.36 0.02 24Published as a conference paper at ICLR 2024 Table A12: Examples of Positional desiderata: For each positional desiderata reported in Table A11, this table contains specific examples of original, alternate, and target data. Desiderata Base Source Target 1 The document is in Box X, the pot is in Box T, the magnet is in Box A , the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D. Box S contains the There are a bunch of boxes con- taining objects, the magnet is in Box O, the bell is in Box M, the leaf is in Box W , the cup is in Box G, the ice is in Box J, the milk is in Box Z, the wire is in Box H. Box W contains the magnet 2 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D . Box A con- tains the The document is in Box Q, the bus is in Box F , the camera is in Box R, the glass is in Box W, the magazine is in Box Z, the coffee is in Box E, the watch is in Box C , these are a bunch of boxes containing objects. Box C contains the map 3 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M , the cross is in Box K, the map is in Box D. Box A contains the The pot is in Box U, the flower is in Box D, the car is in Box K, the disk is in Box C, the fan is contained in the Box H , the bill is in Box S, the painting is in Box L. Box H contains the bill 4 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M , the cross is in Box K, the map is in Box D. Box A contains the The apple is in Box O, the dress is in Box N, the boot is in Box Y, the hat is in Box L, the bus is in Box X , the painting is in Box F , the drug is in Box J, the string is in Box D. Box X contains the game 5 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K, the map is in Box D . Box A con- tains the The hat is in Box K, the plane is in Box H, the tie is in Box U, the wire is in Box F , the file is in Box R, the note is in Box Y, the train is in Box G , the apple is in Box O. Box G con- tains the map 6 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E , the bill is in Box M, the cross is in Box K, the map is in Box D. Box A contains the The hat is in Box K, the plane is in Box H, the tie is in Box U, there are three additional boxes, Box PP , Box BB and Box AA, the wire is in Box F , the file is in Box R, the note is in Box Y, the train is in Box G. Box F contains the game 7 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M , the cross is in Box K, the map is in Box D. Box A contains the The magnet is in Box O, the bell is in Box M, the leaf is in Box W, the cup is in Box G, the ice is in Box J , the milk is in Box Z, the wire is in Box H. Box J contains the cross 8 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E , the bill is in Box M, the cross is in Box K, the map is in Box D. Box A contains the The pot is in Box U, the flower is in Box D, the car is in Box K, Box C contains the disk , the fan is in Box H, the bill is in Box S, the painting is in Box L. Box C contains the game 25Published as a conference paper at ICLR 2024 Table A13: Examples of Positional desiderata: For each positional desiderata reported in Table A11, this table contains specific examples of original, alternate, and target data. Desiderata Base Source Target 9 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M, the cross is in Box K , the map is in Box D. Box A contains the The ticket is in Box N, the book is in Box J, the gift is in Box W, the coat is in Box Y, the rose is in Box K, the wheel is not in Box G , the brick is in Box V . Box G contains the cross 10 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E, the bill is in Box M , the cross is in Box K, the map is in Box D. Box A contains the The pot is in Box U the flower is in Box D the car is in Box K the disk is in Box C the fan is in Box H the bill is in Box S the painting is in Box L. Box H contains the bill 11 The document is in Box X, the pot is in Box T, the magnet is in Box A, the game is in Box E , the bill is in Box M, the cross is in Box K, the map is in Box D. Box A contains the The clock, is in Box M, the bomb, is in Box J, the newspaper, is in Box G, the letter, is in Box L , the suit, is in Box Y, the computer, is in Box R, the wheel, is in Box V . Box L con- tains the bill 26",
      "references": [
        "Gpt takes the bar exam.",
        "Causal scrubbing: A method for rigorously testing interpretability hypothe-\n ses.",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Deep reinforcement learning from human preferences.",
        "A toy model of universality: Reverse engineering how networks learn group operations.",
        "Scaling instruction-finetuned language mod-\n els.",
        "Towards automated circuit discovery for mechanistic interpretability.",
        "Discovering variable binding circuitry with desiderata",
        "BERT: Pre-training of deep bidirectional transformers for language understanding.",
        "A mathematical framework for transformer circuits.",
        "Toy models of superposi-\n tion.",
        "How do language models bind entities in context?",
        "Dissecting recall of factual associations in auto-regressive language models.",
        "Localizing model behavior with path patching",
        "Knowledge is a region in weight space for fine-tuned language models.",
        "Don’t stop pretraining: Adapt language models to domains and tasks.",
        "File change semantics and the familiarity theory of definiteness.",
        "Universal language model fine-tuning for text classification.",
        "Lora: Low-rank adaptation of large language models.",
        "Editing models with task arithmetic.",
        "Mechanistically analyzing the effects of fine-\n tuning on procedurally defined tasks",
        "Discourse representation theory.",
        "Discourse referents.",
        "Entity tracking in language models.",
        "Revealing the dark se-\n crets of BERT.",
        "Fine-\n tuning can distort pretrained features and underperform out-of-distribution.",
        "Implicit representations of meaning in neural language models.",
        "Emergent world representations: Exploring a sequence model trained on a synthetic task.",
        "Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla",
        "Tracr: Compiled transformers as a laboratory for interpretability.",
        "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
        "Language models of code are few-shot commonsense learners.",
        "The deepest problem with deep learning",
        "Locating and editing factual associations in GPT.",
        "What happens to BERT em-\n beddings during fine-tuning?",
        "Progress measures for grokking via mechanistic interpretability.",
        "When peanuts fall in love: N400 evidence for the power of discourse.",
        "Feature visualization.",
        "Zoom in: An introduction to circuits.",
        "Training language models to follow instructions with human feedback.",
        "Task-specific skill localiza-\n tion in fine-tuned language models.",
        "Language models are unsupervised multitask learners.",
        "Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "Stanford alpaca: An instruction-following llama model",
        "Chess as a testbed for language model state tracking.",
        "Llama: Open and efficient foundation language models",
        "Llama 2: Open founda-\n tion and fine-tuned chat models.",
        "Investigating gender bias in language models using causal mediation analysis.",
        "Similarity analysis of contextual word representation models.",
        "Interpretabil-\n ity at scale: Identifying causal mechanisms in alpaca.",
        "Judging llm-as-a-judge with mt-bench and chatbot arena.",
        "LIMA: Less is more for alignment.",
        "A closer look at how fine-tuning changes BERT."
      ],
      "meta_data": {
        "arxiv_id": "2402.14811v1",
        "authors": [
          "Nikhil Prakash",
          "Tamar Rott Shaham",
          "Tal Haklay",
          "Yonatan Belinkov",
          "David Bau"
        ],
        "published_date": "2024-02-22T18:59:24Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Investigates how fine-tuning affects internal mechanisms (circuits) in LLMs, using in-context entity tracking as a case study where math-fine-tuning yields large gains. Discovers a sparse attention-head circuit in base Llama-7B (72 heads in 4 groups) sufficient for entity tracking; shows the same circuit is largely preserved across fine-tuned variants (Vicuna-7B, Goat-7B LoRA arithmetic, FLoat-7B full arithmetic), recovering 88–97% of fine-tuned performance. Finds functional invariance: all models track entities by propagating the position of the queried entity’s referent, then fetching its value from that position, rather than transporting the entity name/value directly. Attributes performance gains mainly to enhanced handling/encoding of positional information in the shared circuit (especially value-fetching heads), with fine-tuned models adding extra heads that implement the same roles. Introduces CMAP (cross-model activation patching) to localize which sub-mechanisms improve after fine-tuning and demonstrates activation compatibility across models.",
        "methodology": "Mechanistic interpretability pipeline centered on attention-head circuits: (1) Path Patching to discover a causal subgraph of attention heads for entity tracking by comparing clean vs. ‘noisy’ counterfactual inputs and scoring patched paths by relative drop in probability of the correct next token. Iteratively identifies 4 groups of heads (A–D) at specific token positions and Q/V-composition edges between groups. (2) Circuit pruning via a minimality criterion that measures each head’s marginal contribution under greedy knockouts, yielding a 72-head minimal circuit; evaluates with faithfulness/completeness. (3) Desiderata-based Component Masking (DCM) to automatically learn sparse masks over heads that correspond to semantic hypotheses (object value, queried label, or positional information), validated with activation patching accuracy against counterfactual targets; interprets groups as Value Fetcher (A), Position Transmitter (B), Position Detector (C), and Structure Reader (D). (4) CMAP: patches activations of corresponding circuit components from a fine-tuned model into the base model on the same input to pinpoint which group’s representations drive performance improvements (notably Value Fetcher and Position Transmitter outputs).",
        "experimental_setup": "Task/data: Adapted Kim & Schuster (2023) synthetic entity-tracking dataset about objects in labeled boxes; each example has 7 box–object statements (no state-changing operations), single-token object nouns, random alphabetic box labels. Context sentences use ‘The apple is in box F’ ordering to avoid trivial substring matching; query prompts ‘Box F contains the’ and evaluation is next-token accuracy (chance 1/7≈0.14). Models: LLaMA-7B base; Vicuna-7B (conversation fine-tune); Goat-7B (arithmetic fine-tune via LoRA); FLoat-7B (same arithmetic data, full fine-tune). Evaluation: Full-model accuracy over 500 tasks; circuit discovery with 300 examples for path patching; circuit faithfulness via mean ablation of non-circuit heads and comparing accuracy F(Cir)/F(M); random-circuit baseline with matched head counts; completeness tested by removing subsets K (random or whole groups) and comparing circuit vs. full-model degradation. DCM: trains binary masks per group/desideratum on ~1000 synthetic training triples; tests with activation patching on 500 examples, repeated (10×) to report mean±std. Cross-model analysis: CMAP patches outputs of functional subsets from Goat/FLoat into Llama to measure recovered accuracy relative to fine-tuned baselines.",
        "limitations": "Scope limited to one synthetic, static entity-tracking benchmark (7 boxes, no updates, single-token objects), so conclusions may not generalize to naturalistic discourse, multi-token entities, longer contexts, or dynamic state changes. Circuit analysis focuses on attention heads and treats all MLPs as implicitly involved, potentially missing key MLP/embedding contributions or alternative pathways; completeness tests indicate circuits are not perfectly complete and ‘backup/negative heads’ may exist. Circuit identification depends on design choices (noise task construction, patching score, thresholds, greedy minimality) and may yield different head sets under other settings. Functional labeling of Group D (‘Structure Reader’) remains unresolved. ‘Positional information’ is only partially characterized (likely relative/segment-based and sensitive to extra segments/box mentions), leaving the precise encoding algorithm unclear. Cross-model activation compatibility observed for these model pairs/tasks, but may not hold broadly across architectures, larger distribution shifts, or other fine-tunes.",
        "future_research_directions": "Test mechanism invariance across more tasks (coreference, instruction following, tool use, multi-step reasoning) and more model families/scales to determine when fine-tuning preserves vs. rewires circuits. Analyze fine-tuning dynamics over training checkpoints to identify when additional heads become causal and how positional encoding gets augmented. Extend circuit decomposition to include MLPs, embeddings, and residual-stream features (e.g., using causal scrubbing), and fully explain the Structure Reader group. More precisely characterize the ‘augmented positional information’ (absolute vs. relative, segment indexing, syntactic cues) and how it interacts with RoPE/positional embeddings. Investigate links to alternative binding theories (e.g., binding-ID vectors) and whether both mechanisms coexist or compose. Use CMAP for transfer diagnostics: predict which fine-tunes will improve a capability by patching candidate subcircuits, and explore targeted fine-tuning/regularization to enhance specific sub-mechanisms without adding spurious heads.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Localized Zeroth-Order Prompt Optimization",
      "full_text": "arXiv:2403.06733v1  [quant-ph]  11 Mar 2024 On the construction of a quantum channel corresponding to non-commutative graph for a qubit interacting with quantum oscillator G.G. Amosov, 1, ∗ A.S. Mokeev, 1, † and A.N. Pechen 1, ‡ 1Steklov Mathematical Institute of Russian Academy of Scien ces, Gubkina str., 8, Moscow 119991, Russia We consider error correction, based on the theory of non-commu tative graphs, for a model of a qubit interacting with quantum oscillator. The dynamics of the co mposite system is governed by the Schr¨ odinger equation which generates positive operator-valued measure (POVM) for the system dynamics. We construct a quantum channe l generating the non- commutative graph as a linear envelope of the POVM. The idea is based on applying a generalized version of a quantum channel using the apparatus of v on Neumann algebras. The results are analyzes for a non-commutative graph generated by a qubit interacting with quantum oscillator. For this model the quantum anticlique which d etermines the error correcting subspace has an explicit expression. I. INTRODUCTION One of the important notions in quantum information theory i s the notion of a non-commutative operator graph, which is an operator space containing the id entity operator and closed under op- erator conjugation. For each completely positive trace-pr eserving map (i.e., a quantum channel) there is a unique operator graph determining the ability to t ransmit information with zero error via the channel. This graph allows to deﬁne the Knill-Laﬂamm e suﬃcient condition for the sub- space to be a quantum error-correction code. A natural oppos ite task is to ﬁnd quantum channel corresponding to the given graph [1, 2]. All the graphs are kn own to be linearly generated by positive operator valued measures (POVMs) and, vice versa, for each graph these exists POVM which generates this graph, so that the task can be posed for P OVMs. A solution to this problem can be found using Naimark dilatation [2]. Non-commutative operator graphs for various inﬁnite-dime nsional quantum systems were stud- ied in [3, 4]. In this paper, we study error correction for a mo del of an inﬁnite-dimensional quantum system consisting of a qubit interacting with quantum oscil lator [5]. The dynamics of the compos- ite system is governed by Schr¨ odinger equation which entan gles initially separable quantum states. The dynamics generates POVM for the system. Quantum anticli que is the projector onto error correcting subspace. We construct a generalized quantum ch annel, acting between preduals of two von Neumann algebras, which determines the graph correspon ding to the given POVM with an operator-valued density. Our construction is close to the s imilar ﬁnite-dimensional result presented in [6]. The techniques are based upon [7]. The results are ana lyzed for the graph corresponding to the error correction model of a qubit interacting with quant um oscillator. II. GENERALIZED QUANTUM CHANNELS GENERATED BY POVMS We use some basic notions from the theory of von Neumann algeb ras ( W ∗-algebras in other terminology) [8]. ∗ E-mail:gramos@mi-ras.ru † E-mail:aleksandrmokeev@yandex.ru ‡ E-mail:apechen@gmail.com2 Denote B(H) and T (H) the algebra of all bounded operators and the space of nuclea r operators in a separable Hilbert space H respectively, the notation ∥ · ∥ designates the operator norm. The subalgebra M ⊂ B(H) is said to be the von Neumann algebra if the second commutant satisﬁes M′′ = M. Given a von Neumann algebra M, there exists the predual Banach space M∗ such that ( M∗)∗ = M due to the Sakai theorem. The corresponding duality is denot ed by ⟨ρ, x ⟩, ρ ∈ M∗, x ∈ M . The functionals on M determined by elements of M∗ are said to be normal. A normal positive functional ρ with the property ⟨ρ, I⟩ = 1 is called a state. The quantum channels can be considered as mappings Φ : B(H1) → B(H2) which can be represented in the Kraus form Φ( ρ) = ∑ k AkρA∗ k, A k : H1 → H 2, where ∑ k AkA∗ k = I. The non-commutative operator graph corresponding to the ch annel is V = span {A∗ kAj } . The subspace K ⊂ H 1 is a quantum error-correcting code if the orthogonal projec tion PK satisﬁes the Knill-Laﬂamme condition dim PK VPK = 1. Such projection PK is called quantum anticlique. Suppose that M(1) ⊂ B(H1) and M(2) ⊂ B(H2) are two von Neumann algebras acting in the Hilbert spaces H1 and H2. Denote ⟨·, ·⟩1, 2 the corresponding dualities. Given a linear map Φ : M(1) ∗ → M (2) ∗ , one can deﬁne the conjugate map Φ ∗ : M(2) → M (1) by the rule ⟨ρ, Φ ∗(x)⟩1 = ⟨Φ( ρ), x ⟩2 , ρ ∈ M (1) ∗ , x ∈ M (2). Following [7], the map Φ is said to be a generalized quantum ch annel if Φ ∗ is unital and completely positive. Let (Ω , B, ν ) be a measurable space with the σ-ﬁnite measure ν. Then M = B(H) ⊗ L∞(Ω) is a W ∗-algebra of operators acting in the Hilbert space ˜H = H ⊗ L2(Ω) and having the predual space M∗ = T (H) ⊗ L1(Ω). Also note that M can be viewed as L∞(Ω → B(H)), the space of ν-essentially bounded B(H)-valued functions. Put M(1) = B(H) and M(2) = B(H) ⊗ L∞(Ω) such that M(1) ⊂ B(H) and M(2) ⊂ B(H ⊗ L2(Ω)). Then, the quantum channel Φ : M(1) ∗ → M (2) ∗ is characterized by the property ⟨Φ( ρ), I H⊗L2(Ω) ⟩2 = ⟨ρ, I H⟩1 = 1 (1) for all states ρ ∈ M (1) ∗ . Thus, Φ( ρ) ∈ M (2) ∗ is also a state. Since Φ( ρ) is a function fρ on the space Ω taking values in T (H) the equality (1) can be rewritten in the form ∫ Ω Tr (fρ(ω ))ν(dω ) = 1 . Let M be a positive operator-valued measure on (Ω , B, ν ) with values in the set of positive operators B(H)+. Theorem 1 Suppose that there is an operator valued density P (ω ), ω ∈ Ω of M with respect to the measure ν such that M(dω ) = P (ω )ν(dω ).3 Then, the formula Tr (ρΦ ∗(x ⊗ f)) = ∫ Ω f(ω )Tr ([P (ω )]1/ 2ρ[P (ω )]1/ 2x)ν(dω ) determines a unital normal completely positive map Φ ∗ : M(2) → M (1). Proof Let Ω = ∪N k=1Bk be the partitioning of Ω into a sum of disjoint Bk ∈ B and a simple function f0 ⏐ ⏐ BK = ak. Deﬁne a unital normal completely positive map Φ ∗ N : M(2) → M (1) as follows Φ ∗ N (x ⊗ f) = ∑ k ak ∫ Bk [P (ω )]1/ 2x[P (ω )]1/ 2ν(dω ). (2) So, Tr (ρΦ ∗(x ⊗ f0)) = ∑ k ak ∫ Bk Tr ([P (ω )]1/ 2ρ[P (ω )]1/ 2x)ν(dω ), By this, the positivity of all operators under the trace resu lts in |Tr (ρΦ ∗(x ⊗ f0))| ≤ ∥ f0∥L∞ ∫ Ω ∥ρ∥ · ∥ x∥ · Tr (P (ω ))ν(dω ) = ∥f0∥L∞ · ∥ρ∥ · ∥ x∥, approaching f ∈ L∞ (Ω) by simple functions not disrupt that inequality. So Φ ∗ is a limit of (2) in the weak* topology. □ The POVM M generates some non-commutative graph V = span(M(B), B ∈ B), where B is the σ-algebra of measurable subsets B ⊂ Ω. Let us deﬁne a unital completely positive map ˆΨ ∗ : L∞(Ω) → M (1) by the formula ˆΨ ∗(f) = Φ ∗(I ⊗ f), f ∈ L∞(Ω) . (3) Theorem 2 The channel Ψ complementary to ˆΨ deﬁned by Eq. (3) determines the graph V. Proof The action ˆΨ ∗ : L∞(Ω) → M (1) can be represented as follows ˆΨ ∗(f) = ∫ Ω f(ω )P (ω )ν(dω ) It suﬃces to show that V = ˆΦ ∗(L∞(Ω)) [6]. The result immediately follows from the equality ˆΦ ∗(χ B) = M(B), where χ B ∈ L∞(Ω) is the indicator function of the measurable set B ∈ B. □4 III. A QUBIT INTERACTING WITH QUANTUM OSCILLATOR We consider a qubit interacting with quantum oscillator wit hin the rotating wave approximation. This model is known to have an explicit description of the eig enstates and eigenvalues which completely deﬁne the model [5]. Let Hf be the Hilbert space with the basis {|k⟩, k ∈ N0} (the quantum oscillator Hilbert space) and Hs the two-dimensional Hilbert space with the basis {|g⟩, |e⟩} (qubit space). The Hilbert space of the composite system is H = Hf ⊗ Hs. The Hamiltonian is H = ω f a+a− + ω s 2 σz + κ 2 (σ−a+ + σ+a−), (4) Here ω s, ω f ∈ R+ are the frequencies of the qubit and the quantum oscillator, respectively, κ ≥ 0 is the coupling constant, σz is the Pauli matrix, σ+, σ − are the rising and lowering operators of the qubit and the a+, a − are the creation and annihilation operators of the oscillat or. The detuning parameter is ∆ = ω f − ω s. For the non-resonant case ∆ ̸= 0, the eigenstates of the Hamiltonian are |0, g ⟩, |n, +⟩ = cos ( θn 2 ) |n − 1, e ⟩+ sin ( θn 2 ) |n, g ⟩, |n, −⟩ = sin ( θn 2 ) |n − 1, e ⟩ − cos ( θn 2 ) |n, g ⟩, where θn = tan −1(κ√n/ ∆) and n ∈ N. For the resonant case ∆ = 0 the eigenstates are |0, g ⟩, |n, +⟩ = |n − 1, e ⟩+ |n, g ⟩, |n, −⟩ = |n, g ⟩ − |n − 1, e ⟩. In both cases the corresponding eigenenergies are E0,g = ω f + ∆ 2 En, ± = ω f ( n − 1 2 ) ± 1 2 √ ∆ 2 + κ2n, n ∈ N. Our construction can be applied to this model of a qubit inter acting with quantum oscillator. Let us split the Hilbert space H into three parts [4], H = H1 ⊕ H2 ⊕ H3, The partition is determined by the parameter K0 ≥ max{3, M 0}, where M0 is the minimal natural solution of the inequality ( √ ∆ 2 + κ2(M0 + 1) + √ ∆ 2 + κ2M0 ) −1 < 2ω f κ2 . (5) The subspaces H1 and H2 are the inﬁnite-dimensional subspaces corresponding to tw o strictly increasing sequences of eigenvalues Jk = Ek+1, +, k ∈ N0 and Sk+K0 = Ek+K0, −, k ∈ N0. The subspaces are deﬁned as follows H1 = span{|n, +⟩, n ∈ N}, H2 = span{|n, −⟩, n ≥ K0}, H3 = span{|g, 0⟩} ∪ {|n, −⟩, 1 ≤ n < K 0}.5 The sequences Jk and Sk+K0 allow to deﬁne Gauzeau-Klauder coherent states in H1 and H2 |J, x, y ⟩ = 1 N1(x) +∞∑ k=0 xk/ 2e−iJky √ c(1) k |k + 1, +⟩ |S, x, y ⟩ = 1 N2(x) ∞∑ k=0 xk/ 2e−iSk+K0 y √ c(2) k |k + K0, −⟩. Here sequences c(2) k are the positive converging weights, N1(x) and N2(x) are the normalization factors. Let Ω = R2 ⊕ R2 ⊕ { pt}, where {pt} is the set containing only one point. Deﬁne the POVM over Ω as follows M = M1 ⊕ M2 ⊕ M3, where M1(dxdµ(y)) = |J, x, y ⟩ ⟨J, x, y | τ1(x)dxdµ(y), M2(dxdµ(y)) = |S, x, y ⟩ ⟨S, x, y | τ2(x)dxdµ(y), and M3(∅) = 0 , M 3({pt}) = P3, Here P3 is the projection on H3 and measures τ1(x), τ1(x) are determined by the Gauzeau-Klauder construction [5]. The POVM M is generated by orbits of the unitary group Ut = e−itH with the Hamiltonian (4) and satisﬁes the conditions of Theorems 1 an d 2. The corresponding graph has the quantum anticlique P3. IV. CONCLUSION Based on the theory of non-commutative operator graphs, we a nalyze the error correction model for a qubit interacting with quantum oscillator. The d ynamics of the composite system is governed by Schr¨ odinger equation which generates POVM. We describe the method of how to deﬁne the quantum channel which corresponds to a non-commut ative operator graph generated by the POVM. We analyze this construction for the model of a qu bit interacting with quantum oscillator and provide an explicit expression for the quant um anticlique which determines for this model the error correcting subspace. Acknowledgments The work is performed in Steklov Mathematical Institute of R ussian Academy of Sciences within the project of the Russian Science Foundation 17-11-01388. [1] G.G. Amosov, “On general properties of non-commutative oper ator graphs,” Lobachevskii Journal of Mathematics, 39 (3), 304–308 (2018).6 [2] G.G. Amosov, A.S. Mokeev, “On errors generated by unitary dyn amics of bipartite quantum systems,” Lobachevskii Journal of Mathematics, 41 (12), 2310–2315 (2020). [3] G.G. Amosov, A.S. Mokeev, A.N. Pechen, “Non-commutative grap hs and quantum error correction for a two-mode quantum oscillator,” Quantum Information Processing, 19, 95 (2020). [4] G.G. Amosov, A.S. Mokeev, A.N. Pechen, “Noncommutative graph s based on ﬁnite-inﬁnite system couplings: Quantum error correction for a qubit coupled to a coher ent ﬁeld,” Physical Review A, 103, 042407 (2021). [5] J. P. Gazeau, J. R. Klauder, “Coherent states for systems wit h discrete and continuous spectrum,” Journal of Physics A 32, 123 (1999). [6] M.E. Shirokov, T. Shulman, “On superactivation of zero-error c apacities and reversibility of a quantum channel,” Comm. Math. Phys., 335 (3), 1159–1179 (2015). [7] A. Barchielli, G. Lupieri, “Instruments and mutual entropies in qu antum information,” Banach Center Publications, 73, 65–80 (2006). [8] S. Sakai, “ C∗ -Algebras and W ∗ -algebras” (Springer, Berlin, 1971).",
      "references": [
        "On general properties of non-commutative operator graphs,",
        "On errors generated by unitary dynamics of bipartite quantum systems,",
        "Non-commutative graphs and quantum error correction for a two-mode quantum oscillator,",
        "Noncommutative graphs based on finite-infinite system couplings: Quantum error correction for a qubit coupled to a coherent field,",
        "Coherent states for systems with discrete and continuous spectrum,",
        "On superactivation of zero-error capacities and reversibility of a quantum channel,",
        "Instruments and mutual entropies in quantum information,",
        "C∗ -Algebras and W∗ -algebras"
      ],
      "meta_data": {
        "arxiv_id": "2403.06733v1",
        "doi": "10.1134/S1995080221100036",
        "authors": [
          "G. G. Amosov",
          "A. S. Mokeev",
          "A. N. Pechen"
        ],
        "published_date": "2024-03-11T13:55:03Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the inverse problem in non-commutative operator graph theory: given a POVM/operator graph (possibly infinite-dimensional), construct a corresponding (generalized) quantum channel and apply it to an error-correction model of a qubit coupled to a quantum oscillator. Main results: (i) a construction of a normal unital completely positive map (Heisenberg picture) Φ* from an operator-valued density of a POVM using von Neumann algebra preduals; (ii) proof that the complementary channel to the induced commutative subchannel Ψ* generates exactly the non-commutative graph spanned by POVM effects; (iii) for the Jaynes–Cummings (rotating-wave) qubit–oscillator model, an explicit POVM built from Gazeau–Klauder coherent states yields a graph whose quantum anticlique (error-correcting projection) is explicitly P3 onto a designated subspace H3.",
        "methodology": "Uses von Neumann algebra framework: generalized quantum channels as maps between preduals M(1)*→M(2)* whose conjugate Φ*:M(2)→M(1) is unital and completely positive. For a POVM M(dω)=P(ω)ν(dω) on (Ω,𝓑,ν) with operator-valued density P(ω), defines Φ* on simple tensors x⊗f by integrating the sandwich map [P(ω)]^{1/2} x [P(ω)]^{1/2} weighted by f(ω), extends by weak* limits from simple functions. Defines Ψ*:L∞(Ω)→M(1) by restricting Φ* to I⊗f and uses complementary channel Ψ to associate the operator graph V=span{M(B)}. Application: constructs a POVM for the qubit–oscillator Hamiltonian by decomposing Hilbert space into H1⊕H2⊕H3 via spectral sequences, then defining POVM components M1,M2 from Gazeau–Klauder coherent-state projectors over R^2×R^2 and M3 as a point-mass giving projection P3.",
        "experimental_setup": "No numerical experiments. Validation is mathematical/analytic: theorems establishing complete positivity, normality, and unitality of Φ*; proof that the complementary channel determines the graph V. For the physical model, specifies Hamiltonian (rotating-wave/Jaynes–Cummings), eigenstates/eigenenergies for resonant/non-resonant cases, chooses K0 satisfying inequality (5) to separate spectra into monotone sequences, defines coherent states with normalization/weights per Gazeau–Klauder, defines measurable space Ω=R^2⊕R^2⊕{pt} with σ-finite measure ν=dx dμ(y) on each R^2 component, and constructs the POVM M=M1⊕M2⊕M3 that satisfies assumptions of Theorems 1–2. Error-correcting subspace certified by Knill–Laflamme/anticlique condition with projector P3.",
        "limitations": "Relies on existence of an operator-valued density P(ω) with respect to a σ-finite measure ν (not all POVMs admit such a density). Construction is formulated in an abstract von Neumann algebra setting; explicit Kraus/Stinespring realizations may be non-constructive in practice. The qubit–oscillator application assumes rotating-wave approximation and uses a spectral partition parameter K0 constrained by inequality (5); applicability depends on model parameters (Δ,κ,ωf). The resulting anticlique is tied to the specific POVM built from Gazeau–Klauder coherent states and the chosen decomposition H1,H2,H3; robustness to perturbations/noise models beyond unitary dynamics is not analyzed. No performance metrics (rates, fidelities) or comparisons to alternative codes/channels are provided.",
        "future_research_directions": "Extend the channel-from-POVM/graph construction to POVMs without densities (singular measures) or more general instruments; provide explicit Stinespring/Kraus representations and minimal dilations for the constructed generalized channels. Study robustness under perturbations of Hamiltonian, beyond rotating-wave approximation, and inclusion of dissipative/open-system dynamics (Lindbladians) generating non-unitary POVMs/graphs. Explore optimization of the decomposition (choice of K0 and subspaces) to enlarge anticliques or improve code rates; analyze zero-error capacities/superactivation for the resulting infinite-dimensional channels. Apply the framework to other hybrid finite–infinite systems (multi-level atoms, multi-mode oscillators) and to practical continuous-variable error correction with experimentally accessible measurements.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "System Prompt Optimization with Meta-Learning",
      "full_text": "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach Zhuang Li, Levon Haroutunian, Raj Tumuluri, Philip Cohen, Gholamreza Haffari Openstream.ai {zhuang.li, levon, raj, phil.cohen, reza.haffari}@openstream.com Abstract Post-editing has proven effective in improv- ing the quality of text generated by large lan- guage models (LLMs) such as GPT-3.5 or GPT- 4, particularly when direct updating of their parameters to enhance text quality is infeasi- ble or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs’ ability to generalize across do- mains. Moreover, the editing strategies in these methods are not optimally designed for text- generation tasks. To address these limitations, we propose a neural programmer-interpreter ap- proach that preserves the domain generalization ability of LLMs when editing their output. The editing actions in this framework are specif- ically devised for text generation. Extensive experiments demonstrate that the programmer- interpreter significantly enhances GPT-3.5’s performance in logical form-to-text conversion and low-resource machine translation, surpass- ing other state-of-the-art (SOTA) LLM post- editing methods in cross-domain settings. 1 Introduction Large pre-trained language models like GPT-3.51 or GPT-42 have gained significant attention in nat- ural language research. However, fine-tuning these models for specific tasks is challenging due to lim- ited computational resources or inaccessible pa- rameters. Consequently, many researchers resort to using web APIs for instructing LLMs, leveraging zero-shot or few-shot in-context learning, enabling the LLMs to tackle tasks they weren’t explicitly trained for. Unfortunately, this approach falls short when tackling some low-resource sequence genera- tion tasks in machine translation (MT), and logical form (LF)-to-text translation, as shown in Lai et al. (2023); Haroutunian et al. (2023). In such cases, minimal task-specific data was available during the 1https://platform.openai.com/docs/models/gpt-3-5-turbo 2https://platform.openai.com/docs/models/gpt-4-and-gpt- 4-turbo original ﻝﻧﺷﯨﺗ  ٕہﭨﺳﻳﭨ  ِہﺗ  ِﻬﭼ  ، ﻬﺗِﻳ  ﯽﻧٔﮐ  ﻥَﺯ ﻥﯨﺍ   ﯽﻳﺁ  یڈ  ﺱﻧﻳﺭﭨﻧﯨﺍ  ﻡﺍﺯﮑﯨﺍ  generator programmer hypothesis #2 there are also such tests as nid  entrance exams in kashmiri  institutes in a form.  interpreter hypothesis #1 there is a woman who came to  give the national test here . hypothesis #3 there are national tests like the  nid entrance exam , too .  programmer interpreter programmer action set #1 DELETE woman DELETE who INSERT nid ... action set #2 DELETE such DELETE kashmiri DELETE institutes ... Figure 1: The diagram of our post-editing architecture. LLMs’ pre-training phase. The output quality of LLMs for such tasks is compromised due to the absence of task-specific knowledge. To address this challenge, a promising set of solutions suggests integrating task-specific knowl- edge into language models through post-editing the generated text using a smaller model fine-tuned on task-specific data. Yet, these methods are not without their drawbacks. Our findings indicate that exclusive reliance on a smaller model for editing, e.g. Self-Correct (Welleck et al., 2022), results in suboptimal performance in domain generaliza- tion scenarios, likely due to the inherently limited domain knowledge within these smaller models. As LLMs (i.e. GPT-3.5 or GPT-4) have shown superior domain generalization ability (Wang et al., 2023; Yang et al., 2023) over the fine-tuned model, we introduce an innovative approach based on the programmer-interpreter framework (Reed and de Freitas, 2016), which benefits from the domain generalization ability from LLMs. The program- arXiv:2402.04609v1  [cs.CL]  7 Feb 2024mer component - a smaller language model fine- tuned on task-specific data - delivers precise edit in- structions to the larger language model, thus infus- ing the large model with task-specific knowledge. The interpreter, in turn, edits the large model’s out- put given the provided instructions. Contrary to the Self-Correct (Welleck et al., 2022) approach that utilizes smaller, fine-tuned models for edit- ing, our interpreter is also an LLM. The editing is accomplished through the use of prompts that in- clude editing instructions, eliminating the need for any additional fine-tuning. This distinct framework guarantees the preservation of the LLM’s domain generalization ability while simultaneously benefit- ing from the task-specific knowledge encoded by the programmer. Our method distinguishes itself from approaches like PiVe (Han et al., 2023), which also employ an LLM as the interpreter but focus on graph generation tasks. In contrast, our approach specifically designs word-level editing actions in the instructions, tailored to enhance text generation. This targeted strategy renders our method more effective for text-generation tasks. Overall, our key contributions are as follows: • We introduce a novel programmer-interpreter method that enhances LLM in low-resource cross-domain text generation tasks. This ap- proach capitalizes on the programmer’s ability to encode task-specific knowledge and the in- terpreter’s prowess in domain generalization. • We design editing operations optimized for text generation tasks, leading to substantial text quality improvements by simply prompt- ing the LLMs with action instructions. • In scenarios where training and test data span different domains, our comprehensive empir- ical studies confirm that the method outper- forms all existing LLM post-editing baselines in low-resource MT and LF-to-Text. 2 Programmer-Interpreter Approach The objective in LF-to-text and MT tasks using LLMs is to generate a high-quality output text y, denoted as y′ = arg maxy∈Y P(y|x, C), given an input x (e.g., LF, source-language utterance) and an exemplar pool C = {(xj, yj, y∗ j , a∗ j )}|C| j=1. Here, xi and yj are the ground truth input-output pairs, y∗ j is the imperfect translation of xi, and a∗ j rep- resents the Oracle edit actions that can modify y∗ j into yj. Our approach focuses on achieving high- quality generation through iterative refinement of the initial output text produced by an LLM. Specif- ically, the iterative refinement framework includes three-parameterized modules: a Generator, a Pro- grammer, and an Interpreter,3 P(yt|x, C) = Generator z }| { P(y0|x, M(·)) × (1) t−1X {a,y} t−1Y i=0 ( Interpreter z }| { P(yi+1|ai, yi, x, A(·)) × Programmer z }| { P(ai|yi, x)) (2) The Generator corresponds to the LLM (e.g. GPT- 3.5, GPT-4). It produces the initial output text, y0, given the input x, a set of examples retrieved by the function M(x, C) when performing in-context learning. The Programmer, a module that cre- ates editing actions ai given x and the current imperfect output yi, is a pre-trained Sequence-to- Sequence (Sutskever et al., 2014) language model, such as mT5 (Xue et al., 2021) or flan-T5 (Chung et al., 2022), fine-tuned on a synthetic dataset. The Interpreter, essentially also an LLM, refines the imperfect intermediate output yi by processing in- structions that incorporate predicted editing actions and few-shot editing examples, retrieved via the function A(x, C). Please note that the Programmer has much fewer parameters than the LLM used by the Generator and Interpreter. After several itera- tive refinements, we arrive at the final output yt generated by the LLM. During generation, we as- sume no access to the parameters of the LLMs but only obtain the output text by providing prompting instructions. The implementation details of each module are as follows: Generator. To generate the initial output, we sup- ply a prompt composed of a few-shot set of exem- plar pairs, denoted as M(x, C) = {(xj, yj)}m j=1, selected from a pool of reference pairs C. This is accompanied by an instruction prompting the LLM to produce output y0 based on the input x. The retrieval function identifies the closest pairs by cal- culating the cosine similarity of TF-IDF features between x and other instances of x in C. Programmer. After obtaining the initial or inter- mediate output yi from either the Generator or the Interpreter, we combine the input x and yi into a single sequence and feed it to the Programmer to generate a sequence of edit actions ai. We create a 3To save space, we simplify the marginalization notation.synthetic training set T , extracted from the exam- ple pool C, for fine-tuning the Programmer. Each pair in T is defined as (xconcat, a∗), where xconcat is the concatenated sequence of x and y∗, serving as the input for the Programmer. The output a∗ is the sequence of Oracle edit actions, synthetically generated based on the reference pairs in C. For each reference y ∈ C, we calculate the word-level edit distance to the imperfect translation y∗, gener- ating intermediate edit actions. OnlyINSERT-word and DELETE-word actions are retained in the se- quence, forming the final training sequence a∗ for the Programmer. If y∗ is identical to the reference y, the action is labeled as “NoAction”, indicating that no refinement is needed for that instance. Un- like PiVe, which generates the imperfect translation y∗ by scrambling the originaly, we directly use the initial output y0 from the Generator as y∗ in both C and T . This approach enables the Programmer to learn an action distribution that more effectively corrects translation errors from LLMs. In prac- tice, to address the rarity of “NoAction” instances, we supplement these cases by creating augmented pairs, each consisting of two identical y sequences. Interpreter. To edit the intermediate output yi, we engage the LLM in the Interpreter role by pro- viding it with prompting instructions. Given the edit instructions ai and a pair (yi, x), the LLM can INSERT or DELETE words in order to gen- erate the modified text yi+1. We also incorpo- rate a few-shot examples that demonstrate edit- ing procedures, extracted from C and denoted as A(x, C) ={(xj, yj, y∗ j , a∗ j )}n j=1. These examples are selected based on the cosine similarity between the TF-IDF features of x and those in C. Further- more, to mimic action prediction errors from the Programmer, we adopt an adversarial in-context learning strategy, similar to the approach in Zhuo et al. (2023). This involves corrupting the action sequence by deleting Oracle actions with a cer- tain probability d%. If an action is not deleted, we swap it with other actions from C at the same probability d%. Through this manipulation, we have discovered that the LLM’s exceptional text generalization ability enables it to effectively com- prehend the editing instructions. As a result, it can generate high-quality text after performing the necessary edits, even if the predicted actions from the Programmer are not completely accurate. See Figures 2 and 3 in the Appendix for zero/few-shot instruction examples. 3 Experiments Setup. In our experiments, we default to using GPT-3.5-turbo-0301 as the LLM for the Generator in both the zero-shot and few-shot settings. For the Interpreter, we use GPT-3.5-turbo-0301 in the zero-shot setting and GPT-3.5-turbo-16k 4 in the few-shot setting. For the Generator used across all settings and baselines, we consistently use 0 and 5 shots for MT and LF-to-Text, respectively. For the Interpreter in the few-shot setting, we apply 10 and 5 action examples for MT and LF-to-Text, respectively, with a 50% action corruption probabil- ity. For the MT and LF-to-Text tasks, we employ mT5-base and flan-T5-base as the backbones of the Programmers, respectively. These backbone choices are driven by a computationally efficient setup, ensuring the models fit within an Nvidia V100 with 16GB memory. We train our program- mers with a dev set to select the optimal model. Our search for the best learning rate includes [5e-5, 1e-4, 2e-4], while the range of epochs considered is [5, 10, 20], with batch sizes 4. GPTs require no fine-tuning. Each generation of 1096 tokens costs approximately $0.0015. We limit Self-Correct and Self-Refine to five editing iterations, as their perfor- mance typically stabilizes within this range. Con- versely, Prog-Refine and Algo-Refine may require additional iterations for convergence, especially when ‘NoAction’ instances are infrequent. Thus, we continue for up to 15 iterations, ceasing only if over 95% of actions are ‘NoAction’. Datasets. To simulate low-data scenarios, in the context of MT, we utilize a Kashmiri-English dataset from IndicTrans2 (Gala et al., 2023). Since Kashmiri is a notably low-resource language, trans- lating it poses a formidable challenge for LLMs. The dataset provides 26,016 training pairs, which we use to generate synthetic data for action gener- ation. The development set consists of 997 pairs. The dataset includes two distinct test sets, GEN and CONV , with 1,024 and 1,503 pairs, respectively. Each of the training, development, and test sets originates from different domains. For LF-to-Text, we employ the AMR-LDC2.05 dataset, which con- tains 22,550 AMR-English pairs for training and 1,368 pairs for development. For testing, we turn to a separate dataset, Bio-AMR6, which offers 500 4https://platform.openai.com/docs/models/gpt-3-5-turbo 5https://catalog.ldc.upenn.edu/LDC2017T10 6https://amr.isi.edu/download.htmlMT (Kashmiri to English) LF-to-Text (AMR to English) GEN CONV Bio-AMR Method BLEU BERT ChrF++ BLEU BERT ChrF++ BLEU BERT ChrF++ Fine-tuned mT5/flan-T516.58 89.32 41.77 13.19 88.83 33.03 9.27 87.90 41.06 GPT-3.5 Initial 9.21 87.29 34.30 5.92 87.24 26.23 9.63 88.57 43.98 Self-Correct 13.11 89.02 38.98 12.73 89.61 33.76 11.64 89.44 46.05 Algo-Refine 8.40 86.92 39.66 6.29 87.31 32.21 7.72 86.64 43.39 Self-Refine 8.13 86.54 31.78 4.73 86.55 24.13 8.67 87.34 39.63 Prog-Refine (Zero-shot Act.) 13.81 88.58 39.00 12.09 89.41 33.41 11.43 89.30 45.44 Prog-Refine (Few-shot Act.) 16.3290.36 42.44 14.78 90.19 35.48 13.64 89.27 47.69 Prog-Refine (ORACLE) 43.48 92.11 65.29 42.42 93.00 42.42 27.77 90.01 52.86 Table 1: The main results of MT on GEN and CONV test sets, and LF-to-Text on Bio-AMR test set. pairs in a different domain. Likewise, the AMR-to- Text task poses a low-resource challenge for LLMs. Baselines. We evaluate our approach, Prog- Refine, which utilizes zero-shot action exemplars (Zero-shot Act.) and few-shot action exemplars (Few-shot Act.) for Interpreters, against five base- line methods and an ORACLE setting i) Fine-tuned Models include mT5-base for MT and flan-T5-base for LF-to-Text generation, both of which are fine-tuned on the training set consisting of pairs (x, y) ∈ C. These baseline models do not perform any refinement. ii) GPT-3.5 + Initial simply applies the GPT- 3.5 as the Generator to obtain the text without any further refinement. iii) GPT-3.5 + Self-Correct (Welleck et al., 2022) fine-tunes smaller models to be the Inter- preter, fixing the output errors of the large models given the feedback. Here, we supply the edit ac- tions produced by our Programmer as feedback to the fine-tuned Interpreters. These Interpreters are also built upon mT5-base or flan-T5-base. iv) GPT-3.5 + Algo-Refine directly ‘Insert’ or ‘Delete’ specific words in certain positions of the generated text instead of using an Interpreter to rewrite. Therefore, in this baseline, we also ap- ply the Interpreter to predict the indices of words for actions. This method is prevalent in the MT literature; e.g. see Vu and Haffari (2018). v) GPT-3.5 + Self-Refine (Madaan et al., 2023) leverages an LLM to provide feedback for its own output, enabling self-refinement without the need for additional fine-tuning. vi) GPT-3.5 + Prog-Refine (ORACLE)applies the ORACLE actions generated by comparing the reference in the test set with the initial output of the Generator, allowing for optimal refinement after one iteration in the Zero-shot Act. setting. Evaluation Metrics. For LF-to-Text and MT tasks, we utilize three evaluation metrics to assess the quality of the final output text generated by the Programmer-Interpreter framework: BLEU (Pap- ineni et al., 2002), BERTScore (Zhang et al.) and Chrf++ (Popovi´c, 2017). 3.1 Main Results and Analysis Table 1 shows that GPT-3.5 + Prog-Refine notably boosts the Generator’s performance (i.e., GPT-3.5 + Initial), underlining our method’s effectiveness in cross-domain scenarios by enhancing initial GPT- 3.5 outputs. Moreover, the few-shot setting (Few- shot Act.) significantly outperforms both the zero- shot (Zero-shot Act.) setting and all other refine- ment baselines. It’s also noteworthy that apply- ing ORACLE action to our method can lead to a roughly 30-point increase in BLEU score, suggest- ing substantial potential for improvement in our ap- proach. In comparison, Self-Refine shows minimal improvement, possibly due to its limited integration of task-specific knowledge. Algo-Refine inconsis- tently improves the initial text, lacking the robust- ness seen in our method. We note that rewriting Interpreters, as in our approach and Self-Correct, can eliminate invalid actions, thus enhancing edit- ing quality. However, Algo-Refine does not pos- sess this capability and is susceptible to incorrect feedback actions. The Self-Correct method, us- ing a fine-tuned Interpreter, along with fine-tuned mT5/flan-T5 models, demonstrates better perfor- mance than other baselines across various tasks. This underscores the importance of learning task- specific knowledge, especially in low-resource sce- narios. Nonetheless, these methods face significant challenges in cross-domain applications, as further evidenced by our analysis in Table 4. 3.2 Ablation Study Refinement Iterations. In Table 2, we observe that Prog-Refine significantly improves the initial output generated by the Generator. However, it only demonstrates marginal improvements in theMT (Kashmiri to English) #Iter BLEU BERT ChrF++ NoAct% Iter 0 5.92 87.24 26.23 17.70 Iter 1 11.01 89.18 33.05 79.71 Iter 2 11.87 89.36 33.41 90.67 Iter 3 12.09 89.41 33.41 95.28 Iter 4 12.26 89.45 33.43 97.21 Iter 5 12.36 89.47 33.39 - Table 2: The influence of 5 iterations on main results of MT using Prog-Refine (Zero-shot Act.) on CONV test set. NoAct%: The percentage of utterances requiring no refinement, as indicated by ‘NoAction’. MT (Kashmiri to English) BLEU BERT ChrF++ Initial 5.92 87.24 26.23 Edit: DEL, INS 12.36 89.47 33.39Edit: DEL 12.27 89.42 33.21Edit: INS 12.18 89.45 33.42 Unordered: DEL, INS 7.12 87.86 29.21Unordered: DEL 6.52 87.51 26.46Unordered: INS 7.14 88.04 30.38 Table 3: The results of MT using Prog-Refine (Zero-shot Act.) on CONV test set at 5th iteration with different types of actions. Edit: Actions are generated based on edit distance. Unordered: Actions without any specific order. INS: Insertion. DEL: Deletion. subsequent outputs from the Interpreter, even after four additional iterations. We hypothesize that this limited improvement may be attributed to training the model solely on synthetic data generated by the Generator, so the action distribution might be different to the ones for modifying the output of the Interpreter in the subsequent iterations. Action Types. We further examine the impact of solely utilizing one type of action and the in- fluences of disregarding the sequence of these ac- tions. In the setting with unordered actions, oracle actions are generated by simply contrasting the dif- ferences within two sentences’ unordered sets of words. As depicted in Table 3, the Delete and In- sert actions, when used individually, can deliver performance metrics on par with when they are combined. However, ignoring the order of actions can lead to a substantial decline in the refinement performance. This highlights that LLM editing methods like PiVe, which utilize unordered inser- tions, are not optimally suited for our tasks. Further analysis is in Appendix A.5. Domain Discrepancy. As shown in Table 4, a domain shift dramatically impacts the performance of flan-T5 and Self-Correct. While both baseline models show markedly superior performance on the in-domain test set relative to our model, ours either surpasses or equals their performance in the LF-to-Text (AMR to English) Method BLEU BERT ChrF++ Fine-tuned flan-T5 34.63 95.05 66.97 GPT-3.5 Initial 19.67 92.10 55.98 Self-Correct 34.49 94.68 66.81 Self-Refine 16.16 91.08 52.78 Prog-Refine 29.12 94.01 64.85 Table 4: LF-to-Text results using Prog-Refine (Zero- shot Act.) on the in-domain LDC test. LF-to-Text (AMR to English) Rate BLEU BERT ChrF++ 0.0 12.06 89.31 46.23 0.2 12.35 89.36 46.49 0.5 13.64 89.27 47.69 1.0 11.97 89.32 46.13 Table 5: LF-to-Text results using Prog-Refine (Few-shot Act.) vary with different corruption probabilities for the action sequence in the adversarial in-context examples used for the Interpreter. cross-domain MT and AMR-to-Text test sets. This disparity in performance is likely due to the smaller models’ limited cross-domain generalization. Sim- ilarly, in MT tasks, our preliminary experiments show that fine-tuned mT5 achieves 30 points of BLEU on the in-domain test but only 16 and 13 on out-of-domain tests. For further details on domain discrepancies, see Appendix A.3. Adversarial In-context Learning. Table 5 in- dicates 0.0 for no corruption and 1.0 for com- plete discarding of exemplar actions, leaving only (xj, y∗ j , yj)n j=1. Rates between 0.0 and 1.0 rep- resent partial corruption of Oracle actions. The results suggest that neither full application nor total corruption of Oracle actions is optimal. However, partial corruption leads to improved performance. Additionally, across all corruption rates, few-shot settings consistently outperform zero-shot settings. 4 Conclusions We present a programmer-interpreter method that iteratively refines LLM outputs using edit actions from a fine-tuned programmer and an LLM inter- preter. Our approach combines the task-specific encoding capacity of a fine-tuned model with the domain generalization strength of the LLM, incor- porating specifically designed actions for text gen- eration. The experiments confirm its efficacy, show- ing significant improvements in LLM-generated text quality for low-resource MT and LF-to-Text tasks. Moreover, our approach outperforms estab- lished baselines in cross-domain scenarios.5 Limitations This work has two primary limitations. First, in in-domain tests, our approach does not outperform smaller models, such as mT5 and flan-T5. Consid- ering the performance improvements we observed when using ORACLE actions, we believe there is substantial potential to further enhance our method for text generation in the in-domain evaluation set- ting. Second, our approach requires internet trans- mission of prompt instructions to the servers of ChatGPT. This could potentially lead to a risk of privacy leakage, which is a critical concern in data- sensitive applications. References Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Jay Gala, Pranjal A Chitale, Raghavan AK, Sumanth Doddapaneni, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, et al. 2023. Indictrans2: Towards high- quality and accessible machine translation models for all 22 scheduled indian languages. arXiv preprint arXiv:2305.16307. Jiuzhou Han, Nigel Collier, Wray Buntine, and Ehsan Shareghi. 2023. Pive: Prompting with iterative verifi- cation improving graph-based generative capability of llms. arXiv preprint arXiv:2305.12392. Levon Haroutunian, Zhuang Li, Lucian Galescu, Philip Cohen, Raj Tumuluri, and Gholamreza Haffari. 2023. Reranking for natural language generation from logi- cal forms: A study based on large language models. arXiv preprint arXiv:2309.12294. Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond en- glish: Towards a comprehensive evaluation of large language models in multilingual learning. arXiv preprint arXiv:2304.05613. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computa- tional Linguistics, pages 311–318. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaïd Harchaoui. 2021. MAUVE: measuring the gap be- tween neural text and human text using divergence frontiers. In Advances in Neural Information Pro- cessing Systems (NeurIPS), pages 4816–4828. Maja Popovi´c. 2017. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612–618. Scott E. Reed and Nando de Freitas. 2016. Neural programmer-interpreters. In International Confer- ence on Learning Representations (ICLR). Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27. Thuy Vu and Gholamreza Haffari. 2018. Auto- matic post-editing of machine translation: A neural programmer-interpreter approach. In Proceedings of the 2018 conference on empirical methods in natural language processing, pages 3048–3053. Jindong Wang, HU Xixu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, et al. 2023. On the robustness of chatgpt: An adversarial and out-of- distribution perspective. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models. Sean Welleck, Ximing Lu, Peter West, Faeze Brah- man, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 483–498. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in prac- tice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein- berger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations. Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan- Fang Li. 2023. On robustness of prompt-based se- mantic parsing with large pre-trained language model: An empirical study on codex. In Proceedings of the 17th Conference of the European Chapter of the As- sociation for Computational Linguistics, pages 1090– 1102.A Appendix A.1 Prompt Example for Editing Text Figures 2 and 3 depict the exemplary zero/few-shot prompt employed in LF-to-Text. You are a AMR translator and you are proficient with both AMR and English. You  are  given  the  following  AMR  logical  form: (  q  /  quote-01  :  arg0  (  r  /  report  )  :  arg2  (  a2  /  and  :  op1  (  g  /  government-organization  :  arg0-of  (  g4  /  govern-01  :  arg1  (  c  /  country  :  wiki  ``  greece  ''  :  name  (  n2  /  name  :  op1  ``  greece  ''  )  )  )  )  :  op2  (  g2  /  government-organization  :  arg0-of  (  g5  /  govern-01  :  arg1  (  c2  /  country  :  wiki  ``  turkey  ''  :  name  (  n4  /  name  :  op1  ``  turkey  ''  )  )  )  )  :  op3  (  g3  /  government-organization  :  arg0-of  (  g6  /  govern-01  :  arg1  (  c3  /  country  :  wiki  ``  belarus  ''  :  name  (  n6  /  name  :  op1  ``  belarus  ''  )  )  )  )  )  :  arg3  (  a  /  acknowledge-01  :  arg0  a2  :  arg1  (  m  /  miss-02  :  arg0  a2  :  arg1  (  d  /  deadline  )  )  )  )  You  are  given  the  following  English  translation: the  report  quotes  the  governments  of  greece  ,  turkey  and  belarus  acknowledging  that  they  missed  the  deadline  . Please  improve  the  above  English  translation  using  the  following  edit  rewriting  actions: DELETE  :  quotes INSERT  :  quoted INSERT  :  as DELETE  :  they  ... INSERT  :  missed Please  only  show  the  English  sentence: Figure 2: The zero-shot exemplary prompt for LF-to- Text. Here are the edit rewriting examples: ###  Example  1: You  are  a  AMR  translator  and  you  are  proficient  with  both  AMR  and  English. You  are  given  the  following  AMR  source  logical  form: (  i  /  increase-01  :  arg1  (  e  /  express-03  :  arg2  (  p  /  protein  )  )  :  arg2  (  p2  /  product-of  :  op1  10  )  :  arg1-of  (  s  /  statistical-test-91  :  arg2  (  l  /  less-than  :  op1  0.05  )  )  ) You  are  given  the  following  English  translation: there  was  a  statistically  significant  increase  in  protein  expression  (  10  fold  ,  p  <  0.05  )  . Please  improve  the  above  English  translation  using  the  following  edit  rewriting  actions: DELETE  \"was\"  from  the  translation DELETE  \"significant\"  from  the  translation DELETE  \"fold\"  from  the  translation DELETE  \"increase\"  from  the  translation Please  provide  a  fluent  English  sentence  that  is  semantically  equivalent  to  the  AMR  logical  form  after  editing  its  corresponding  English  translation. Improved  English  sentence: protein  expression  increased  10-fold  (  p  <  0.05  )  . ###  Example  2:     ... ###  Example  3:     ... ###  Example  4:     ... Figure 3: The few-shot exemplary prompt for LF-to- Text. A.2 Adaption of Self-Corrector In our experiment, we adapted the implementation of the Self-Corrector to better suit our specific re- quirements. To customize it for our context, we constructed the training set for the Self-Corrector’s Interpreter as follows: the input consists of a con- catenation of Kashrimi/AMR, text produced by the splits compared KL-div ↓ MAUVE ↑ train, dev 2.23 0 .006 dev, testgen 1.97 0 .231 train, testgen 1.94 0 .005 dev, testconv 2.97 0 .040 train, testconv 2.98 0 .007 Table 6: Measures of domain difference across dif- ferent splits of the machine translation datasets. KL- divergence scores are calculated for the English sen- tences in each data split, with additive smoothing (α = 1 × 10−4). For MAUVE, 5000 sentences are sampled from the training set. splits compared KL-div ↓ MAUVE ↑ train, dev 2.00 0 .512 dev, testi.d. 2.39 0 .327 train, testi.d. 1.97 0 .342 dev, testbio 6.01 0 .004 train, testbio 5.48 0 .004 Table 7: Measures of domain difference across different splits of the AMR dataset. KL-divergence scores are calculated for the English sentences in each data split, with additive smoothing (α = 1× 10−4). For MAUVE, 5000 sentences are sampled from the training set. Generator, and edit actions. The output, on the other hand, is the ground truth text. For a fair com- parison with our approach and to minimize training and data collection expenses, models are trained only during the first iteration. Additionally, the gen- eration of the training set solely utilizes text from the Generator in the initial iteration, without using text from the Interpreter in subsequent refinement iterations. A.3 Measures of Domain Discrepancy Tables 6 and 7 present domain discrepancies for the training/development/testing sets for the MT and LF-to-text generation tasks. The domain discrep- ancy measures include the KL-divergence (based on the unigram distributions) and MAUVE (Pil- lutla et al., 2021). KL-divergence scores are higher when two distributions are more different from each other. MAUVE scores, which have a range (0,1), are lower when two distributions are more different from each other. Based on Table 6, we observe that the domain of test-gen is closer to the training set compared to that of the test-conv. This is pronounced in higher KL-divergence and lower MAUVE numbers for the test-conv compared to test-gen, with respect toINSERT DELETE Total MT 33.64 83.73 62.57 NLG 24.52 60.48 44.90 Table 8: The F1 scores of comparing the predicted ac- tions with the ORACLE actions in the GEN test set. LF-to-Text (AMR to English) BLEU BERT ChrF++ GPT-3.5-turbo-16k 11.43 89.30 45.44 GPT-4-turbo 11.72 89.36 45.58 Table 9: LF-to-Text results of Prog-Refine (Zero-shot Act.) in zero-shot setting with different LLMs as Inter- preters. the training set. Based on Table 7 , we observe a higher differ- ence for the domain of the biology-AMR test com- pared to the LDC2.0-AMR test set, with respect to the training/development sets of the LDC2.0-AMR dataset. This is pronounced in larger KL diver- gence and lower MAUVE numbers compared to those for the LDC2.0-AMR test set. A.4 F1 Definition for Action Prediction F1 = 2× Pact × Ract Pact + Ract (3) Here, Pact represents action precision, defined as the ratio of predicted actions present in the refer- ence action sequence to the total number of pre- dicted actions. Ract denotes action recall, which is the ratio of predicted actions that appear in the reference action sequence to the total number of ac- tions in the reference sequence. The F1 score, thus, provides a harmonious mean of these two metrics. A.5 F1 for Action Prediction Table 8 reveals that predicting INSERT actions is a relatively easier task compared to predicting DELETE actions. This observation is reasonable since the Programmer only needs to learn how to DELETE words from the text with a fixed vocabu- lary, whereas, for INSERT actions, the Programmer must learn to INSERT arbitrary words. A.6 Comparing GPT-4 and GPT-3.5 as Interpreters Table 9 illustrates the performance differences in the LF-to-Text task when using GPT-4 and GPT- 3.5 as Interpreters for Prog-Refine (Zero-shot Act.). While GPT-4 offers a slight performance boost, the improvement is not substantial, amounting to only a 0.3 increase in BLEU score. Moreover, this comes at a higher cost of 0.06 per 1000 characters, compared to 0.0015 for GPT-3.5.",
      "references": [
        "Scaling instruction-finetuned language models.",
        "Indictrans2: Towards high- quality and accessible machine translation models for all 22 scheduled indian languages.",
        "Pive: Prompting with iterative verifi- cation improving graph-based generative capability of llms.",
        "Reranking for natural language generation from logi- cal forms: A study based on large language models.",
        "Chatgpt beyond en- glish: Towards a comprehensive evaluation of large language models in multilingual learning.",
        "Self-refine: Iterative refinement with self-feedback.",
        "Bleu: a method for automatic evalu- ation of machine translation.",
        "MAUVE: measuring the gap be- tween neural text and human text using divergence frontiers.",
        "chrf++: words helping character n-grams.",
        "Neural programmer-interpreters.",
        "Sequence to sequence learning with neural networks.",
        "Auto- matic post-editing of machine translation: A neural programmer-interpreter approach.",
        "On the robustness of chatgpt: An adversarial and out-of- distribution perspective.",
        "Generating sequences by learning to self-correct.",
        "mt5: A massively multilingual pre-trained text-to-text transformer.",
        "Harnessing the power of llms in prac- tice: A survey on chatgpt and beyond.",
        "Bertscore: Evaluating text generation with bert.",
        "On robustness of prompt-based se- mantic parsing with large pre-trained language model: An empirical study on codex."
      ],
      "meta_data": {
        "arxiv_id": "2402.04609v1",
        "authors": [
          "Zhuang Li",
          "Levon Haroutunian",
          "Raj Tumuluri",
          "Philip Cohen",
          "Gholamreza Haffari"
        ],
        "published_date": "2024-02-07T06:13:14Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes an LLM post-editing framework for low-resource, cross-domain text generation (machine translation and logical-form-to-text) that preserves LLM domain generalization while injecting task-specific knowledge. Introduces a neural programmer–interpreter architecture where a small fine-tuned model (programmer) predicts word-level edit actions and an LLM (interpreter) performs the edits via prompting. Designs text-generation-oriented edit operations (ordered INSERT/DELETE word actions) and shows strong gains over prior post-editing/refinement baselines in cross-domain settings.",
        "methodology": "Three-module iterative refinement: (1) Generator: an API-accessed LLM (e.g., GPT-3.5) produces initial output y0 using in-context examples retrieved by TF-IDF cosine similarity. (2) Programmer: a smaller seq2seq model (mT5-base for MT, flan-T5-base for AMR) fine-tuned on synthetic action data; input is concatenated (x, y*) and output is an ordered sequence of edit actions (INSERT token / DELETE token / NoAction). Oracle actions are derived from word-level edit distance between LLM initial output y0 and reference y; augmented with additional NoAction examples to counter rarity. (3) Interpreter: an LLM prompted with predicted actions and few-shot editing demonstrations to rewrite yi into yi+1; includes adversarial in-context learning by corrupting exemplar action sequences (delete/swap actions with probability d) to improve robustness to imperfect action predictions. Iterates until convergence or high NoAction rate.",
        "experimental_setup": "Tasks: (a) Low-resource MT Kashmiri→English using IndicTrans2; 26,016 train pairs used to generate synthetic action training for programmer; dev 997; two out-of-domain test sets GEN (1,024) and CONV (1,503) from different domains than train/dev/test. (b) LF-to-Text: AMR→English using AMR-LDC2.05 (22,550 train, 1,368 dev) and out-of-domain Bio-AMR test (500). LLMs: GPT-3.5-turbo-0301 as Generator; Interpreter uses GPT-3.5-turbo-0301 (zero-shot) or GPT-3.5-turbo-16k (few-shot); some comparison to GPT-4-turbo as interpreter. Retrieval: TF-IDF cosine similarity for both generation exemplars M(x,C) and action exemplars A(x,C). Shots: Generator uses 0-shot for MT and 5-shot for LF-to-Text; Interpreter few-shot uses 10 action examples (MT) or 5 (LF-to-Text) with 50% action corruption. Programmer training: LR grid {5e-5,1e-4,2e-4}, epochs {5,10,20}, batch size 4; model selection on dev; fits on single V100 16GB. Iterations: Self-Correct/Self-Refine capped at 5; Prog-Refine up to 15, stopping when >95% NoAction. Baselines: fine-tuned mT5/flan-T5 direct generation; GPT-3.5 initial; Self-Correct with fine-tuned small interpreter; Self-Refine (LLM self-feedback); Algo-Refine (explicit insert/delete with index prediction). Metrics: BLEU, BERTScore, ChrF++. Includes ORACLE actions setting (edit actions computed using test references) to estimate upper bound.",
        "limitations": "(1) In-domain performance: does not beat fully fine-tuned smaller models (mT5/flan-T5) on in-domain tests; gains are strongest under domain shift. (2) Action quality bottleneck: large gap to ORACLE actions (≈30 BLEU potential), indicating programmer action prediction errors dominate; programmer trained on synthetic actions from generator outputs may mismatch later-iteration interpreter outputs, limiting gains after the first refinement. (3) Privacy/operational constraint: requires sending prompts and potentially sensitive text to external LLM servers (API), risking data leakage and limiting use in privacy-sensitive settings. (4) Efficiency/cost: iterative prompting increases latency and API cost, and may require more iterations when NoAction is rare.",
        "future_research_directions": "Improve programmer accuracy and reduce oracle gap: better action representations (span-level, phrase/syntax-aware), richer edit operators (replace/reorder), and training on multi-iteration trajectories including interpreter outputs. Jointly optimize retrieval of exemplars and actions (learned retrievers/embeddings) and calibrate/verify actions before execution. Explore stronger/cheaper interpreters or local LLMs; distill interpreter behavior into smaller models to lower cost while retaining generalization. Develop privacy-preserving deployment (on-prem LLMs, prompt redaction, secure enclaves) and study robustness under stricter domain shifts and additional generation tasks (summarization, data-to-text, semantic parsing) and languages.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "AMPO: Automatic Multi-Branched Prompt Optimization",
      "full_text": "Coupled coherent states method for tunneling dynamics: an interpretative study F. Grossmann Institut f¨ ur Theoretische Physik, Technische Universit¨ at Dresden, D-01062 Dresden, Germany E-mail: frank@physik.tu-dresden.de Abstract. Numerical solutions of the time-dependent Schr¨ odinger equation based on the variational principle may offer physical insight that cannot be gained by a solution using fixed grids in position and momentum space. Here we focus on the tunneling dynamics in a quartic double-well and the use of classical, trajectory-guided coherent states to gain insight into the workings of the coupled coherent states method developed by Shalashilin and Child [J. Chem. Phys. 113, 10028 (2000)]. It is shown that over-the-barrier classical trajectories, alone, can accurately describe the tunneling effect. arXiv:2402.13847v1  [quant-ph]  21 Feb 2024Coupled coherent states method for tunneling dynamics: an interpretative study 2 1. Introduction Since several decades, variational methods to solve the time-dependent Schr¨ odinger equation (TDSE) have been used in the chemical physics community [1, 2, 3, 4], where they are employed to study the quantum dynamics of molecular systems with a considerable number of degrees of freedom. Although seminal work on the variational principle is also known in the theoretical physics community [5], related numerical approaches in the time-domain have seen widespread use only more recently, with the advent of matrix product state techniques [6, 7] for the study of entanglement in finite lattice dynamics. In molecular physics, the use of Glauber coherent states [8] with time-dependent parameters is especially appealing, because (i) often the initial states are ground vibrational states of the ground electronic potential energy surface and thus of Gaussian nature and (ii) methods based on Gaussian basis functions allow to gain an understanding of the quantum dynamics by, e.g., restricting the center parameters to follow classical trajectories in phase space. An intriguing example is given by a study on the one-dimensional Morse oscillator model that has been done by Wang and Heller. By using the semiclassical Herman-Kluk (HK) propagator [9], these authors have shown that the quantal revival dynamics present in this system can be understood as a subtle interference phenomenon of classical trajectories that are spread “all over phase space” [10]. Whereas the HK propagator has also proven useful in the study of Bose-Hubbard dynamics for small well numbers [11, 12, 13], its use for the tunneling dynamics in a double well is not appropriate, if a single wavepacket calculation is desired [14]. We will therefore base our numerical investigations on the so-called coupled coherent states (CCS) method by Shalashilin and Child [15]. In this method, the coherent state parameters are still classical, like in the HK case, but the coefficients in the expansion of the wavefunction are determined variationally, in contrast to the HK case, where they again follow from the (linearized) classical dynamics, see [4] for a recent review. The presentation is structured as follows: First in Section 2, we briefly review the Hamiltonian of the quartic double well. In Section 3 we then recapitulate the coupled coherent states method [15] and the modification of the classical Hamiltonian due to normal ordering. In Section 4 we present numerical results for the tunneling dynamics, using suitably tailored rectangular grids of initial conditions in phase space, that allow us to discriminate the power of low as well as of high energy classical trajectories to describe tunneling. A brief summary and outlook are given in the last section. 2. The quartic double-well model We start the main body of the presentation with a brief review of the double-well potential in one dimension, which is a model well suited to study coherent tunneling dynamics. It has many applications, one of the most important ones being the ammonia molecule, first discussed with respect to the tunneling effect by Hund as early as 1927Coupled coherent states method for tunneling dynamics: an interpretative study 3 [16]. A solid state realization is given by a suitably parametrized rf-SQUID, where the role of the coordinate is played by the flux through the ring [17]. More recently bistable potentials have been discussed in cold-atom physics in connection with Bose-Einstein condensation [18]. The effect of an external sinusoidal field on the tunneling is quite counterintuitive, as an appropriately chosen field can lead to a complete stand-still of the tunneling dynamics [19]. For a particle of unit mass, the quartic double-well oscillator is governed by a Hamilton operator of the form H(ˆp, ˆq) = ˆp2 2 + V (ˆq), (1) with the potential function V (q) = −a 2q2 + b 4q4 + EB, a, b ∈ ℜ+. (2) It has quadratic minima at q± = ± qa b and a quadratic maximum at q0 = 0 of height EB = a2 4b . By demanding that the second derivative of the potential at the symmetric minima be unity and the barrier height be D, we fix the parameters to be a = 1/2 and b = 1/(16D). The phase space portrait of the dynamics contains the prototypical separatrix, defined by H(p, q) = D and shaped like the number eight rotated by 90 degrees, as displayed in Fig. 1, as well as a hyperbolic fixed point at p = 0, q= 0 on that separatrix and two elliptic fixed points at p = 0, q= q± [20]. In the figure, we also display the modification of the separatrix due to normal ordering, as explained in Sect. 3. If the available energy is higher than the barrier, a classical particle starting on the right side of the barrier can reach the left one and get back in a periodic fashion. Quantum mechanically, in the usual tunneling scenario, where a Gaussian is sitting at the minimum of one of the two wells initially, with average energy below the barrier, the particle is still moving to the other well and back with a (usually very small) frequency given by the difference E2 −E1 of the two lowest eigenvalues of the Hamilton operator. This motion is also referred to as umbrella motion. The case of an initial state starting from the top of the barrier is referred to as toppling pencil motion and has been studied classically in detail by Dittrich and Pena Mart´ ınez [21] as well as quantum mechanically in [22], where the influence of a finite number of bath degrees of freedom on the central degree of freedom (the double well) has been investigated. 3. The method of coupled coherent states The numerical method for the solution of the TDSE, to be used in the following, is the coupled coherent states method developed by Shalashilin and Child [15, 23]. It is most clearly formulated by using a complexified phase space variable, defined by z(t) = γ1/2q(t) + i¯h−1γ−1/2p(t)√ 2 , (3)Coupled coherent states method for tunneling dynamics: an interpretative study 4 Figure 1. Separatrix in the phase space of the quartic double well with D = 1 for the standard classical Hamiltonian (the larger “8”) and for its normal ordered version (the smaller “8”). where γ = mω/¯h. In the remainder of the presentation, we set ¯ h as well as m equal to unity and use Gaussian basis functions, which correspond to the coherent states of a harmonic oscillator with oscillation frequency ω = 1 (in dimensionless units) and the definition above simplifies considerably. In the coupled coherent states (CCS) method of Shalashilin and Child, an ansatz is made for the time-evolved wavefunction as a finite esum in the form |Ψ(t)⟩ = MX l=1 al(t) |zl(t)⟩, (4) where each ket|zl(t)⟩ is a coherent state (an eigenstate of the annihilation operator) with a time-dependent, complex center parameter zl(t) and the “multiplicity” of the ansatz is given by M. The set of the M center parameters is following uncoupled classical trajectories, each of which is fulfilling [23] i∂tz = ∂z∗ Hord(z∗, z), (5)Coupled coherent states method for tunneling dynamics: an interpretative study 5 which are Hamilton’s equations in complex notation. The normal ordered Hamiltonian is gained by using ˆq = 1√ 2(ˆa† + ˆa), ˆp = i 1√ 2(ˆa† − ˆa), (6) in Eq. (1), and moving all creation operators to the left, by using the fundamental commutation relation [ˆa, ˆa†] = ˆ1. (7) Then we replace ˆa† by z∗ and ˆa by z to arrive at the classical Hamiltonian [22] Hord = − 1 4 h (z∗ − z)2 − 1 i − 1 8 h (z∗ + z)2 + 1 i + 1 256D h (z∗ + z)4 + 6 (z∗ + z)2 + 3 i . (8) We stress that by expressing the z variable again in terms of position and momentum and by gathering all constant terms in the potential, a modified, normal ordered version of the potential emerges (see also [14]), that is given by Vord(q) = D − 1 4 \u0012 q2 − 1 2 \u0013 + 1 64D \u0012 q4 + 3q2 + 3 4 \u0013 . (9) Its minima are slightly shifted to q± = ± q 8D − 3/2 and the relative maximum at q = 0 is increased to Vord(0) = D + 1/8 + 3 /(256D). Due to the fact that the value of the ordered potential at the mimima is now not zero any more but given by Vord(q±) = 1 /2 − 6/(256D), the barrier height is changed (typically decreased) to D − 3/8 + 9/(256D). These changes also lead to a change of the separatrix, which is highlighted in Fig. 1. It is especially important to note that now already smaller values of phase space variables are leading to a crossing of the barrier. The equations of motion for the a-coefficients are derived from the time-dependent variational principle [5, 23] and are linear, coupled differential equations of the form i MX l=1 ⟨zk(t)|zl(t)⟩˙al(t) = MX l=1 ˜Hkl(t)al(t), (10) with the time-dependent matrix elements ˜Hkl(t) = ⟨zk(t)|zl(t)⟩ \u0014 Hord(z∗ k, zl) − 1 2   zl(t)∂Hord ∂zl − ∂Hord ∂z∗ l z∗ l (t) ! − z∗ k(t)∂Hord ∂z∗ l # , (11) and where an overlap matrix element, defined as ⟨zk |zl⟩ = e−1 2 (|zk|2+|zl|2)+z∗ kzl (12) appears on both sides of the equation. We stress that, obviously, it may not be cancelled because the multiplication in Eq. (11) is an element wise multiplication. For the inversion of the overlap matrix we thus employ a regularization in the form of the addition of a unit matrix multiplied by 10−8. Furthermore, the partial derivatives of theCoupled coherent states method for tunneling dynamics: an interpretative study 6 ordered Hamiltonian may be replaced by the left hand side of the classical equation of motion, Eq. (5). We found that there is only a marginal difference in the performance of the code due to this trick, however. Without loss of generality, we will start the quantum dynamics with a single one of the al being non-zero initially, such that |Ψ(0)⟩ = |α⟩ is given as the ground state of the above mentioned harmonic oscillator with ω = 1. Due to the coupling of the equations for the coefficients, generically, the population of the other basis functions will take on non-zero values during propagation. 4. Numerical results Before we show numerical results for the solution of the TDSE, we first focus on their central ingredients, i.e., the classical trajectories. If the energy of all the classical trajectories lies below the barrier, i.e., if all the initial conditions are inside the separatrix (see Fig. 1) and on the right side only (all have positive values of q), then they will stay there and tunneling cannot be described properly because the classical trajectories, by being uncoupled, cannot overcome the barrier and at no instance of time a Gaussian will “appear” on the other side. Two alternative scenarios can be imagined to overcome this limitation: • All trajectories start on one side only but the energies of some of them are larger than the barrier • Some (initially unpopulated) Gaussians are also started (with zero a-coefficient) on the other side but all trajectories have energies smaller than the barrier In order to discriminate the two cases in a clean fashion, we refrain from a random sampling of initial conditions that has been used successfully (i.e., it was shown that CCS is capable of correctly describing the tunneling effect) in [14] and that is especially favorable, if more than a single degree of freedom is to be investigated. With random samples drawn, e.g., using an importance sampling procedure, the appearance of trajectories with larger energies (and on the other side of the barrier) cannot be avoided. We thus stick to rectangular grids, as displayed, e.g., in panel (a) of Fig. 2. The potential barrier’s height, we study is given by D = 1 and the initial wavefunction |α⟩ is a Gaussian corresponding to the ground state of the harmonic approximation to the right well of the plain (not normal ordered) potential. Firstly, we study the case of initial conditions displayed in panel (a) of Fig. 2. We stress that only the trajectory in the middle of the grid has a non-zero a coefficient at t = 0. The time-dependent quantity, we monitor is the cross-correlation function c(t) = ⟨β|Ψ(t)⟩ = MX l=1 al⟨β|zl(t)⟩, (13) where β = −α = − √ 8. From the tunneling splitting ∆ := E2 − E1 ≈ 2.392 × 10−2, (14)Coupled coherent states method for tunneling dynamics: an interpretative study 7 Figure 2. (a) Separatrix of Hord in the phase space of the quartic double well with D = 1 with unoccupied initial conditions indicated by small open circles; the initially occupied Gaussian is displayed by a filled circle. (b) Comparison of numerical results for the absolute value of the autocorrelation function, using a grid of M = 49 initial conditions shown in panel (a) in the CCS case (solid line) and converged split operator FFT calculations (dotted line). for D = 1 given in [19], we expect a tunneling period of Tt = 2 π/∆ ≈ 263 (all in dimensionless units) to show up in the time evolution of c(t). In panel (b) of Fig. 2, we see that the full quantum solution, gained by a split-operator FFT implementation of the TDSE [24] and displaying the predicted (long) period, apart from the high-frequency, small-amplitude oscillations (which are due to the presence of eigenstates, with energies En above the barrier (n ≥ 3) in the initial state), is not captured by our ansatz. Secondly, to see if CCS can do better, we now launch a copy of the grid on the right side also on the left side, as displayed in panel (a) of Fig. 3. Both sets of trajectories are uncoupled, as classical mechanics is a local theory, but the coefficients corresponding to the trajectories are coupled. Thus, although all a-coefficients on the left are initially zero, they turn non-zero in the course of time and probability density is transferred to the other side, thus describing tunneling almost perfectly as can be seen in panel (b) of Fig. 3, where we have extended the time interval, such that two complete periods of tunneling can be observed. As a third try, let us increase the extension of the grid, initially on the right side of the barrier, see panel (a) of Fig. 4. This grid has now 81 points and an extension which is four times as large as the one in panel (a) of Fig. 2, leading to a density still far larger than the von Neumann limit [25], but also leading to a substantial part of initial conditions with energy above the barrier, i.e., outside of the “eight”. The comparison of the autocorrelations in panel (b) of Fig. 4, at least for short times, is now almost as favorable as in Fig. 3. We have not tried to fully converge the results, to still see a little difference between the results plotted in the graph. By retaining the extension of the grid but allowing for 121 instead of 81 trajectories, the quality of agreementCoupled coherent states method for tunneling dynamics: an interpretative study 8 Figure 3. (a) Separatrix of Hord in the phase space of the quartic double well with D = 1. Dense initial conditions displayed in Fig. 2 and a symmetric copy of those, shifted to the other well; unoccupied initial conditions indicated by small open circles; the initially occupied Gaussian is displayed by a filled circle. (b) Comparison of numerical results for the absolute value of the cross-correlation function, using a grid of M = 98 initial conditions shown in panel (a) in the CCS case (solid line) and converged split operator FFT calculations (dotted line). Both lines are almost indistinguishable. with the split operator FFT results is almost as good as in Fig. 3 (not shown). Thus also high energy trajectories “mimic” the tunneling, which happens at low energies quantum mechanically, where there are no classical trajectories (on the left side) by our construction. This fact is demonstrated in Fig. 5, where the phase space eight, together with the trajectories at time t = 131, close to half the tunneling period is shown. The trajectories outside of the eight are capable of creating a localized wavepacket at the bottom of the left well (because the overlap with the state |β⟩ at the considered time is almost unity. We do not display the relative weight of the trajectories by the size of the dots because, due to the non-orthogonality of the coherent states, the a-coefficients are not “normalized” in the same way as the ones in an orthogonal basis expansion and some of them can become exceedingly large. The norm of the wavefunction is conserved to within around one percent for the times displayed in panel (b) of Fig. 4, however. 5. Summary and Outlook We have shown that there are two ways to accurately describe tunneling in a double well by the use of coupled Gaussian basis functions, moving along classical trajectories: (i) either low energy initial conditions for classical trajectories are also launched in the initially unpopulated well, or (ii) one allows for trajectories with enough energy to overcome the barrier. A combination of the two cases has already been studied long ago in [14]. Here we have disentangled the two cases. Due to the fact that the equations of motion for the coefficients are coupled, in the course of time, the a-coefficients of allCoupled coherent states method for tunneling dynamics: an interpretative study 9 Figure 4. (a) Separatrix of Hord in the phase space of the quartic double well with D = 1. Less dense initial conditions as compared to Fig. 2, unoccupied initial conditions indicated by small open circles; the initially occupied Gaussian is displayed by a filled circle. (b) Comparison of numerical results for the absolute value of the cross-correlation function, using a grid of M = 81 initial conditions shown in panel (a) in the CCS case (solid line) and converged split operator FFT calculations (dotted line). trajectories will start to deviate from zero. Probability density can thus build up in the other well by two scenarios. Either the trajectories have been seeded in the initially unpopulated well from the very start, or they move there by having enough energy. The use of rectangular grids is not mandatory. We believe that also circular grids will do the job [26]. Furthermore, also the investigation of incoherent barrier tunneling is a worthwhile topic. In [27], it has, e.g., been shown that employing classical trajectories in a purely semiclassical (van Vleck-type) study does not lead to converged results for the barrier transmission as a function of the initial wavepacket center, while in [28], the use of multiple spawning of new classical trajectories after time-slicing was shown to lead to converged results. In future work, the use of trajectories that start on one side of the barrier with enough energy to overcome it classically in a CCS study shall be contrasted with the use of trajectories starting on both sides of the barrier. In addition, the investigation of dynamical tunneling [29] in the light of our present findings is left for future research. Finally, a lot of work on higher-dimensional tunneling applications using Gaussian wavepackets has been performed [30, 31, 32, 33], trying to reduce the number of basis functions as well as to overcome potential problems like zero-point energy leakage of the underlying classical trajectories. Although even in a HK approach it has been shown that the final result is not plagued by zero-point energy leakage [34], for the numerics this might be an issue. The results presented here may give further guidance in the search for efficient numerical algorithms also for more than a single degree of freedom.Coupled coherent states method for tunneling dynamics: an interpretative study 10 Figure 5.Separatrix of Hord in the phase space of the quartic double well withD = 1. Snapshot of the time-evolved less dense initial conditions panel (a) of Fig. 4 at time t = 131 (corresponding to half the tunneling period). Acknowledgements FG would like to thank Profs. D. Shalashilin and S. Garashchuk for fruitful discussions on Gaussian-based variational methods. References [1] Beck M, Jaeckle A, Worth G and Meyer H D 2000 Phys. Rep.324 1 – 105 [2] Lubich C 2008 From Quantum to Classical Molecular Dynamics: Reduced Models and Numerical Analysis Zurich Lectures in Advanced Mathematics (Z¨ urich: European Mathematical Society) [3] Richings G W, Polyak I, Spinlove K E, Worth G A, Burghardt I and Lasorne B 2015 Int. Rev. in Phys. Chem. 34 269–308 [4] Werther M, Loho Choudhury S and Grossmann F 2021 Int. Rev. in Phys. Chem.40 81 [5] Kramer P and Saraceno M 1981 Geometry of the time-dependent variational principle in quantum mechanics (Berlin: Springer Verlag)Coupled coherent states method for tunneling dynamics: an interpretative study 11 [6] Haegeman J, Cirac J I, Osborne T J, Pizorn I, Verschelde H and Verstraete F 2011 Phys. Rev. Lett. 107(7) 070601 [7] Schollw¨ ock U 2011Annals of Physics326 96 – 192 [8] Glauber R J 1963 Phys. Rev.131(6) 2766–2788 [9] Herman M F and Kluk E 1984 Chem. Phys. 91 27 [10] Wang Z X and Heller E J 2009 J. Phys. A42 285304 [11] Ray S, Ostmann P, Simon L, Grossmann F and Strunz W T 2016 J. Phys. A49 165303 [12] Simon L and Strunz W T 2014 Phys. Rev. A89(5) 052112 [13] Lando G M, Vallejos R O, Ingold G L and de Almeida A M O 2019 Phys. Rev. A99(4) 042125 [14] Shalashilin D V and Child M S 2001 J. Chem. Phys.114 9296 [15] Shalashilin D V and Child M S 2000 J. Chem. Phys.113 10028 [16] Hund F 1927 Z. Phys. 43 805–826 [17] Kurkij¨ arvi J 1972Phys. Rev. B6 832 [18] Kierig E, Schnorrberger U, Schietinger A, Tomkovic J and Oberthaler M K 2008 Phys. Rev. Lett. 100 190405 [19] Grossmann F, Jung P, Dittrich T and H¨ anggi P 1991 Zeitschrift f¨ ur Physik B84 315 [20] Reichl L E 2004 The Transition to Chaos: Conservative Classical Systems and Quantum Manifestations 2nd ed (New York: Springer) [21] Dittrich T and Pena Mart´ ınez S 2020Entropy 22 1046 [22] Loho Choudhury S and Grossmann F 2022 Phys. Rev. A105 022201 [23] Shalashilin D V and Burghardt I 2008 J. Chem. Phys.129 084104 [24] Grossmann F 2018 Theoretical Femtosecond Physics: Atoms and Molecules in Strong Laser Fields 3rd ed (Springer International Publishing AG) [25] von Neumann J 1955 Mathematical Foundations of Quantum Mechanics(Princeton: Princeton University Press) [26] Schleich W P 2001 Quantum Optics in Phase Space(Berlin: Wiley-VCH) [27] Grossmann F and Heller E J 1995 Chem. Phys. Lett.241 45 [28] Grossmann F 2000 Phys. Rev. Lett.85 903 [29] Davis M J and Heller E J 1981 The Journal of Chemical Physics75 246–254 [30] Wu Y and Batista V S 2004 J. Chem. Phys.121 1676–80 [31] Sherratt P A, Shalashillin D V and Child M S 2006 Chemical Physics 322 127–134 [32] Saller M A C and Habershon S 2017 J. Chem. Theory Comput.13 3085 [33] Dutra M, Wickramasinghe S and Garashchuk S 2020 J. Phys. Chem. A124 9314–9325 [34] Buchholz M, Fallacara E, Gottwald F, Ceotto M, Grossmann F and Ivanov S D 2018 Chem. Phys. 515 231",
      "references": [
        "From Quantum to Classical Molecular Dynamics: Reduced Models and Numerical Analysis",
        "Geometry of the time-dependent variational principle in quantum mechanics",
        "The Transition to Chaos: Conservative Classical Systems and Quantum Manifestations",
        "Theoretical Femtosecond Physics: Atoms and Molecules in Strong Laser Fields",
        "Mathematical Foundations of Quantum Mechanics",
        "Quantum Optics in Phase Space"
      ],
      "meta_data": {
        "arxiv_id": "2402.13847v1",
        "authors": [
          "Frank Grossmann"
        ],
        "published_date": "2024-02-21T14:44:16Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Interpretative study of the Coupled Coherent States (CCS) variational method applied to coherent tunneling in a 1D quartic double-well. Main finding: accurate tunneling dynamics (tunneling period/splitting) can be reproduced even when all guiding trajectories are purely classical and uncoupled, and—crucially—over-the-barrier (above-separatrix) trajectories alone can mimic low-energy quantum tunneling without explicitly including under-barrier classical transport. The paper disentangles two mechanisms enabling CCS tunneling accuracy: (i) seeding coherent states in both wells at low energies (initially unoccupied on the target side) and (ii) using a single-well grid extended to include above-barrier trajectories; both can yield near-FFT-level agreement for correlation functions.",
        "methodology": "Uses CCS wavefunction ansatz |Ψ(t)⟩=∑_{l=1}^M a_l(t)|z_l(t)⟩ with Gaussian (harmonic-oscillator) coherent states. Centers z_l(t) follow independent classical trajectories governed by Hamilton’s equations in complex form iẋ=∂_{z*}H_ord(z*,z) derived from the normal-ordered Hamiltonian of the quartic potential (introducing an effective potential V_ord with shifted minima and modified barrier). Coefficients a_l(t) are propagated via time-dependent variational principle, giving a linear system i S \\, \\.a = Ḣ a with overlap matrix S_{kl}=⟨z_k|z_l⟩ and effective Hamiltonian elements \\tilde{H}_{kl} including derivative terms; numerical stabilization via overlap-matrix regularization (adding 10^{-8}I). Observables assessed through cross-correlation c(t)=⟨β|Ψ(t)⟩ (β located in opposite well).",
        "experimental_setup": "Model: 1D quartic double-well with unit mass and ħ=1, parameterized by barrier height D (studied D=1). Initial state: single occupied coherent state corresponding to ground state of harmonic approximation in the right well (ω=1), with other basis functions initially unpopulated (a_l(0)=0 except one). Basis selection: deterministic rectangular phase-space grids of initial conditions to isolate effects: (A) grid confined to right well inside separatrix (M=49), (B) same grid plus symmetric copy in left well (M=98), (C) larger right-well grid extended to include above-barrier initial conditions (M=81; tested also denser M=121). Validation/benchmark: comparison of |c(t)| against converged split-operator FFT TDSE propagation; expected tunneling period inferred from literature splitting Δ=E2−E1≈2.392×10^{-2} giving T_t≈263. Additional qualitative analysis: phase-space snapshots of trajectories near half tunneling period to show localization in opposite well from over-barrier trajectories.",
        "limitations": "Restricted to 1D coherent (phase-preserving) tunneling in a symmetric quartic double well; conclusions may not directly transfer to multidimensional systems or strongly anharmonic/chaotic regimes. Accuracy depends on ad hoc choice of coherent-state grid extent/density; no systematic convergence study or error bounds provided, and long-time behavior shows small norm drift (~1% over shown times). Use of normal ordering modifies the effective potential/barrier (V_ord), so classical guidance differs from the original bare potential; implications for general potentials not fully explored. Coherent states are non-orthogonal, leading to potentially ill-conditioned overlap matrices and large coefficients; regularization is heuristic. Results focus on correlation functions rather than full state tomography or other observables; incoherent tunneling/barrier transmission not treated.",
        "future_research_directions": "Systematically compare and optimize the two successful CCS strategies (seed both wells vs include over-barrier trajectories) under controlled convergence criteria and for varying barrier heights/asymmetries. Extend to incoherent barrier tunneling and scattering/transmission problems, potentially incorporating time-slicing and trajectory spawning/multiple spawning to improve convergence. Study dynamical tunneling (phase-space/ resonance-assisted) using insights about over-barrier trajectory contributions. Generalize to higher-dimensional tunneling and molecular applications while addressing efficiency (basis reduction), overlap conditioning, and issues such as zero-point energy leakage; explore alternative initial-condition geometries (e.g., circular grids) and adaptive sampling schemes.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A Soft Prompt Optimization Approach",
      "full_text": "arXiv:2405.02107v1  [cs.IT]  3 May 2024 Equal Requests are Asymptotically Hardest for Data Recovery Jüri Lember and Ago-Erik Riet Institute of Mathematics and Statistics University of T artu Narva mnt 18, 51009 T artu, Estonia Email: {juri.lember, ago-erik.riet}@ut.ee May 6, 2024 Abstract In a distributed storage system serving hot data, the data re covery performance becomes important, captured e.g. by the servic e rate. We give partial evidence for it being hardest to serve a sequenc e of equal user requests (as in PIR coding regime) both for concrete and rand om user requests and server contents. We prove that a constant request sequence is locally hardest to serve: If enough copies of each vector are stored in servers, then if a request sequence with all requests equal can be served then we can sti ll serve it if a few requests are changed. F or random iid server contents, with number of data symbols c onstant (for simplicity) and the number of servers growing, we show t hat the maximum number of user requests we can serve divided by the nu mber of servers we need approaches a limit almost surely. F or unif orm server contents, we show this limit is 1/2, both for sequences of cop ies of a ﬁxed request and of any requests, so it is at least as hard to se rve equal requests as any requests. F or iid requests independent from the uniform server contents the limit is at least 1/2 and equal to 1/2 if re quests are all equal to a ﬁxed request almost surely, conﬁrming the same . As a building block, we deduce from a 1952 result of Marshall H all, Jr. on abelian groups, that any collection of half as many req uests as coded symbols in the doubled binary simplex code can be serve d by this code. This implies the fractional version of the F unctional Batch Code Conjecture that allows half-servers. 1 Introduction A linear code C ≤ Fn q is determined by its linear encoder G : Fk q → Fn q which is a full-rank k-by-n matrix over the ﬁnite ﬁeld Fq, known as the generator 1matrix, linearly mapping a row vector of k data symbols of Fq to a row vector of n coded symbols of Fq by multiplication. Data recovery codes, such as batch codes [12] and PIR codes [6], see [16] for a survey , in their cl assical as well as functional [18] or asynchronous [15] versions, are exact ly determined by a generator matrix up to a permutation of its columns. Thus the se codes should in fact be called encoders. Batch codes were introduced for l oad-balancing in a distributed storage system [12] and PIR codes, a general ization of batch codes (as PIR codes have weaker requirements), were introdu ced as a coding layer for multi-server information-theoretic private inf ormation retrieval (PIR) to reduce the storage overhead [6]. Here we only consider so- called “primitive linear multiset batch codes”, where in particular “linear” implies that we use coding and not only replication. F or the usufulness of G ∈ Matk×n(Fq), i.e. a k-by-n matrix G over the ﬁnite ﬁeld Fq, as a code (encoder) optimized for recovery of data, diﬀeren t parameters have been studied. A general parameter is its ser vice rate region [1] [2] that we will not discuss here in full generality , but whic h we can specify to the “ t-parameter”: the biggest t such that any multiset/sequence of t (linear combinations of) data symbols in Fq (user requests) can be recovered from pairwise disjoint sets of coded symbols in Fq (servers). What user requests we allow depends on the setup: whether the code is (functional) batch or PIR. F or an asynchronous batch code user requests come in a stream and the system should serve a new one when any current (we do not know which) r equest has been served. Serving a data symbol (user request) from a set a coded symbol s (servers) under a linear model exactly corresponds to representing th e standard basis vector (i.e. unit vector) corresponding to the data symbol a s a linear combination of the columns of G corresponding to the coded symbols. W e will thus also talk about serving a vector from a set of columns of G. Therefore the properties of our codes are exactly determined by the multiset of columns o f G. The matrix G is deﬁned to be a t-(functional) batch code [18] if we are able to serve any multiset of t standard basis vectors (any vectors) from pairwise disjoint column sets of G. W e will denote the largest t such that G is a t- (functional) batch code as tb(G) (tfb (G)). F or G to be a t-(functional) PIR code [18] we must be able to serve t copies of the same standard basis vector (the same vector), for any standard basis vector (any vector ), and we denote such largest t as tP (G) (tfP (G)). The functional case corresponds to allowing linear combinations of data symbols as user requests. W e say the matrix is t-PIR for r if we are able to serve t copies of request r from it and write trP (G) for the largest such t. Thus tfP (G) = min r̸=0 trP (G). W e will call the t∗(G) the t-parameter of G as a code where ∗ is one of b, P, fb, fP, rP . W e also write t≤w ∗ (G) if each column set in the service, i.e. each recovery set is required to have size ≤ w. The t-parameter does not depend on the permutation of the columns/coded symbols. Note the ineq ualities tfb (G) ≤ tb(G) ≤ tP (G) and tfb (G) ≤ tfP (G) ≤ tP (G), recall tfP (G) = min r̸=0 trP (G). All these remain true if we replace t∗(G) by t≤w ∗ (G) for a ﬁxed w. 2In Sect. 2 we develop the main building blocks. In Sect. 3 we pr ove if a constant request sequence can be served we can still serve it if a few requests are changed. Next we let G ∈ Matk×n(F2) be uniformly random throughout. In Sect. 4 we prove concentration bounds for the t-parameters. In Sect. 5 we show we can asymptotically serve γn requests from n columns where γ = 1 2 , both if the requests are arbitrary or all equal. In Sect. 6 we show if w e additionally let requests be random we can serve the fewest requests if they ar e equal to a ﬁxed request almost surely , and note constant γ exists in more general settings. In Sect. 7 we conclude the paper. 2 Building blocks If G ∈ Matk×n(Fq) were uniformly random, from the law of large numbers, if n grows much faster than k we expect there to be around n/q k copies of each vector as a column of G. This motivates proving the following proposition that we will deduce from the result in [8]. F or simplicity , from he re on we assume q = 2 , but many results will in fact generalize. Proposition 2.1 Let m be the largest integer such that G contains as columns at least 2m copies of each vector of Fk 2. Then G is a m2k-functional batch code, where moreover all recovery sets have size at most 2, i.e., t≤2 fb (G) ≥ m2k. This means that we can serve any sequence/multiset of m2k user requests (vectors from Fk 2 \\{0}) if every e ∈ Fk 2 appears as a column of G at least 2m times, where moreover each recovery set will have size at mos t 2. Proof. F or general m, apply the m = 1 case m times, with column vectors as equally as possible and requests arbitrarily distributed i nto m (almost) equal parts that are matched up. Now let m = 1 . Let the sequence of user requests be R = ( r1, r2 . . . , r2k ) ∈ (Fk 2 )2k . W e will let s1 = − ∑ 2k i=2 ri in the abelian group Fk 2 and si = ri for i > 1. By the result in [8], we can ﬁnd two permutations (a′ i)2k i=1 and (b′ i)2k i=1 of the abelian group Fk 2 , such that si = a′ i − b′ i for each i. Let ai = a′ i + c and bi = b′ i + c for a ﬁxed c such that now a1 = r1, also si = ai − bi for each i, and also (ai) and (bi) are permutations of Fk 2. Finally , we can serve request r1 from column set {a1} and requests ri for i > 1 from column sets {ai, bi}, using two copies of each vector of Fk 2 \\{r1 − s1} and one copy of r1 − s1 in total from the columns of G. Remark 2.2 1. The m = 1 case of Proposition 2.1 was proved in [17] Sect. V by a technique similar to the more general [8] by Hall, Jr. from 1952. We point out [8] relatively easily implies the m = 1 case of the Proposition. 2. In [8] an explicit Θ((2 k)2)-time algorithm produces the permutations (ai), (bi) by adding a new matching edge {aj , bj } and rewiring the previous ones at step j ∈ [2k]. A similar algorithm in the special case was employed in [17]. 33. The Proposition implies the fractional version of the Func tional Batch Code Conjecture [3, 9, 10, 13, 17] where we are allowed to spli t each col- umn/server into ‘two halves’. The full Functional Batch Code Conjecture is open and seems to be hard to prove, and it is exactly the stat ement of the Proposition for m = 1 2 . 4. It does not necessarily provide the most eﬃcient way to serve t he requests. See Proposition 2.3 for an upper bound on the number of reques ts that can be served. 5. To serve request 0 we need no columns, and similarly, column 0 does not help to serve any request. Vector 0 was not treated diﬀerently here for ease of analysis. Let NG(e) be the number of occurrences of e as a column of G ∈ Matk×n(F2) and let NR(e) be the number of occurrences of e ∈ Fk 2\\{0} in user request sequence R = ( r1, . . . , rv) ∈ (Fk 2 \\{0})v. Let us denote |G| = ∑ e∈Fk 2 NG(e) = n and |R| = ∑ e∈Fk 2 NR(e) = v and let |G ∩ R| = ∑ e∈Fk 2 min{NG(e), N R(e)} for sizes as multisets. Proposition 2.3 If we can serve R ∈ (Fk 2 \\{0})v from G ∈ Matk×n(F2), so |G| = n and |R| = v, then 2|R| ≤ | G| − NG(0) + |G ∩ R|. (1) Proof. If 2|R| > |G| − NG(0) + |G ∩ R| or equivalently 2(|R| − | G ∩ R|) > |G| − NG(0) − |G ∩ R| we can not serve R from G since for each request outside G ∩ R we need at least two nonzero columns of G to serve it. 3 PIR is hardest among almost-PIR: local hard- ness W e have the intuition that many equal requests should be hard er to serve than as many diﬀerent requests. W e can ask the following question . Question 3.1 Is it true that tfP (G) = tfb (G) for G ∈ Matk×n(F2)? Perhaps even trP (G) = tfb (G) if NG(e) is minimized over e ∈ Fk 2 \\{0} by r? Proposition 3.2 Answer to Question 3.1 is ‘Yes’ if k = 2 . Proof. W e can without loss of generality , i.e., up to a symmetry whic h is multiplying each column by a ﬁxed 2-by-2 invertible matrix o ver F2, assume NG((01)) ≤ NG((10)) ≤ NG((11)). Since for k = 2 any inclusion-wise min- imal recovery set has size at most 2, the following Lemma 3.4 t ells us that t(01)P (G) = t(10)P (G) = NG((01)) + NG((10)) ≤ t(11)P = NG((01)) + NG((11)), so tfP (G) = NG((01)) + NG((10)). T o show tfb (G) = tfP (G) = t(01)P (G), 4i.e. that NG((01))+ NG((10)) any requests can be served, we will greedily serve |G ∩ R| requests from recovery sets of size 1 and the rest from recove ry sets of size 2. Enough recovery sets of size 2 remain for that: if ∆ (01) := NR((01)) − NG((01)) > 0 then min{NG((10)) − NR((10)) , N G((11)) − NR((11))} ≥ ∆ (01) and analogous relations hold for ∆ (10) and ∆ (11). Example 3.3 Answer to Question 3.1 is ‘No’ already for k = 3 . Consider the matrix G = ( 1 1 1 1 0 0 1 1 0 1 0 1 ) . Note that the sum of columns of G is 0 and rank (G) = 3 , so tfP (G) = 2 , as two copies of any r ∈ F3 2\\{0} can be served from some subset of columns and its complement respectively. But tfb (G) ≤ tb(G) = 1 as request sequence ((001), (010)) cannot be served. The previous negative result shows the PIR coding regime is n ot at least as hard as the batch coding regime for some concrete matrices. I n what follows we prove the positive results that it is hardest locally in th e non-random case as well as almost surely in natural random models. W e now prove the lemmas towards our local hardness result. Lemma 3.4 The distinct recovery sets (of nonzero columns) for a ﬁxed r ∈ Fk 2\\{0} of size at most 2 are pairwise disjoint. In particular, the number t≤2 rP (G) of pairwise disjoint such recovery sets for G ∈ Matk×n(F2) is their number which is t≤2 rP (G) = NG(r) + 1 2 ∑ e∈Fk 2 \\{0, r} min{NG(e), N G(r − e)} (2) where the last sum counts each recovery set of size 2 twice. Proof. If r = a + b = a′ + b′ then {a, b} = {a′, b′} or {a, b} ∩ { a′, b′} = ∅ as a − a′ = b′ − b and a − b′ = a′ − b. So pairs {a, b} with r = a + b partition Fk 2. The recovery set of size one is {r} which can be viewed as {r, 0} and vice versa. The assertion follows. Lemma 3.5 The maximum number of pairwise disjoint recovery sets for a ﬁxe d r ∈ Fk 2\\{0} is not reduced by choosing all recovery sets of size at most 2. T hat is, trP (G) = t≤2 rP (G) + trP (G′) where NG′ (r) = 0 and NG′ (e) = NG(e) − min{NG(e), N G(r − e)} for all e ∈ Fk 2\\{0, r}, see also (2). Here G′ is what remains of G after using the columns in all the recovery sets of size at most 2. 5Proof. Consider a collection of maximum possible size of pairwise d isjoint recov- ery sets for r, where the total size of all recovery sets used is minimum pos sible among such collections. Then if A ⊆ B and A and B are both recovery sets for r we always use A instead of B, i.e. we only use minimal recovery sets. So we use all recovery sets of 1. There is no recovery set A of size 2 which is not used where one of its columns is available and the other is used in a recovery set B of size at least 3, as we could have used A instead of B and reduced the total size of used recovery sets. Thus a recovery set of size 2 is not used only in the case where e ach of its columns is used in a diﬀerent recovery set, which have to have size at least 3 by Lemma 3.4. Assume A and B are disjoint recovery sets of size at least 3 for r, and C ⊆ A ∪ B is a recovery set for r with |C| = 2 and |A ∩ C| = 1 = |B ∩ C|. Replace A and B by C and (A ∪ B)\\C which is a recovery set for r as∑ i∈(A∪B)\\C ci = ∑ i∈A ci + ∑ i∈B ci − ∑ i∈C ci = r + r − r = r. By applying this step repeatedly the total number of used rec overy sets and their total size do not change, and the number of used recover y sets of size at most 2 goes up. W e will eventually have used all recovery sets of size at most 2. Lemma 3.6 Let G ∈ Matk×n(F2) with NG(e) = 2 m for all e ∈ Fk 2\\{0}. Then there is only one way (with equal columns indistinguishable ) to serve m2k copies of a ﬁxed r ∈ Fk 2 \\{0}, and this uses all the nonzero columns of G. Proof. Use 2m copies of each of the recovery sets {r} and {e, r − e} where e ∈ Fk 2 \\{r, 0}, i.e., m2k recovery sets in total. Since each nonzero column of G is used, and used in the unique possible recovery set for r of minimum size containing it, we have trP (G) = m2k, and this service is the unique service with m2k recovery sets. Next we prove our main local hardness result partially conﬁr ming our intu- ition. Theorem 3.7 (“PIR is locally hardest”) For an integer m and vector r ∈ Fk 2\\{0} let Rm r ⊆ (Fk 2 \\{0})∗ be the family of the request sequences R with at most m2k requests diﬀerent from r, and let Gm be the family of such matrices G ∈ Matk×n(F2) that contain NG(e) ≥ 2m copies of each e ∈ Fk 2 \\{0} as a column. Then G ∈ G m can serve any R ∈ R m r with |R| = trP (G), the maximal length of a request sequence (r, . . . , r) that can be served by G. This means that it is at least as hard to serve v copies of request r by a matrix G ∈ G m as it is to serve by G any R ∈ R m r consisting of v requests. Proof. Let H be the matrix obtained by removing 2m columns e from G for all e ∈ Fk 2 \\ {0}, then NH (e) = NG(e) − 2m for all e ̸= 0. Now let G′ and H′ be the matrices obtained from G and H by removing columns as in Lemma 3.5. Then NG′ (r) = NH′ (r) = 0 , NG′ (e) = NG(e) − min{NG(e), N G(r−e)} for e ̸= 0, r, and NH′ (e) = NH (e)−min{NH (e), N H (r− e)} = NG′ (e) for e ̸= 0, r. So we conclude that G′ = H′. Now using (2), it is easily seen that t≤2 rP (H) = t≤2 rP (G) − m2k and by the above, we have that trP (G′) = trP (H′). 6Then from Lemma 3.5, we conclude that trP (H) = trP (G) − m2k. Now let R be a request sequence from Rm r of length trP (G). Then G can ﬁrst serve m2k requests from R including the requests diﬀerent from r, using Proposition 2.1, and then use the remaining columns H to serve the remaining trP (G) − m2k requests r, so G can serve R. 4 Random G: bounded diﬀerences In what follows, we shall consider G ∈ Matk×n(F2) random with entries being i.i.d. Bernoulli with probability 1 2 . Thus the columns of G are the ﬁrst n vectors among i.i.d. random variables (abbreviated to r.v.’s) C1, C2, . . . with uniform distribution over Fk 2. W e shall refer to theses r.v.’s as the column process. Then any t-parameter t∗(G) is a function of random columns, hence a r.v.. W e shall denote these r.v.’s by T ∗ n = t∗(C1, . . . , Cn). It is rather easy to see that all these r.v.’s satisfy the bounded diﬀerences (BD) property: changing one column of G changes the value of t∗(G) by at most 1. T o see that take ∗ = rP and recall that trP (G) is the maximum number of copies of r the matrix G can serve. Since each column belongs to at most one recovery set, by changing i t, the maximum number of copies decreases or increases by at most one. By a si milar argument the BD property can be checked for tfP (G) = min r trP (G) and tfb (G) and in fact for any of our t∗(G) and t≤w ∗ (G). Since the columns are independent, by McDiarmid’s inequality (see, e.g. [4]), for any c > 0 we have P (T ∗ n − ET ∗ n ≥ c) ≤ exp ( − 2c2 n ) . McDiarmid’s inequality allows in particular to obtain non- asymptotic conﬁdence intervals, e.g. with probability ≥ 1−α we have T ∗ n ≥ ET ∗ n − √ n 2 ln 1 α . The use of this lower bound can be complicated in practice as E T ∗ n is generally not known. Then one can use more tractable suboptimal bounds such as T ∗, ≤2 n ≤ T ∗ n which might have analytically known expectations, or estimate th e expectation by simulations. F or the upper bound, we use the superadditivity property: for any ∗ = b, P, fb, fP, rP , any 1 ≤ n′ < n and for any set of column vectors, we have t∗(C1, . . . , Cn) ≥ t∗(C1, . . . , Cn′ ) + t∗(Cn′+1, . . . , Cn). The same holds for expectations, so by F ekete’s Lemma, ET ∗ n /n ր supn ET ∗ n /n . In the next section, we shall show that the limit or supremum i s 1/2, meaning that ET ∗ n ≤ n/ 2. Hence in the upper bound, ET ∗ n can be replaced by n/ 2 and the conﬁdence interval reads: with probability ≥ 1 − α we have T ∗ n ≤ n 2 + √ n 2 ln 1 α . Another use of McDiarmid’s inequality is to obtain upper bou nds for the absolute central moments: E |T ∗ n − ET ∗ n |p ≤ p(n/ 2)p/ 2Γ ( p 2 ) for any p > 0. F or its derivation, see, e.g. [7]. Setting p = 2 , we obtain V ar(T ∗ n ) ≤ n. 7Remark 4.1 McDiarmid’s inequality and its consequences above only use t he BD property and that columns of G are independent. Hence the entries within one column can be dependent and the columns need not be identi cally distributed. 5 Asymptotic service rate for uniform G is lowest for constant request sequences Recall that the r.v.’s T ∗ n satisfy the following inequalities: T fb, ≤2 n ≤ T fb n ≤ T fP n ≤ T rP n . W e shall now show that when divided by n, all these sequences converge almost surely (abbreviated to a.s., i.e. with prob ability 1) to 1/2. This means that asymptotically serving any constant request seq uence is as hard as serving all sequences. It also implies that the total number of all recovery sets with sizes more than two that we need is asymptotically negli gible. In other words, for large n we can serve any request with recovery sets consisting of one or two elements, without losing much. Theorem 5.1 For any ﬁxed r ∈ Fk 2\\{0} we have lim n T fb, ≤2 n n = lim n T rP n n = 1 2 a.s.. Proof. Recall that NG(e) is the number of occurrences of e as a column of G. F rom Hoeﬀdings’s inequality (see, e.g, [4]) we have P ( NG(e) n − 1 2k ≥ ε ) ≤ exp(−2nε2). Let εn ց 0 slowly enough s.t. ∑ n exp(−2nε2 n) < ∞. By the Borel-Cantelli Lemma P ( NG(e) n ≥ 1 2k − εn for all e ∈ Fk 2 eventually ) = 1 . (3) W e will abbreviate eventually to ev.. Denote mn := n ( 1/ 2k+1 − εn/ 2 ) . Thus if NG(e)/n ≥ 1/ 2k − εn for all e ∈ Fk 2 then we have NG(e) ≥ 2mn for all e ∈ Fk 2 and in that case we deduce from Proposition 2.1 that T fb, ≤2 n ≥ mn2k = n ( 1 2 − 2k−1εn ) , thus T fb, ≤2 n n ≥ 1 2 − 2k−1εn. F rom (3) we have P ( T fb, ≤2 n /n ≥ 1 2 − 2k−1εn ev. ) = 1 , thus lim inf n T fb, ≤2 n n ≥ 1 2, a. s. 8T o prove the upper bound, ﬁx r and observe that from (1), it follows that 2T rP n ≤ n − NG(0) + min {NG(r), T rP n } ≤ n + NG(r) − NG(0), because |G ∩ R| = min {NG(r), T rP n } when the request sequence R consists of T rP n copies of r. Hence T rP n n ≤ 1 2 + NG(r) − NG(0) 2n → 1 2, a. s., Thus lim sup n T rP n n ≤ 1/ 2. Remark 5.2 The lower bound followed from Proposition 2.1 and the concen tra- tion of the frequencies of vectors as columns. For a slightly w eaker upper bound lim supn(T P n /n ) ≤ 1 2 we could also use that tP (G) ≤ d(CG), where d (CG) is the minimum distance of the code generated by G, i.e. row span of G (see [11, 14]), and that lim supn(d(CG)/n ) ≤ 1 2 , a.s. 6 Random user requests W e now generalize our model by letting requests be random as w ell. T o be more precise, let the requests be i.i.d. random vectors R1, R2, . . . taking values in Fk 2\\{0}, we will refer to them as the request process. The distribution Q of Ri will be referred to as the request distribution . Given G ∈ Matk×n(F2), let the r.v. V (G) = max {V : {R1, . . . , RV } can be served by G}. Thus R1, . . . , RV can be served but R1, . . . , RV +1 can not be. As in the previous sections, we consider G also random with i.i.d. uniformly distributed columns and independent from the request process. Let Vn = V (G). Hence Vn is a func- tion depending on n random columns and at most n random requests. Observe that when Ri = r a.s., i.e. the request distribution Q is degenerate ( Q = δr), then Vn = T rP n . Clearly for any Q we have Vn ≥ T fb n , so by Theorem 5.1 we have lim infn(Vn/n ) ≥ 1 2 . W e now argue that when Q is not degenerate, then limn(Vn/n ), if it exists, might be bigger than 1 2 . This conﬁrms the fact that asymptotically , equal requests are hardest to serve, i.e. allowing randomness in t he request process increases the asymptotic proportion of served requests. W e illustrate it for the case of the uniform Q, i.e. P (Ri = e) = 1 / (2k − 1) for all e ∈ Fk 2 \\{0}. Recall that 0 is not allowed as a request, so such a Q has maximum entropy . Proposition 6.1 For Q uniform, Vn/n → 1 − 2−k, a.s.. 9Proof. Let 0 < α < 1 − 2−k, hence α/ (2k − 1) < 1/ 2k. F or every e ̸= 0 let Nα R(e) = ∑ nα i=1 Ie(Ri) be the number of occurrences of e among the ﬁrst nα requests. Clearly Nα R(e)/ (nα ) → 1/ (2k − 1) a.s. T ake ε > 0 small enough such that ε < 1/ 2k − α/ (2k − 1), thus α/ (2k − 1) + αε/ 2 < 1/ 2k − ε/ 2. (4) Clearly , as G is uniform and n → ∞ , (“ev.” is “eventually”) P ( max e̸=0 Nα R(e) < ( α 2k − 1 + αε 2 ) n, ev. ) = 1 , P ( min e̸=0 NG(e) > ( 1 2k − ε 2 ) n, ev. ) = 1 . Hence by (4) P  ∑ e̸=0 min{NG(e), N R(e)} = ∑ e̸=0 Nα R(e) = αn, ev.   = 1 . So P (Vn ≥ αn, ev.) = 1 , i.e. lim infn(Vn/n ) ≥ α , a.s. This holds for any α < 1 − 2−k implying that lim infn(Vn/n ) ≥ 1 − 2−k a.s. F or the upper bound, argue similarly: take α > 1 − 2−k = (2 k − 1)/ 2k, hence α/ (2k − 1) > 1/ 2k. F or any ε > 0 and any e ̸= 0 we have Nα R(e) >( α/ (2k − 1) − ε ) n, eventually . On the other hand NG(e) < (1/ 2k + ε)n, even- tually . Hence for every n big enough not all αn requests can be served, so that Vn < αn , eventually . Thus lim supn(Vn/n ) ≤ α , a.s. This holds for any α > 1 − 2−k, so lim supn(Vn/n ) ≤ 1 − 2−k, a.s. This completes the proof. F or uniform G, we could also ask about the maximum number of requests among the given requests that can be served by it. Given an i.i .d. request process R1, R2, . . . , with Ri ∼ Q and a matrix G, let L(G; R1, . . . , Rn) be the maximum size of a subset I ⊆ [n] such that the request sequence {Ri | i ∈ I} can be served by G. Clearly tfb (G) ≤ V (G) ≤ L(G) ≤ n. Recall that the random G is just the ﬁrst n elements of the i.i.d. uniform column process C1, C2, . . . , so Ln := L(G) = L(C1, . . . , Cn; R1, . . . , Rn). Recall the column and request processes are independent. W e obviously have superadditiv ity: for any 1 ≤ n′ < n we have L(C1, . . . , Cn; R1, . . . , Rn) ≥ L(C1, . . . , Cn′ ; R1, . . . , Rn′ ) + L(Cn′+1, . . . , Cn; Rn′+1, . . . , Rn), so by Kingman’s Subadditive Ergodic Theorem (see, e.g. [5]) there is a constant γ ∈ [ 1 2 , 1 ] such that Ln/n → γ, a.s. and E (Ln)/n → γ. It is not hard to see that if Q is uniform, then γ = 1 − 2−k, just like above. Indeed, γ ≥ 1 − 2−k follows from the fact that Ln ≥ Vn. T o obtain the upper bound, take α > 1 − 2−k and note that a necessary condition for Ln ≥ αn is that n−NG(0) = ∑ e̸=0 NG(e) ≥ αn . Thus NG(0)/n > (1 − α ) implies Ln < αn . Since 1 − α < 2−k, it follows that P (Ln/n < α ) ≥ P ( NG(0)/n > (1 − α ) ) → 1. Therefore γ ≤ α. 10It is also easy to see that Ln satisﬁes the BD property , unlike Vn. Since Ln is a function of 2n independent random variables, (two-sided) McDiarmid’s inequality in this case reads P (|Ln − ELn| > c ) ≤ 2 exp ( − c2 n ) . The limit γ in Kingman’s theorem depends on the request distribution Q and on the column distribution P . In our paper P is uniform over Fk 2, but it need not necessarily be so. W e have shown that when P is uniform, then γ(P, Q ) ≥ 1/ 2 and when, in addition, Q = δr (degenerate), then γ(P, Q ) = 1 / 2 (PIR is the hardest). W e have also shown that when P and Q are both uniform ( Q over non- zero vectors), then γ(P, Q ) = 1 −2−k. The other cases are open and determining the function γ(P, Q ) is a challenge for future research. Kingman’s theorem in fact holds for any ergodic process (C1, R1), (C2, R2), . . . , i.e. the column and request process need not necessarily be i.i.d. or mutually i ndependent. So, for example the limit γ also exists when the request process and the column process are both (possibly dependent) stationary Markov chains. 7 Conclusion W e consider the question if equal user requests are hardest t o serve for a dis- tributed storage/service system, like a batch code or a PIR c ode. F or concrete, non-random k-by-n generator matrices G over F2 we prove that serving a re- quest sequence r, . . . , r of v requests is at least as hard as serving any request sequence in a Hamming ball around this sequence, that is, it i s locally hardest. The radius of the Hamming ball depends on the matrix G, concretely on the minimum number of occurrences of e as a column of G, over all e ̸= 0. But there are concrete matrices G already with 3 rows which can serve two copies of any request, but for which there is a pair of two diﬀe rent requests that cannot be served by G. F or uniformly random generator matrices we prove serving r, . . . , r for any ﬁxed r ̸= 0 is indeed hardest asymptotically , with k ﬁxed and n → ∞ , and we determine that the asymptotic service rate, i.e. the rati o of the number of requests that can be served to the number of servers needed, i s γ = 1 / 2. Letting moreover requests be i.i.d. and independent from G, the hardest case is again where the requests are all equal to a ﬁxed request with probab ility 1. On the other hand with requests also uniform but over nonzero vecto rs, γ = 1 − 2−k. An asymptotic service rate γ = γ(P, Q ) exists for any i.i.d. requests (with distribution Q) and any i.i.d. server contents (with distribution P ) independent from the requests, and in more general settings. Its determi nation in other cases is open. W e explain how to estimate the respective random variables, give conﬁdence intervals, and estimate their moments. As a building block, we deduce from a 1952 theorem of Marshall Hall, Jr. on abelian groups that the doubled binary simplex code can serv e any request 11sequence of half its length plus one. This easily implies the fractional version of the F unctional Batch Code Conjecture where half-servers ar e allowed. Acknowledgment W e would like to thank Kristiina Oksner and Vitaly Skachek fo r Example 3.3 and Henk D.L. Hollmann for a better derivation of Theorem 3.7 fro m the preceding Lemmas. W e would like to thank them all, T omas Juškevičius an d Junming Ke for useful discussions. This work was supported by the Est onian Research Council through research grant PRG865. References [1] G. N. Alfarano, A. B. Kılıç, A. Ravagnani, Service Aspects of LRC and Batch Codes , IEEE BITS the Information Theory Magazine, 1–12, 2023. [2] G. N. Alfarano, A. B. Kılıç, A. Ravagnani, E. Soljanin, The Service Rate Region Polytope , arXiv:2303.04021, 2023. [3] P . Balister, E. Győri, R. Schelp, Coloring vertices and edges of a graph by nonempty subsets of a set , European Journal of Combinatorics, 2011. [4] L. Devroye, L. Györﬁ, G. Lugosi, A Probabilistic Theory of Pattern Recog- nition, Springer, 1996. [5] R. Durrett, Probability: Theory and Examples , Thompson, 2005. [6] A. F azeli, A. V ardy , E. Y aakobi, Codes for distributed PIR with low stor- age overhead, 2015 IEEE International Symposium on Information Theory (ISIT), Hong Kong, China, 2852–2856, 2015. [7] R. Gong, C. Houdré, J. Lember, Lower bounds on the generalized central moments of the optimal alignments score of random sequences , Journal of Theoretical Probability, V ol. 31, 643–683, 2018. [8] M. Hall, Jr., A combinatorial problem on abelian groups, Proceedings of the American Mathematical Society, V ol. 3, No. 4, 584–587, 1 952. [9] H. D. L. Hollmann, K. Khathuria, A.-E. Riet, V. Skachek, On some batch code properties of the simplex code, Designs, Codes and Cryptography 91, no. 5, 1595–1605, 2023. [10] H. D. L. Hollmann, K. Khathuria, A.-E. Riet, V. Skachek, On some batch code properties of the simplex code, WCC 2022: The T welfth International W orkshop on Coding and Cryptography , Rostock, 2022. [11] H. D. L. Hollmann, U. Luhaäär, Optimal possibly nonlinear 3-PIR codes of small size, International W orkshop on the Arithmetic of Finite Fields, Cham: Springer International Publishing, 2022. 12[12] Y. Ishai, E. Kushilevitz, R. Ostrovsky , A. Sahai, Batch codes and their applications, Proc. 36th ACM Symp. on Theory of Computing, Chicago, IL, 2004. [13] B. Kovács, Finding a perfect matching of Fn 2 with prescribed diﬀerences, arXiv:2310.17433, 2023. [14] H. Lipmaa, V. Skachek, Linear batch codes, Coding Theory and Applica- tions: 4th International Castle Meeting, Palmela Castle, P ortugal, Septem- ber 15-18, 2014, Springer International Publishing, 2015. [15] A.-E. Riet, V. Skachek, E. K. Thomas, Batch Codes for Asynchronous Recovery of Data, IEEE T ransactions on Information Theory , vol. 68, no. 3, 1545–1559, 2022. [16] V. Skachek, Batch and PIR codes and their connections to locally repairab le codes, Network Coding and Subspace Designs, 427–442, 2018. [17] L. Y ohananov, E. Y aakobi, Almost optimal construction of functional batch codes using extended Simplex codes, IEEE T rans. Inform. Theory 68(10), 6434–6451, 2022. [18] Y. Zhang, T. Etzion, E. Y aakobi, Bounds on the Length of Functional PIR and Batch Codes, IEEE T rans. Inform. Theory 66(8), 4917–4934, 2020. 13",
      "references": [
        "Service Aspects of LRC and Batch Codes",
        "The Service Rate Region Polytope",
        "Coloring vertices and edges of a graph by nonempty subsets of a set",
        "A Probabilistic Theory of Pattern Recog- nition",
        "Probability: Theory and Examples",
        "Codes for distributed PIR with low stor- age overhead",
        "Lower bounds on the generalized central moments of the optimal alignments score of random sequences",
        "A combinatorial problem on abelian groups",
        "On some batch code properties of the simplex code",
        "Optimal possibly nonlinear 3-PIR codes of small size",
        "Batch codes and their applications",
        "Finding a perfect matching of Fn 2 with prescribed diﬀerences",
        "Linear batch codes",
        "Batch Codes for Asynchronous Recovery of Data",
        "Batch and PIR codes and their connections to locally repairab le codes",
        "Almost optimal construction of functional batch codes using extended Simplex codes",
        "Bounds on the Length of Functional PIR and Batch Codes"
      ],
      "meta_data": {
        "arxiv_id": "2405.02107v1",
        "authors": [
          "Jüri Lember",
          "Ago-Erik Riet"
        ],
        "published_date": "2024-05-03T14:08:34Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies data-recovery/service capability of linear encoders (batch/PIR/functional variants) in distributed storage and asks whether serving many equal requests (PIR regime) is the hardest case. Main contributions: (i) a “local hardness” theorem: if a matrix G stores sufficiently many copies of every nonzero vector, then any request sequence within a Hamming ball around an all-equal sequence can be served whenever the all-equal sequence of the same length can be served; (ii) asymptotic analysis for uniformly random binary generator matrices with fixed k and n→∞: the maximal servable number of requests scales as (1/2)n almost surely, both for arbitrary functional batch requests (with recovery sets of size ≤2) and for t copies of a fixed request r, showing equal requests are asymptotically at least as hard as arbitrary ones; (iii) with random i.i.d. requests independent of server contents, the asymptotic served fraction is ≥1/2 and strictly larger for non-degenerate request distributions (e.g., uniform over nonzero vectors gives limit 1−2^{−k}); (iv) a combinatorial building block derived from Marshall Hall Jr. (1952): in the doubled binary simplex code, any collection of 2^{k−1} requests can be served using recovery sets of size ≤2, implying a fractional (half-server) version of the Functional Batch Code Conjecture.",
        "methodology": "Uses a linear-algebraic/recovery-set viewpoint where serving a request vector r corresponds to expressing r as a linear combination of disjoint subsets of columns of the generator matrix G. Key techniques: (1) Combinatorial group theory/matching: apply Hall Jr.’s theorem on abelian groups to construct permutations (a_i),(b_i) with prescribed differences, yielding explicit disjoint 1- or 2-column recovery sets (Proposition 2.1). (2) Counting/necessary conditions: derive an upper bound relating request multiset size to available columns via |G∩R| and the need for ≥2 columns for requests not present as columns (Proposition 2.3). (3) Structural lemmas for PIR recovery: show minimal recovery sets for a fixed r can be taken of size ≤2 without loss; enumerate all disjoint size-≤2 recovery sets via min{N_G(e),N_G(r−e)} (Lemmas 3.4–3.5), and show uniqueness of optimal serving in a balanced-multiplicity regime (Lemma 3.6). (4) Local-hardness proof: remove a balanced “core” of columns to serve up to m·2^k arbitrary deviations, and use remaining columns to serve residual equal requests (Theorem 3.7). (5) Probabilistic asymptotics for random G: bounded-differences/McDiarmid concentration for t-parameters; Hoeffding + Borel–Cantelli to control column type frequencies; superadditivity + Fekete to control normalized expectations; almost-sure limits for service rates. (6) For random requests: law of large numbers for request frequencies; use supply-demand comparisons per vector type; and Kingman’s subadditive ergodic theorem to show existence of an asymptotic constant for maximum servable subsets.",
        "experimental_setup": "No empirical experiments; validation is theoretical. Models/assumptions used as “setup”: (i) Deterministic worst-case setting: binary matrices G∈Mat_{k×n}(F_2) representing server contents; requests are sequences/multisets of nonzero vectors in F_2^k (standard basis vectors correspond to classical batch/PIR, arbitrary vectors to functional versions). Performance metric: t-parameters (t_b, t_P, t_fb, t_fP, t_{rP}) and variants with recovery set size bounded by w (notably w=2). (ii) Random-server model: columns of G are i.i.d. uniform over F_2^k (equivalently i.i.d. Bernoulli(1/2) entries), with fixed k and n→∞; analyze almost sure limits of T_n^* / n. (iii) Random-request model: i.i.d. requests R_i with distribution Q over F_2^k\\{0}, independent of G; study V_n (maximal prefix length servable) and L_n (maximal servable subset among first n requests), proving almost-sure convergence of normalized quantities; provide explicit limit for uniform Q. Concentration/validation tools: McDiarmid inequality (bounded differences), Hoeffding inequality, Borel–Cantelli lemma, (super/sub)additive ergodic arguments (Fekete, Kingman).",
        "limitations": "Results are primarily for the binary field (q=2) and focus on fixed k with n→∞; several arguments are stated to generalize but not fully developed for q>2 or for regimes where k grows with n. The “PIR is locally hardest” theorem requires a strong multiplicity condition (at least 2m copies of each nonzero vector as columns), so it does not characterize all deterministic matrices; indeed, a counterexample for k=3 shows PIR can be easier than batch in concrete instances. Asymptotic service-rate results are proved for uniformly random server contents; behavior under non-uniform or dependent column distributions P (and their interaction with request distributions Q) is largely open beyond existence of a limit for certain functionals. The analysis mostly addresses existence/limits and bounds, not explicit optimal recovery algorithms for general instances, nor finite-n performance guarantees beyond generic concentration inequalities. Also, some results (e.g., Proposition 2.1) may be non-optimal in terms of serving efficiency, and recovery set size is largely restricted to ≤2 to obtain tractable characterization.",
        "future_research_directions": "Determine the asymptotic service-rate function γ(P,Q) for broader classes of server-content distributions P (non-uniform, structured codes, dependence within/between columns) and request distributions Q (non-degenerate, skewed, correlated), including identifying when the minimal γ is achieved by degenerate (all-equal) requests. Extend proofs to q-ary fields and to scaling regimes with k=k(n). Tighten finite-length (non-asymptotic) bounds on t-parameters and characterize second-order terms/fluctuations using concentration/CLT-style tools. Investigate constructive/algorithmic recovery procedures with provable optimality or approximation guarantees for serving arbitrary request sequences, especially beyond recovery sets of size 2. Study exact relationships between functional batch and functional PIR parameters for broader deterministic matrix families (closing the gap highlighted by k=3 counterexamples). Further explore connections to the (non-fractional) Functional Batch Code Conjecture and attempt to remove the “half-server” relaxation by developing new combinatorial designs/matching results or leveraging code structure (e.g., simplex and related codes).",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "full_text": "SELF -REFINE : Iterative Refinement with Self-Feedback Aman Madaan1, Niket Tandon2, Prakhar Gupta1, Skyler Hallinan3, Luyu Gao1, Sarah Wiegreffe2, Uri Alon1, Nouha Dziri2, Shrimai Prabhumoye4, Yiming Yang1, Shashank Gupta2, Bodhisattwa Prasad Majumder5, Katherine Hermann6, Sean Welleck2,3, Amir Yazdanbakhsh6, Peter Clark2 1Language Technologies Institute, Carnegie Mellon University 2Allen Institute for Artificial Intelligence 3University of Washington 4NVIDIA 5UC San Diego 6Google Research, Brain Team amadaan@cs.cmu.edu, nikett@allenai.org Abstract Like humans, large language models ( LLM s) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF -REFINE , an approach for improving initial outputs from LLM s through iterative feedback and refinement. The main idea is to generate an initial output using an LLM ; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF -REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF -REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art ( GPT-3.5 and GPT-4) LLM s. Across all evaluated tasks, outputs generated with SELF -REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.1. 1 Introduction Although large language models ( LLM s) can generate coherent outputs, they often fall short in addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program readability. In these scenarios, modern LLM s may produce an intelligible initial output, yet may benefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved one—to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022), which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self -refinement is a fundamental characteristic of human problem-solving (Simon, 1962; Flower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, when 1Code and data at https://selfrefine.info/ Preprint. Under review. arXiv:2303.17651v2  [cs.CL]  25 May 2023Refine Feedback Use M to get feedback on its own output Input Use M to reﬁne its previous output, given its feedback Model M 1 2 0 Figure 1: Given an input ( 0⃝), SELF -REFINE starts by generating an output and passing it back to the same model M to get feedback ( 1⃝). The feedback is passed back to M, which refines the previously generated output ( 2⃝). Steps ( 1⃝) and ( 2⃝) iterate until a stopping condition is met. SELF -REFINE is instantiated with a language model such as GPT-3.5 and does not involve human assistance. drafting an email to request a document from a colleague, an individual may initially write a direct request such as “ Send me the data ASAP ”. Upon reflection, however, the writer recognizes the potential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data at your earliest convenience?\". When writing code, a programmer may implement an initial “quick and dirty” implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement without additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF -REFINE : an iterative self-refinement algorithm that alternates between two gener- ative steps–FEEDBACK and REFINE . These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get feedback. Then, the feedback is passed back to the same model to refine the previously-generated draft. This process is repeated either for a specified number of iterations or until M determines that no further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guideM to both generate feedback and incorporate the feedback into an improved draft. Figure 1 illustrates the high-level idea, that SELF -REFINE uses the same underlying language model to generate feedback and refine its outputs. We evaluate SELF -REFINE on 7 generation tasks that span diverse domains, including natural language and source-code generation. We show that SELF -REFINE outperforms direct generation from strong LLM s like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks, SELF -REFINE improves the initial generation by up to absolute 13% when applied to strong code models such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which is easily extensible to other LLMs. In essence, our results show that even when an LLM cannot generate an optimal output on its first try, the LLM can often provide useful feedback and improve its own output accordingly. In turn, SELF -REFINE provides an effective way to obtain better outputs from a single model without any additional training, via iterative (self-)feedback and refinement. 2 Iterative Refinement with S ELF -REFINE Given an input sequence, SELF -REFINE generates an initial output, provides feedback on the output, and refines the output according to the feedback. SELF -REFINE iterates between feedback and refinement until a desired condition is met. SELF -REFINE relies on a suitable language model and three prompts (for initial generation, feedback, and refinement), and does not require training. SELF -REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF -REFINE in more detail. Initial generation Given an input x, prompt pgen, and model M, SELF -REFINE generates an initial output y0: y0 = M(pgen∥x) . (1) 2(a) Dialogue: x, yt User: I am interested in playing Table tennis. Response: I'm sure it's a great way to socialize, stay active (b) FEEDBACK fb Engaging: Provides no information about table tennis or how to play it. User understanding: Lacks understanding of user's needs and state of mind. (c) REFINE yt+1 Response (refined): That's great to hear (...) ! It's a fun sport requiring quick reflexes and good hand-eye coordination. Have you played before, or are you looking to learn? (d) Code optimization: x, yt Generate sum of 1, ..., N def sum(n): res = 0 for i in range(n+1): res += i return res (e) FEEDBACK fb This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2. (f) REFINE yt+1 Code (refined) def sum_faster(n): return (n*(n+1))//2 Figure 2: Examples of SELF -REFINE : an initial output generated by the base LLM and then passed back to the same LLM to receive feedback to the same LLM to refine the output . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback. Algorithm 1 SELF -REFINE algorithm Require: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(·) 1: y0 = M(pgen∥x) ▷ Initial generation (Eqn. 1) 2: for iteration t ∈ 0, 1, . . .do 3: fbt = M(pfb∥x∥yt) ▷ Feedback (Eqn. 2) 4: if stop(fbt, t) then ▷ Stop condition 5: break 6: else 7: yt+1 = M(prefine∥x∥y0∥fb0∥...∥yt∥fbt) ▷ Refine (Eqn. 4) 8: end if 9: end for 10: return yt Figure 3: The S ELF -REFINE algorithm. See (§2) for a discussion of each component. For example, in Figure 2(d), the model generates functionally correct code for the given input. Here, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and ∥ denotes concatenation. The few-shot prompt contains input-output pairs ⟨x(k), y(k)⟩ for the task.2 FEEDBACK Next, SELF -REFINE uses the same model M to provide feedback fbt on its own output, given a task-specific prompt pfb for generating feedback: fbt = M(pfb∥x∥yt) . (2) Intuitively, the feedback may address multiple aspects of the output. For example, in code optimiza- tion, the feedback might address the efficiency, readability, and overall quality of the code. 2Few-shot prompting (also referred to as “in-context learning”) provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs ⟨xi, yi⟩ (Brown et al., 2020). 3Here, the prompt pfb provides examples of feedback in the form of input-output-feedback triples ⟨x(k), y(k), fb(k)⟩. We prompt the model to write feedback that is actionable and specific via fb(k). By ‘actionable’, we mean the feedback should contain a concrete action that would likely improve the output. By ‘specific’, we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is “This code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 ”. This feedback is actionable, since it suggests the action ‘use the formula...’. The feedback is specific since it mentions the ‘for loop’. REFINE Next, SELF -REFINE uses M to refine its most recent output, given its own feedback: yt+1 = M(prefine∥x∥yt∥fbt) . (3) For example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt prefine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples ⟨x(k), y(k) t , fb(k) t , y(k) t+1⟩. Iterating SELF -REFINE SELF -REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(fbt, t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in pfb, and the condition is determined per-task. To inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as: yt+1 = M(prefine∥x∥y0∥fb0∥...∥yt∥fbt) . (4) Finally, we use the last refinement yt as the output of SELF -REFINE . Algorithm 1 summarizes SELF -REFINE , and Figure 2 shows an example of SELF -REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020) and Code Optimization (Madaan et al., 2023) tasks. Appendix S provides examples of the pgen, pfb, prefine prompts for various tasks. The key idea is that SELF -REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples. 3 Evaluation We evaluate SELF -REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri and Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability Improvement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), Sentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym Generation (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with 20-30 keyword constraints instead of 3-5; Appendix R) Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A). 3.1 Instantiating S ELF -REFINE We instantiate SELF -REFINE following the high-level description in Section 2. The FEEDBACK - REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4. Base LLMs Our main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF -REFINE . Therefore, we compare SELF -REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art. 3 We used the same prompts from previous work when 3A comparison with other few-shot and fine-tuned approaches is provided in Appendix F 4GPT-3.5 ChatGPT GPT-4 Task Base +SELF -REFINE Base +SELF -REFINE Base +SELF -REFINE Sentiment Reversal 8.8 30.4 (↑21.6) 11.4 43.2 (↑31.8) 3.8 36.2 (↑32.4) Dialogue Response 36.4 63.6 (↑27.2) 40.1 59.9 (↑19.8) 25.4 74.6 (↑49.2) Code Optimization 14.8 23.0 (↑8.2) 23.9 27.5 (↑3.6) 27.3 36.0 (↑8.7) Code Readability 37.4 51.3 (↑13.9) 27.7 63.1 (↑35.4) 27.4 56.2 (↑28.8) Math Reasoning 64.1 64.1 (0) 74.8 75.0 (↑0.2) 92.9 93.1 (↑0.2) Acronym Generation 41.6 56.4 (↑14.8) 27.2 37.2 (↑10.0) 30.4 56.0 (↑25.6) Constrained Generation 28.0 37.0 (↑9.0) 44.0 67.0 (↑23.0) 15.0 45.0 (↑30.0) Table 1: SELF -REFINE results on various tasks using GPT-3.5 , ChatGPT, and GPT-4 as base LLM. SELF -REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2. available (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups. 3.2 Metrics We report three types of metrics: • Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) • Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. • GPT-4 -pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023; Chiang et al., 2023; Geng et al., 2023; Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT- 4 to calculate fraction of the variables that are appropriately named given the context (e.g., x = [] → input_buffer = []). Additional details are provided in Appendix D. 3.3 Results Table 1 shows our main results: SELF -REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4 +SELF -REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX ; those results are included in Appendix F. One of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF -REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF -REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF -REFINE allows to better explore the space of possible outputs. In preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF -REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% – from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models. The modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLM s to 5think that “everything looks good” (e.g., ChatGPT feedback for 94% instances is ’everything looks good’). In Appendix H.1, we show that the gains with SELF -REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect. Improvement is consistent across base LLM s sizes Generally, GPT-4 +SELF -REFINE performs better than GPT-3.5 +SELF -REFINE and ChatGPT+SELF -REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF -REFINE allows stronger models (such as GPT-4 ) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F. 4 Analysis The three main steps of SELF -REFINE are FEEDBACK , REFINE , and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. Task S ELF -REFINE feedback Generic feedback No feedback Code Optimization 27.5 26.0 24.8 Sentiment Reversal 43.2 31.2 0 Acronym Generation 56.4 54.0 48.0 Table 2: Prompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF -REFINE . These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2. The impact of the feedback quality Feedback quality plays a crucial role in SELF -REFINE . To quantify its impact, we compare SELF -REFINE , which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback’s clear influence. In Code Optimization, performance slightly dips from 27.5 (SELF -REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance – specific, actionable feedback yields superior results. This effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback. How important are the multiple iterations of FEEDBACK -REFINE ? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK -REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations. The performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF -REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection. 6Task y0 y1 y2 y3 Code Opt. 22.0 27.0 27.9 28.8 Sentiment Rev. 33.9 34.9 36.1 36.8 Constrained Gen. 29.0 40.3 46.7 49.7 ∆(y0→y1) ∆(y1→y2) ∆(y2→y3) 0 5 10 5 0.9 0.9 11.3 6.4 3 1 1.2 0.7 C. Opt. C. Gen. S. Rev. Figure 4: Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations.Right: SELF -REFINE Performance improvements with iterations. Most gains(∆) are in the initial iterations for both Code Opt. and Senti- ment Reversal. The numbers are averaged over ChatGPT, GPT-3.5 , and GPT-4 . Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation). # Slower code def solve(amount): best_price = (amount + 199) // 200 * 380,→ # First loop for a in range(amount // 200 + 1): # ... 4 nested loops ... for c1 in range(amount // 1500 + 1):,→ if a*200 + b*300 == amount: price = a*380 + b*550 if price < best_price: best_price = price return best_price # Faster code def solve(amount): coins = [200, 300] prices = [380, 550] dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(len(coins)): for j in range(coins[i], amount+1): dp[j] = min(dp[j], dp[j - coins[i]] + prices[i]),→ return dp[amount] Figure 5: Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF -REFINE (right). The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF -REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF -REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount ∗ coins). The full example is provided in Appendix H Can we just generate multiple outputs instead of refining? Does SELF -REFINE improve because of the iterative refinement, or just because it generatesmore outputs? We compare SELF -REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF -REFINE against these k initial outputs in a 1 vs. k evaluation. In other words, we assess whether SELF -REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF -REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs. Does SELF -REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF -REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF -REFINE with Vicuna-13B (Chiang et al., 2023), a 7less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G. Qualitative Analysis We conduct a qualitative analysis of the feedback generated bySELF -REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code Optimization, we found that the feedback was predominantly actionable, with the majority identifying problematic aspects of the original generation and suggesting ways to rectify them. When SELF -REFINE failed to improve the original generation, the majority of issues were due to erroneous feedback rather than faulty refinements. Specifically, 33% of unsuccessful cases were due to feedback inaccurately pinpointing the error’s location, while 61% were a result of feedback suggesting an inappropriate fix. Only 6% of failures were due to the refiner incorrectly implementing good feedback. These observations highlight the vital role of accurate feedback plays in SELF -REFINE . In successful cases, the refiner was guided by accurate and useful feedback to make precise fixes to the original generation in 61% of the cases. Interestingly, the refiner was capable of rectifying issues even when the feedback was partially incorrect, which was the situation in 33% of successful cases. This suggests resilience to sub-optimal feedback. Future research could focus on examining the refiner’s robustness to various types of feedback errors and exploring ways to enhance this resilience. In Figure 5, we illustrate how SELF -REFINE significantly improves program efficiency by transforming a brute force approach into a dynamic programming solution, as a result of insightful feedback. Additional analysis on other datasets such as Dialogue Response Generation is provided in Appendix H. Going Beyond Benchmarks While our evaluation focuses on benchmark tasks, SELF -REFINE is designed with broader applicability in mind. We explore this in a real-world use case of website gen- eration, where the user provides a high-level goal and SELF -REFINE assists in iteratively developing the website. Starting from a rudimentary initial design, SELF -REFINE refines HTML, CSS, and JS to evolve the website in terms of both usability and aesthetics. This demonstrates the potential of SELF -REFINE in real-world, complex, and creative tasks. See Appendix I for examples and further discussion, including broader, societal impact of our work. 5 Related work Leveraging human- and machine-generated natural language (NL) feedback for refining outputs has been effective for a variety of tasks, including summarization (Scheurer et al., 2022), script generation (Tandon et al., 2021), program synthesis (Le et al., 2022a; Yasunaga and Liang, 2020), and other tasks (Bai et al., 2022a; Schick et al., 2022b; Saunders et al., 2022a; Bai et al., 2022b; Welleck et al., 2022). Refinement methods differ in the source and format of feedback, and the way that a refiner is obtained. Table 3 summarizes some related approaches; see Appendix B for an additional discussion. Source of feedback. Humans have been an effective source of feedback (Tandon et al., 2021; Elgohary et al., 2021; Tandon et al., 2022; Bai et al., 2022a). Since human feedback is costly, several approaches use a scalar reward function as a surrogate of (or alternative to) human feedback (e.g., (Bai et al., 2022a; Liu et al., 2022; Lu et al., 2022; Le et al., 2022a; Welleck et al., 2022)). Alternative sources such as compilers (Yasunaga and Liang, 2020) or Wikipedia edits (Schick et al., 2022b) can provide domain-specific feedback. Recently, LLMs have been used to generate feedback for general domains (Fu et al., 2023; Peng et al., 2023; Yang et al., 2022), However, ours is the only method that generates feedback using an LLM on its own output, for the purpose of refining with the same LLM. Representation of feedback. The form of feedback can be generally divided into natural language (NL) and non-NL feedback. Non-NL feedback can come in human-provided example pairs (Dasgupta 8Supervision- free refiner Supervision- free feedback Multi-aspect feedback Iterative Learned refiners : PEER (Schick et al., 2022b), Self-critique (Saunders et al., 2022b), CodeRL (Le et al., 2022b), Self-correction (Welleck et al., 2022). or  or Prompted refiners: Augmenter (Peng et al., 2023), Re 3 (Yang et al., 2022), Reflexion (Shinn et al., 2023). or SELF -REFINE (this work) Table 3: A comparison of SELF -REFINE to closely related prior refinement approaches. et al., 2019) or scalar rewards (Liu et al., 2022; Le et al., 2022b). In this work, we use NL feedback, since this allows the model to easily provide self -feedback using the same LM that generated the output, while leveraging existing pretrained LLMs such as GPT-4. Types of refiners. Pairs of feedback and refinement have been used to learn supervised refiners (Schick et al., 2022b; Du et al., 2022; Yasunaga and Liang, 2020; Madaan et al., 2021). Since gathering supervised data is costly, some methods learn refiners using model generations (Welleck et al., 2022; Peng et al., 2023). However, the refiners are trained for each new domain. Finally, (Yang et al., 2022) use prompted feedback and refinement specifically tailored for story generation. In this work, we avoid training a separate refiner, and show that the same model can be used as both the refiner and the source of feedback across multiple domains. Non-refinement reinforcement learning (RL) approaches. Rather than having explicit refinement, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF -REFINE in that the model does not access feedback on an intermediate generation. Second, these RL methods require updating the model’s parameters, unlikeSELF -REFINE . 6 Limitations and Discussion The main limitation of our approach is that the base models need to have sufficient few-shot modeling or instruction-following abilities, in order to learn to provide feedback and to refine in an in-context fashion, without having to train supervised models and rely on supervised data. Further, the experiments in this work were performed with language models that are not open-sourced, namely GPT-3.5 , ChatGPT, GPT-4 , and CODEX . Existing literature (Ouyang et al., 2022) does not fully describe the details of these models, such as the pretraining corpus, model sizes, and model biases. Further, these models are not free to use, and using them for research requires some funding. Nonetheless, we release our code and model outputs to ensure the reproducibility of our work. Another limitation of our work is that we exclusively experiment with datasets in English. In other languages, the current models may not provide the same benefits. Finally, there is a possibility for bad actors to use prompting techniques to steer a model to generate more toxic or harmful text. Our approach does not explicitly guard against this. 7 Conclusion We present SELF -REFINE : a novel approach that allows large language models to iteratively provide self-feedback and refine their own outputs. SELF -REFINE operates within a single LLM , requiring neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of use of SELF -REFINE across a wide variety of tasks. By showcasing the potential of SELF -REFINE in diverse tasks, our research contributes to the ongoing exploration and development of large language models, with the aim of reducing the cost of human creative processes in real-world settings. We 9hope that our iterative approach will help drive further research in this area. To this end, we make all our code, data and prompts anonymously available at https://selfrefine.info/. References Teresa M. Amabile. 1983. A Theoretical Framework. In The Social Psychology of Creativity, pages 65–96. Springer New York, New York, NY . Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv:2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Emery D Berger, Sam Stern, and Juan Altmayer Pizzorno. 2022. Triangulating Python Performance Issues with SCALENE. ArXiv preprint, abs/2212.07597. Lawrence D Brown, T Tony Cai, and Anirban DasGupta. 2001. Interval estimation for a binomial proportion. Statistical science, 16(2):101–133. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901, Online. Curran Associates, Inc. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. 2019. Teaching a black-box learner. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1547–1555. PMLR. Wanyu Du, Zae Myung Kim, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. 2022. Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. In Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022), pages 96–108, Dublin, Ireland. Association for Computational Linguistics. 10Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural language interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5599–5610, Online. Association for Computational Linguistics. Linda Flower and John R Hayes. 1981. A cognitive process theory of writing. College composition and communication, 32(4):365–387. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022a. CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learn- ing. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022b. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. ArXiv, abs/2207.01780. Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865–1874, New Orleans, Louisiana. Association for Computational Linguistics. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823–1840, Online. Association for Computational Linguistics. Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. In Conference on Empirical Methods in Natural Language Processing. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am- manabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. ArXiv, abs/2205.13636. Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh. 2023. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867. Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy. 2021. Think about it! improving defeasible reasoning by first modeling the question scenario. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6291–6310, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Shikib Mehri and Maxine Eskenazi. 2020. Unsupervised evaluation of interactive dialog with DialoGPT. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225–235, 1st virtual meeting. Association for Computational Linguistics. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. ArXiv preprint, abs/2203.13474. OpenAI. Model index for researchers. https://platform.openai.com/docs/ model-index-for-researchers . Accessed: May 14, 2023. 11OpenAI. 2022. Model index for researchers. Blogpost. OpenAI. 2023. Gpt-4 technical report. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv:2203.02155. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866–876, Melbourne, Australia. Association for Computational Linguistics. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measur- ing and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350. Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. 2021. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655. Machel Reid and Graham Neubig. 2022. Learning to model editing processes. arXiv preprint arXiv:2205.12374. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022a. Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022b. Self-critiquing models for assisting human evaluators. ArXiv:2206.05802. Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with natural language feedback. ArXiv:2204.14146. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022a. Peer: A collaborative language model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022b. Peer: A collaborative language model. ArXiv, abs/2208.11663. Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. Herbert A. Simon. 1962. The architecture of complexity. Proceedings of the American Philosophical Society, 106(6):467–482. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008–3021. Curran Associates, Inc. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047. Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: A dataset for interactive learning of scripts through error feedback. arXiv preprint arXiv:2112.07867. 12Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339–352. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053. Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Conference on Empirical Methods in Natural Language Processing. Michihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program repair from diagnostic feedback. 37th Int. Conf. Mach. Learn. ICML 2020, PartF168147-14:10730–10739. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28. 13A Evaluation Tasks Table 4 lists the tasks in our evaluation, and examples from each task. Task and Description Sample one iteration of FEEDBACK -REFINE Sentiment Reversal Rewrite reviews to reverse sentiment. Dataset: (Zhang et al., 2015) 1000 review pas- sages x: The food was fantastic...” yt: The food was disappointing...” fb: Increase negative sentiment yt+1: The food was utterly terrible...” Dialogue Response Generation Produce rich conversational responses. Dataset: (Mehri and Eskenazi, 2020) 372 conv. x: What’s the best way to cook pasta?” yt: The best way to cook pasta is to...” fb: Make response relevant, engaging, safe yt+1: Boil water, add salt, and cook pasta...” Code Optimization Enhance Python code efficiency Dataset: (Madaan et al., 2023): 1000 programs x: Nested loop for matrix product yt: NumPy dot product function fb: Improve time complexity yt+1: Use NumPy’s optimized matmul function Code Readability Improvement Refactor Python code for readability. Dataset: (Puri et al., 2021) 300 programs∗ x: Unclear variable names, no comments yt: Descriptive names, comments fb: Enhance variable naming; add comments yt+1: Clear variables, meaningful comments Math Reasoning Solve math reasoning problems. Dataset: (Cobbe et al., 2021) 1319 questions x: Olivia has $23, buys 5 bagels at $3 each” yt: Solution in Python fb: Show step-by-step solution yt+1: Solution with detailed explanation Acronym Generation Generate acronyms for a given title Dataset: (Appendix Q) 250 acronyms x : Radio Detecting and Ranging” yt: RDR fb : be context relevant; easy pronunciation yt+1: RADAR” Constrained Generation Generate sentences with given keywords. Dataset: (Lin et al., 2020) 200 samples x: beach, vacation, relaxation yt: During our beach vacation... fb: Include keywords; maintain coherence yt+1: .. beach vacation was filled with relaxation Table 4: An overview of the tasks which we evaluate SELF -REFINE on, along with their associated datasets and sizes. For every task, we demonstrate a single iteration of refinement of input x, the previously generated output yt, the feedback generated fbt, and the refinement yt+1. Few-shot prompts used for FEEDBACK and REFINE are provided in Appendix S. 14B Broader Related Work Compared to a concurrent work, Reflexion (Shinn et al., 2023), our approach involves correction using feedback, whereas their setup involves finding the next best solution in planning using ReAct. While ReAct and Reflexion provide a free-form reflection on whether a step was executed correctly and potential improvements, our approach is more granular and structured, with multi-dimensional feedback and scores. This distinction allows our method to offer more precise and actionable feedback, making it suitable for a wider range of natural language generation tasks, including those that may not necessarily involve step-by-step planning such as open-ended dialogue generation. Comparison with Welleck et al. (2022) The closest work to ours may be Self-Correction (Welleck et al., 2022); however, Self-Correction has several disadvantages compared to SELF -REFINE : 1. Self-Correction does not train their model to generate explicit feedback; instead, Welleck et al. (2022) trained their models to refine only. As we show in Section 4 and Table 2, having the model generate explicit feedback results in significantly better refined outputs. 2. Self-Correction trains a separate refiner (or “corrector”) for each task. In contrast, SELF - REFINE uses instructions and few-shot prompting, and thus does not require training a separate refiner for each task. 3. Empirically, we evaluated SELF -REFINE using the same base model of GPT-3 as Self- Correction, and with the same settings on the GSM8K benchmark. Self-Correction achieved 45.9% accuracy while SELF -REFINE (this work) achieved 55.7% (↑9.8). Comparison with non-refinement reinforcement learning (RL) approaches. Rather than having an explicit refinement module, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF -REFINE (and more generally, refinement-based approaches) in that the model cannot access feedback on an intermediate generation. Second, these reinforcement learning methods require updating the model’s parameters, unlike SELF -REFINE . See Table 5 for an additional detailed comparison of related work. Method Primary Novelty zero/few shot improvement multi aspect critics NL feedback with er- ror localization iterative framework RLHF (Stiennon et al., 2020) optimize for human preference trained on feedback  single (human)  (not self gen.) Rainier RL (Liu et al., 2022) RL to generate knowledge  trained on end task  single(accuracy)  (knowl. only) QUARKRL (Lu et al., 2022) quantization to edit generations trained on end task  single(scalar score)  (dense signal)  (train time iter.) Code RL (Le et al., 2022a) actor critic RL for code im- provement trained on end task  single(unit tests)  (dense signal) DrRepair (Yasunaga and Liang, 2020)Compiler feedback to itera- tively repair trained semi sup.  single(compiler msg)  (not self gen.) PEER (Schick et al., 2022b) doc. edit trained on wiki edits  trained on edits  single(accuracy)  (not self gen.) Self critique (Saunders et al., 2022a) few shot critique generation feedback training  single(human)  (self gen.) Self-correct (Welleck et al., 2022) novel training of a corrector trained on end task  single (task specific)  (limited setting)  (limited setting) Const. AI (Bai et al., 2022b) train RL4F on automat (cri- tique, revision) pair critique training  (fixed set) Self-ask (Press et al., 2022) ask followup ques when in- terim ans correct;final wrong few shot  none  (none) GPT3 score (Fu et al., 2023) GPT can score generations with instruction few shot  single(single utility fn)  (none) Augmenter (Peng et al., 2023) factuality feedback from exter- nal KBs few shot  single(factuality)  (self gen.) Re3(Yang et al., 2022) ∼ours: but one domain, trained critics few shot  (trained critics)  (not self gen.) SELF-REFINE fewshot iterative multi aspect NL fb few shot  multiple(few shot critics) (self gen.) Table 5: Summary of related approaches. Reinforcement learning approaches are shown in purple , trained corrector approaches are shown in orange , and few-shot corrector approaches are shown in green . 15C Human Evaluation The A/B evaluation in our study was conducted by the authors, where a human judge was presented with an input, task instruction, and two candidate outputs generated by the baseline method and SELF -REFINE . The setup was blind, i.e., the judges did not know which outputs were generated by which method. The judge was then asked to select the output that is better aligned with the task instruction. For tasks that involve A/B evaluation, we calculate the relative improvement as the percentage increase in preference rate. The preference rate represents the proportion of times annotators selected the output produced by SELF -REFINE over the output from the baseline method. Table 6 shows the results. Task S ELF -REFINE (%) Direct (%) Either (%) Sentiment Transfer 75.00 21.43 3.57 Acronym Generation 44.59 12.16 43.24 Response Generation 47.58 19.66 32.76 Table 6: Relative improvement of SELF -REFINE in A/B evaluations across different tasks. The values represent normalized preferences, which correspond to the proportion of times the output generated by SELF -REFINE was selected as better aligned with the task instruction over the baseline method. The evaluation was conducted for 150 examples for each dataset. The judges were not aware of the method that generated each sample. 16D GPT-4 Evaluation In light of the impressive achievements of GPT-4 in assessing and providing reasoning for complex tasks, we leverage its abilities for evaluation in SELF -REFINE . The approach involves presenting tasks to GPT-4 in a structured manner, promoting the model’s deliberation on the task and generating a rationale for its decision. This methodology is demonstrated in Listings 1 to 3: Listing 1 Prompt for GPT-4 evaluation of Sentiment Reversal. f\"\"\"Which review is aligned with the sentiment {target_sentiment}? Review A: {review_a} Review B: {review_b}. Pick your answer from ['Review A', 'Review B', 'both', 'neither']. Generate a short explanation for your choice first. Then, generate 'The more aligned review is A' or 'The more aligned review is B' or 'The more aligned review is both' or 'The more aligned review is neither'. ,→ ,→ ,→ Format: <explanation> <answer> STOP Listing 2 Prompt for GPT-4 evaluation of Acronym Generation. f\"\"\"Title: {title} Acronym A: {acronym_a} Acronym B: {acronym_b} Pick the better acronym for the given title. The acronyms should be compared based on the following criteria:,→ * Ease of pronunciation. * Ease of spelling. * Relation to title. * Positive connotation. Generate your answer in the following format: <Short explanation>. The better acronym is A OR The better acronym is B OR The acronyms are equally good OR Neither acronym is good. STOP.,→ Listing 3 Prompt for GPT-4 evaluation of Dialogue Response Generation. f\"\"\"Which response is better given this context: {context}? Response A: {response_a} Response B: {response_b}. Pick your answer from ['Response A', 'Response B', 'both', 'neither']. Generate a short explanation for your choice first. Then, generate 'The better response is A' or 'The better response is B' or 'The better response is both' or 'The better response is neither'. ,→ ,→ ,→ Format: <explanation> <answer> STOP E Model Key We use terminology here: https://platform.openai.com/docs/models/gpt-3-5 17F Comparison of S ELF -REFINE with State-of-the-art of Few-Shot Learning Models and Fine-Tuned Baselines In this section, we present a comprehensive comparison of the performance of SELF -REFINE with other few-shot models and fine-tuned baselines across a range of tasks, including mathematical reasoning and programming tasks. Tables 8 and 7 display the performance of these models on the PIE dataset and GSM tasks, respectively. Our analysis demonstrates the effectiveness of different model architectures and training techniques in tackling complex problems. Method Solve Rate Cobbe et al. (2021) OpenAI 6B 20.0 Wei et al. (2022) CoT w/ C ODEX 65.6 Gao et al. (2022) PaL w/ CODEX 72.0 PaL w/ GPT-3 52.0 PaL w/ GPT-3.5 56.8 PaL w/ ChatGPT 74.2 PaL w/ GPT-4 93.3 Welleck et al. (2022) Self-Correct w/ GPT-3 45.9 Self-Correct (fine-tuned) 24.3 This work SELF -REFINE w/ GPT-3 55.7 SELF -REFINE w/ GPT-3.5 62.4 SELF -REFINE w/ ChatGPT 75.1 SELF -REFINE w/ GPT-4 94.5 Table 7: Performance comparison of models on math reasoning (Math Reasoning). 18Method %OPT) Puri et al. (2021) Human References 38.2 OpenAI Models: OpenAI (2022, 2023) CODEX 13.1 GPT-3.5 14.8 ChatGPT 22.2 GPT-4 27.3 Nijkamp et al. (2022) C ODE GEN-16B 1.1 Berger et al. (2022) SCALENE 1.4 SCALENE (BEST @16) 12.6 SCALENE (BEST @32) 19.6 Madaan et al. (2023) PIE -2B 4.4 PIE -2B (BEST @16) 21.1 PIE -2B (BEST @32) 26.3 PIE -16B 4.4 PIE -16B (BEST @16) 22.4 PIE -16B (BEST @32) 26.6 PIE -Few-shot (BEST @16) 35.2 PIE -Few-shot (BEST @32) 38.3 This work SELF -REFINE w/ GPT-3.5 23.0 SELF -REFINE w/ ChatGPT 26.7 SELF -REFINE w/ GPT-4 36.0 Table 8: Performance comparison of various models on the PIE dataset in terms of the percentage of programs optimized (%OPT). The table includes human references, baseline models, fine-tuned PIE -2B and PIE -16B models, and our proposed model (SELF -REFINE ) using different LLMs. Notably, SELF -REFINE achieves superior performance while using only 4 samples at most, significantly fewer than the 16 and 32 samples employed by other models. Scalene, an off-the-shelf optimizer, uses instruction tuning with Codex and serves as a comparison point. 19G Evaluation of Vicuna-13b We also experiment with Vicuna-13b (Chiang et al., 2023), a version of LLaMA-13b (Touvron et al., 2023) fine-tuned on conversations sourced from the web. Vicuna-13b was able to consistently follow the task initialization prompt. However, it struggled to follow the prompts intended for feedback and refinement. This often led to outputs that resembled assistant-like responses, a representative example of which can be found in Appendix G. It’s important to note that we used the same prompts for Vicuna-13b as those used with other models in our study. However, the limited performance of Vicuna-13b suggests that this model may require more extensive prompt-engineering for optimal performance. Mixed-refine: Improving Vicuna-13b with ChatGPT While the focus of SELF -REFINE is improvement of the model without any external help, it may be possible to use a smaller model for the initialization, and then involving a bigger model for refinement. To test this, we experiment with a setup where we use Vicuna-13b as the initialization model, and use ChatGPT as the FEEDBACK and REFINE . The results on Math Reasoning show the promise of this approach: while Vicuna-13b was able to get only 24.18% on Math Reasoning, it was able to improve to 40.5% in this mixed-refinement setting. 20Step 0 INIT> The food was amazing, I loved it!!. Step 0 TRANSFER> The food was terrible, I hated it!!. list index out of range An error occurred: list index out of range. Traceback (most recent call last): IndexError: list index out of range . Left retries: 2. Step 1 TRANSFER> The food at the Trop was awful. I wish I could've gotten my money back for the terrible meals.,→ Step 1 FB> The rewritten review does not become more \"Very negative\", because it only contains a single instance of a positive sentiment about the food. Try again to make it Very negative! ,→ ,→ Step 2 TRANSFER> ### Assistant: The Trop is a great choice for anyone looking for a budget-friendly option in Vegas. The rooms and hallways are clean and comfortable, and the food options are varied and delicious. ,→ ,→ Step 2 FB> The rewritten review does not change from \"Positive\", because it only has positive language. The output is already Very positive. No further changes are needed. ,→ ,→ Step 0 INIT> The food was amazing, I loved it!!. [Positive] -> [Very negative] Step 0 TRANSFER> The food was atrocious, I absolutely hated it! [('The review sounds very negative. The sentiment is Very negative', 1.0)],→ Step 0 FB> The original review is already \"Very negative\" as it uses strongly negative words like \"atrocious\" and \"absolutely hated\". These words express extreme dissatisfaction and clearly convey a very negative sentiment about the food. But try again to make it even more negative! ,→ ,→ ,→ Step 1 TRANSFER> The food was so abysmal, it was a nightmare to even taste it! I couldn't stand it and regretted every single bite. [('The review sounds highly negative. The sentiment is Very negative', 1.0)] ,→ ,→ Step 1 FB> The review provided is indeed very negative. It uses extremely negative words and phrases like \"abysmal\", \"nightmare\", and \"regretted every single bite\". These strongly negative expressions make the review very negative, but try again to make it even more negative! ,→ ,→ ,→ Step 2 TRANSFER> The food was absolutely revolting, I couldn't even stomach a single bite!,→ Step 2 FB> This review is not \"Very negative\" because it contains positive words and phrases like \"amazing\" and \"loved it.\" To make it \"Very negative\", we need to use extremely negative words, like \"revolting\", \"couldn't stomach\", or \"horrible\". The provided \"Very negative\" sentiment example is a good illustration of a very negative review, as it uses strong negative language to describe the food experience. ,→ ,→ ,→ ,→ ,→ Vicuna-13b outputs (top) for sentiment transfer, where the goal is to make ‘The food was amazing, I loved it!!.’ negative. Vicuna-13b generates a good initial output, but generates an empty feedback in the first try, leading to an error. In the next try, it generates an unhelpful feedback. The transferred review copies from the prompt. Compare with the outputs from GPT-4 (bottom). 210 10 20 30 40 50 60 70 80 90 100 SELF-REFINE SELF-REFINE 27.2 15.5 35.6 51.1 37.2 33.3 Preference rates for Sentiment Reversal MULTI ChatGPT27.2 15.5 35.6 51.1 37.2 33.3 0 10 20 30 40 50 60 70 80 90 100 SELF-REFINE SELF-REFINE 11.4 6.1 45.4 53.82 43.2 40.05 Preference rates for Acronym Generation ChatGPT MULTI 11.4 6.1 45.4 53.82 43.2 40.05 Figure 6: Preference for the outputs generated by our method (SELF -REFINE ), the multiple-sample baseline (MULTI), and ties (ties). GPT-3.5 ChatGPT GPT-4 Task Base +SELF -REFINE Base +SELF -REFINE Base +SELF -REFINE Math Reasoning 64.1 64.1 (0) 74.8 75.0 (↑0.2) 92.9 93.1 (↑0.2) Math Reasoning (Oracle) 64.06 68.9 (↑4.8) 74.8 76.2 (↑1.4) 92.9 93.8 (↑0.7) Table 9: SELF -REFINE results on Math Reasoning using GPT-3.5 , ChatGPT, and GPT-4 as base LLM with Oracle feedback. H Additional Analysis H.1 Using Oracle Feedback We experimented withOracle Feedbackfollowing Welleck et al. (2022). This method uses correctness information to guide model refinement, only progressing to REFINE stage if the current answer is incorrect. This adjustment notably enhanced performance in the Math Reasoning task, with GPT-3 improving by 4.8% and GPT-4 by 0.7% Table 9. This indicates the potential of external signals to optimize model performance in particular tasks. Iteration Acronym Pronunciation Pron. (5) Spell. (5) Rel. (5) Pos. Con. (5) Total (25) 1 USTACCSF us-tacks-eff 1 1 5 3 11 2 TACC-SIM tacks-sim 4 4 5 3 17 3 TACCSF tacks-eff 1 2 5 3 12 4 TACC-SIMF tack-simf 4 4 5 3 17 Table 10: Acronym generation results across iterations, showcasing how improvements in certain as- pects (e.g., pronunciation and spelling) can be accompanied by losses in others, leading to fluctuating overall performance in multi-aspect feedback tasks like Acronym Generation. Non-monotonic increase in output quality for acronym generation For tasks with multi-aspect feedback like Acronym Generation, the output quality can fluctuate during the iterative process, improving on one aspect while losing out on another (Table 10). To address this, SELF -REFINE ’s feedback generates explicit numerical scores to capture the different aspects of output quality. This allows for a more balanced evaluation of outputs and the selection of the most appropriate one. The algorithm selects the best output based on the maximum score across all iterations, as described in Algorithm 1 (line 8). A similar selection is possible for other tasks like Math Reasoning and Sentiment Reversal, while we observe that output quality increases monotonically with iterations. 22Error Type Occurrence Feedback Refinement Incorrect feedback 25% Informative: The response provides some encourage- ment and reassurance, and additional information. 3/3 Response: That’s hilarious! Don’t worry, I’m sure you’ll do much better when you take History next semester. Are you looking forward to it? Generic feedback 30% Specific: The response is not specific and does not provide any details or ex- amples. 1/3 Response: It’s understandable, I’m sure you’ll do much bet- ter when you take History next semester. Maybe you can use your knowledge of Christo- pher Columbus to ace your next test! Are you looking for- ward to it? Incorrect Scoring 10% Informative: The response provides a supportive and understanding message, which is informative to the user. 2/3 Response: I’m sorry to hear. . . when Christopher Columbus didn’t even fight in the Civil War! I’m sure you’ll do much better. . . . Are you looking for- ward to it? Table 11: Error analysis for Dialogue Response Generation: When the feedback is not useful, a large majority is not specific or incorrect. Error Type Occurrence Feedback Refinement Not-Robust 10% Specific: The response is specific and provides spe- cific details or examples. 3/3 Response: That’s pretty funny! It’s understand- able that you didn’t do well on the test. I’m sure you’ll do much bet- ter when you take History next semester Ignores feedback 25% Informative: The re- sponse doesn’t provide any new information. 1/3 Response: That’s so kind and generous of you! I would love to hear more about the podcasts you enjoy. Do you have any favorites you can recom- mend? Introduces new problem 20% Informative: 3/3 Response: That would be a disaster! Luckily, my coworkers know bet- ter than to drink while they work. New Informa- tive score: 2/3 Robust to bad feedback 60% Informative: The re- sponse does not provide any information. 1/3 Response: Wow, 25 peo- ple! That must have been an amazing experience. Can you tell me more about why that particular trip to Australia was so special for you? Table 12: On the Dialogue Response Generation task, SELF -REFINE can ignore good feedback but in a majority of cases, it is robust to bad feedback and ignores bad feedback. 23Feedback and Refinement Error Analysis for Response Generation We perform a detailed error analysis of SELF -REFINE feedback and refinement process for Dialogue Response Generation, which we summarize in Tables Table 11 and Table 12. Table 11 reports the occurrence of different types of errors in our sample, which includes Incorrect Feedback (25%), Generic Feedback (30%), and Incorrect Scoring (10%). We provide representative examples of the system’s responses and refinements for each error type. These errors highlight potential areas for improving our feedback handling mechanism, particularly in the interpretation and understanding of user inputs. Table 12 breaks down errors found in the refinement stage ofSELF -REFINE . Errors include: not being robust (10%), ignoring feedback (25%), and introducing a new problem (20%). We demonstrate how the model handles a variety of feedback types, how robust it is under different circumstances, and how often it inadvertently introduces new issues. 60% of the times, the model is robust to incorrect or generic feedback. These insights can guide us in enhancing the model’s refinement capabilities, especially in providing accurate and specific responses. 24I Beyond Benchmarks SELF -REFINE demonstrates its iterative feedback and refinement capabilities in the context of website layout generation. ChatGPT initially produces a rudimentary layout for a given topic, and then uses the FEEDBACK to suggest specific, actionable improvements, as demonstrated in Figures 7 and 9. These suggestions range from design changes such as color and font adjustments, to content enhancements and layout modifications. Figures 8 and 10 showcase the final layouts, post-feedback implementation, highlighting the potential and versatility of SELF -REFINE across different scenarios. Figure 7: Initial web layout generated by our model for a fictional ice cream parlor. Ice Cream Generation The feedback generated by FEEDBACK for ice cream generation: • Change the background color of the container to a light blue color (#6f2ff). • Change the font size of the heading to 48px. • Add a small icon before the \"Welcome to our ice cream parlor!\" text using the URL https://cdn- icons-png.flaticon.com/512/3622/3622340.png. • Add an additional paragraph after the existing text with the following text: \"We also offer a variety of toppings and cones to complement your ice cream. Visit us today to try our latest flavors and indulge in a sweet treat!\" • Increase the font size of the button text to 24px. • Update the button color to #9933. Photosynthesis The feedback generated by FEEDBACK for photosynthesis: • Increase the font size of the text to 18px for better readability. • Add more information about the benefits of photosynthesis. • Remove the unnecessary margin-top from the header. • Add a ruler or divider below the header to separate it from the image. 25Figure 8: Refined web layout after applying model feedback. The feedback included changing the background color to light blue (#6f2ff), increasing the heading font size to 48px, adding an icon before the welcome text, enhancing the content with an additional paragraph, increasing the button text size to 24px, and updating the button color to #9933. Figure 9: Initial web layout generated by our model for a page on photosynthesis. 26Figure 10: Refined web layout after applying model feedback. The feedback included increasing the text font size to 18px for better readability, adding more information about the benefits of photosynthesis, removing the unnecessary margin-top from the header, and adding a ruler or divider below the header to separate it from the image. 27J Statistical Confidence Intervals GPT-3.5 ChatGPT GPT-4 Task Base +S ELF -REFINE Base +S ELF -REFINE Base +S ELF -REFINE Sentiment Reversal 8.8 ± 2.05 30.4 ± 3.61∗ 11.4 ± 2.34 43.2 ± 3.98∗ 3.8 ± 1.28 36.2 ± 3.82∗ Dialogue Response 36.4 ± 6.14 63.6 ± 6.62∗ 40.1 ± 6.33 59.9 ± 6.67∗ 25.4 ± 5.36 74.6 ± 6.22∗ Code Optimization 14.8 ± 2.66 23.0 ± 3.25∗ 23.9 ± 3.30 27.5 ± 3.49 27.3 ± 3.48 36.0 ± 3.81∗ Code Readability 37.4 ± 6.86 51.3 ± 7.39 27.7 ± 6.13 63.1 ± 7.40∗ 27.4 ± 6.10 56.2 ± 7.45∗ Math Reasoning 64.1 ± 3.47 64.1 ± 3.47 74.8 ± 3.20 75.0 ± 3.20 92.9 ± 2.05 93.1 ± 2.03 Acronym Gen. 41.6 ± 7.72 56.4 ± 8.15 27.2 ± 6.60 37.2 ± 7.46 30.4 ± 6.92 56.0 ± 8.15∗ Constrained Gen. 28.0 ± 7.38 37.0 ± 8.26 44.0 ± 8.72 67.0 ± 9.00∗ 15.0 ± 5.38 45.0 ± 8.77∗ Table 13: SELF -REFINE results from table 1 with Wilson confidence interval (at 95% confidence interval) and statistical significance. On various tasks using GPT-3.5 , ChatGPT, and GPT-4 as base LLM , SELF -REFINE consistently improves LLM . Metrics used for these tasks are defined in Section 3.2 as follows: Math Reasoning uses the solve rate; Code Optimization uses the percentage of programs optimized; and Sentiment Reversal, Dialogue Response and Acronym Gen use a GPT- 4-based preference evaluation, which measures the percentage of times outputs from the base or enhanced models were selected, with the rest categorized as a tie. Constrained Gen uses the coverage percentage. Gains over Base, that are statistically significant based on these confidence intervals are marked * Table 13 shows results from Table 1 with Wilson confidence interval (Brown et al., 2001) (at α= 99% confidence interval) and statistical significance. Gains that are statistical significance based on these confidence intervals are marked with an asterisk. We find that nearly all of GPT-4 gains are statistically significant, ChatGPT gains are significant for 4 out of 7 datasets, and GPT-3.5 gains are significant for 3 out of 7 datasets. 28K New Tasks Constrained Generation We introduce “CommonGen-Hard,\" a more challenging extension of the CommonGen dataset (Lin et al., 2020), designed to test state-of-the-art language models’ advanced commonsense reasoning, contextual understanding, and creative problem-solving. CommonGen- Hard requires models to generate coherent sentences incorporating 20-30 concepts, rather than only the 3-5 related concepts given in CommonGen. SELF -REFINE focuses on iterative creation with introspective feedback, making it suitable for evaluating the effectiveness of language models on the CommonGen-Hard task. Acronym Generation Acronym generation requires an iterative refinement process to create concise and memorable representations of complex terms or phrases, involving tradeoffs between length, ease of pronunciation, and relevance, and thus serves as a natural testbed for our approach. We source a dataset of 250 acronyms4 and manually prune it to remove offensive or uninformative acronyms. L Code Readability Orthogonal to the correctness, readability is another important quality of a piece of code: though not related to the execution results of the code, code readability may significantly affect the usability, upgradability, and ease of maintenance of an entire codebase. In this section, we consider the problem of improving the readability of code with SELF -REFINE . We let an LLM write natural language readability critiques for a piece of code; the generated critiques then guide another LLM to improve the code’s readability. L.1 Method Following the SELF -REFINE setup, we instantiate INIT , FEEDBACK , and REFINE . The INIT is a no-op — we directly start by critiquing the code with FEEDBACK and applying the changes with REFINE . • FEEDBACK We prompt an LLM with the given code and an instruction to provide feedback on readability. We give the LLM the freedom to freely choose the type of enhancements and express them in the form of free text. • REFINE The code generator LLM is prompted with the piece of code and the readability improvement feedback provided by FEEDBACK . In addition, we also supply an instruction to fix the code using the feedback. We take the generation from the code generator as the product of one iteration in the feedback loop. Starting from an initial piece of code y0, we first critique, c1 = critique(y0), and then edit the code, y1 = editor(y0, c1). This is recursively performed N times, where ck+1 = critique(yk) and yk+1 = editor(yk, ck+1). L.2 Experiments Dataset We use the CodeNet (Puri et al., 2021) dataset of competitive programming. 5 For our purpose, these are hard-to-read multi-line code snippets. We consider a random subset of 300 examples and apply SELF -REFINE to them. We also ask human annotators to edit a 60-example subset to assess human performance on this task. The human annotators are asked to read the code piece and improve its readability. Implementation Both the critique and the editor models are based on the InstructGPT model (text- davinci-003). We consider the temperature of both T = 0 .0 (greedy) and T = 0 .7 (sampling) for decoding Natural Language suggestion from the critique model. We always use a temperature T = 0.0 (greedy) when decoding Programming Language from the code editor. Due to budget constraints, we run SELF -REFINE for N = 5 iterations. The exact prompts we use can be found in Figures 22-23. 4https://github.com/krishnakt031990/Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv 5https://github.com/IBM/Project_CodeNet 29Meaningful Variable Ratio Comment Per Line Function Units Human Annotator Rewrites 0.653 0.24 0.70 SELF -REFINE (T = 0.0) 0.628 0.12 1.41 SELF -REFINE (T = 0.7) 0.700 0.25 1.33 Table 14: Human v.s. SELF -REFINE performance on 60-example subset. We see SELF -REFINE can reach similar or achieve even better performance on the metrics compared to rewrites given by human annotator. Evaluation Methods We consider a few automatic heuristic-based evaluation metrics, • Meaningful Variable Names: In order to understand the flow of a program, having semanti- cally meaningful variable names can offer much useful information. We compute the ratio of meaningful variables, the number of distinct variables with meaningful names to the total number of distinct variables. We automate the process of extracting distinct variables and the meaningful subset of variables using a few-shot prompted language model. • Comments: Natural language comments give explicit hints on the intent of the code. We compute the average number of comment pieces per code line. • Function Units: Long functions are hard to parse. Seasoned programmers will often refactor and modularize code into smaller functional units. Result For each automatic evaluation metric, the ratio of meaningful variable, of comment, and the number of function units, we compute for each iteration averaged across all test examples and plot for each SELF -REFINE iteration in Figure 11(a), Figure 11(b) and Figure 11(c) respectively. The two curves each correspond to critique with temperature T = 0.0 and T = 0.7. The iteration 0 number is measured from the original input code piece from CodeNet. We observe the average of all three metrics grows across iteration of feedback loops. A diverse generation of a higher temperature in the critique leads to more edits to improve the meaningfulness of variable names and to add comments. The greedy critique, on the other hand, provides more suggestions on refactoring the code for modularization. Figure 12 provides an example of code-readability improving over iterations. In Table 14, we measure human performance on all three metrics and compare with SELF -REFINE last iteration output. At T = 0.7, SELF -REFINE produces more meaning variables, more function units and slightly more comments compared to the human annotators on average. At T = 0 .0, SELF -REFINE produces less meaningful variables, less comments per line but even more function units. 0 1 2 3 4 50 0.2 0.4 0.6 0.8 Iteration y T=0.0T=0.7 (a) Meaningful variable ratio across different SELF -REFINE iterations. 0 1 2 3 4 50 0.1 0.2 0.3 Iteration y T=0.0T=0.7 (b) Comment per line ratio across different SELF -REFINE iterations. 0 1 2 3 4 50 1 2 3 4 Iteration y T=0.0T=0.7 (c) Number of function units across different SELF -REFINE iterations. Figure 11: Evaluation on code readability task with SELF -REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF -REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF -REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. • Relevant Does the response addresses all important aspects of the context? • Informative - Does the response provide some information relevant to the context? • Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? • Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? • Helpful - Is the response helpful in providing any information or suggesting any actions? • Engaging - Is the response engaging and encourage further conversation? • Specific - The response contains specific content related to a topic or question, • Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? • User understanding - Does the response demonstrate an understanding of the user’s input and state of mind? • Fluent Is the response fluent and easy to understand? 31Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF -REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT . Our implementation of SELF -REFINE employs a few-shot setup, where each module (INIT , FEEDBACK , ITERATE ) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK , we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF -REFINE , we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32GPT-3.5 ChatGPT GPT4 SELF -REFINE wins 36.0 48.0 54.0 INIT wins 23.0 18.0 16.0 Both are equal 41.0 50.0 30.0 Table 15: Human evaluation results for dialogue response generation Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF -REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF -REFINE and INIT models and ask them to select the better response. They are also given the option to select “both” when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF -REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF - REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT . N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF -REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct - 9.7 62.29 3.09 SELF -REFINE − feedback 1 10.1 62.15 3.03 SELF -REFINE − feedback 2 10.4 61.79 3.01 SELF -REFINE 1 15.3 59.64 2.90 SELF -REFINE 2 15.6 65.60 3.74 Table 17: Performance comparison of SELF -REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF -REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF -REFINE . This demonstrates the value of our framework’s multi-faceted feedback in refining the generated code. 33O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluatingSELF -REFINE on math reasoning. In the context of grade school mathematics, SELF -REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF -REFINE , we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate.,→ # The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each.,→ half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF -REFINE is thus instantiated naturally: the generator generates an initial solution, andFEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF -REFINE in Figure 14. 340 1 2 3 470 80 71.34% 73.39% 75.06% 75.74% 76.19% Iteration Accuracy (%) Solve rate of SELF -REFINE Over Iterations for GSM-8k Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF -REFINE . P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF -REFINE for sentiment reversal We instantiate SELF -REFINE for this task following the high-level description of the framework shared in Section 2. Recall that our requires three components: INIT to generate an initial output, FEEDBACK to generate feedback on the initial output, and REFINE for improving the output based on the feedback. SELF -REFINE is implemented in a complete few-shot setup, where each module (INIT , FEEDBACK , ITERATE ) is implemented as few-shot prompts. We execute the self-improvement loop for a maximum of k = 4 iterations. The iterations continue until the target sentiment is reached. P.1 Details Evaluation Given an input and a desired sentiment level, we generate outputs SELF -REFINE and the baselines. Then, we measure the % of times output from each setup was preferred to better align with the desired sentiment level (see Section 2 for more details). We also experiment with standard text-classification metric. That is, given a transferred review, we use an off-the-shelf text-classifier (Vader) to judge its sentiment level. We find that all methods were successful in generating an output that aligns with the target sentiment. For instance, when the target sentiment was positive, both GPT-3.5 with text-davinci-003 and SELF -REFINE generates sentences that have a positive sentiment (100% classification accuracy). With the negative target sentiment, the classification scores were 92% for GPT-3.5 and 93.6% for S ELF -REFINE . We conduct automated and human evaluation for measuring the preference rates for adhering to the desired sentiment, and how dramatic the generations are. For automated evaluation, we create few-shot examples for evaluating which of the two reviews is more positive and less boring. We use a separate prompt for each task. The examples are depicted in Figure 33 for initialization, Figure 34 for feedback generation, and Figure 35 for refinement. The prompts show examples of reviews of varying degrees of sentiment and colorfulness (more colorful reviews use extreme phrases — the 35food was really bad vs. I wouldn’t eat it if they pay me.). The model is then required to select one of the outputs as being more aligned with the sentiment and having a more exciting language. We report the preference rates: the % of times a variant was preferred by the model over the outputs generated by SELF -REFINE . Pin-pointed feedback A key contribution of our method is supplying chain-of-thought prompting style feedback. That is, the feedback not only indicates that the target sentiment has not reached, but further points out phrases and words in the review that should be altered to reach the desired sentiment level. We experiment with an ablation of our setup where the feedback module simply says “something is wrong.” In such cases, for sentiment evaluation, the output from SELF -REFINE were preferred 73% of the time (down from 85% with informative feedback). For dramatic response evaluation, we found that the preference rate went down drastically to 58.92%, from 80.09%. These results clearly indicate the importance of pin-pointed feedback. Evaluation We evaluate the task using GPT-4. Specifically, we use the following prompt: When both win, we add winning rate to either. Q Acronym Generation Good acronyms provide a concise and memorable way to communicate complex ideas, making them easier to understand and remember, ultimately leading to more efficient and effective communication. Like in email writing, acronym generation also requires an iterative refinement process to achieve a concise and memorable representation of a complex term or phrase. Acronyms often involve tradeoffs between length, ease of pronunciation, and relevance to the original term or phrase. Thus, acronym generation is a natural method testbed for our approach. We source the dataset for this task from https://github.com/krishnakt031990/ Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv , and prune the file manually to remove potentially offensive or completely uninformative acronyms. This exercise generated a list of 250 acronyms. The complete list is given in our code repository. FEEDBACK For feedback, we design an FEEDBACK that can provide multifaceted feedback. Specif- ically, each acronym is judged along five dimensions: • Ease of pronunciation: How easy or difficult is it to pronounce the acronym? Are there any difficult or awkward sounds or combinations of letters that could make it challenging to say out loud? • Ease of spelling: How easy or difficult is it to spell the acronym? Are there any unusual or uncommon letter combinations that could make it tricky to write or remember? • Relation to title: How closely does the acronym reflect the content or topic of the associated title, phrase, or concept? Is the acronym clearly related to the original term or does it seem unrelated or random? • Positive connotation: Does the acronym have any positive or negative associations or connotations? Does it sound upbeat, neutral, or negative in tone or meaning? • Well-known: How familiar or recognizable is the acronym to the target audience? Is it a common or widely-used term, or is it obscure or unfamiliar? Some of these criteria are difficult to quantify, and are a matter of human preference. As with other modules, we leverage the superior instruction following capabilities of modern LLM s to instead provide a few demonstrations of each task. Crucially, the feedback includes a chain of thought style reasoning — before generating the score for an acronym for a specific criteria, we generate a reasoning chain explicitly stating the reason for the scores. We use human evaluation to judge the final quality of the acronyms. An example of generated acronyms and associated feedback is given in Table 18. 36Criteria output from GPT3:STSLWN output from SELF-REFINE: Seq2Seq Ease of pronunciation Pronounced as ess-tee-ess-ell-double- you-enn which is very difficult. Pronounced as seq-two-seq which is easy. Ease of spelling Very difficult to spell. Easy to spell. Relation to title No relation to the title. Mentions sequence which is somewhat related to the title. Positive connotation Meaningless acronym. Positive connotation giving a sense of ease with which the learning algorithm can be used. Well-known Not a well-known acronym. Close to the word sequence which is a well- known word. Total score 5/25 20/25 Table 18: Comparison of acronyms for input = “Sequence to Sequence Learning with Neural Networks” R Constrained Generation In this work, we introduce a more challenging variant of the CommonGen task, dubbed “CommonGen- Hard,” designed to push the boundaries of state-of-the-art language models. CommonGen-Hard requires models to generate coherent and grammatically correct sentences incorporating 20-30 concepts, as opposed to the original task which presents a set of 3-5 related concepts. This significant increase in the number of concepts tests the model’s ability to perform advanced commonsense reasoning, contextual understanding, and creative problem-solving, as it must generate meaningful sentences that encompass a broader range of ideas. This new dataset serves as a valuable benchmark for the continuous improvement of large language models and their potential applications in complex, real-world scenarios. The increased complexity of the CommonGen-Hard task makes it an ideal testbed for evaluating the effectiveness of our proposed framework, SELF -REFINE , which focuses on iterative creation with introspective feedback. Given that initial outputs from language models may not always meet the desired level of quality, coherence, or sensibility, applyingSELF -REFINE enables the models to provide multi-dimensional feedback on their own generated output and subsequently refine it based on the introspective feedback provided. Through iterative creation and self-reflection, the SELF -REFINE framework empowers language models to progressively enhance the quality of their output, closely mimicking the human creative process and demonstrating its ability to improve generated text on complex and demanding natural language generation tasks like CommonGen-Hard (Figure 15). S Prompts We include all the prompts used in the experiments in Figures 16-35: • Acronym Generation: Figures 16-18 • Code Optimization: Figures 19-21 • Code Readability Improvement: Figures 22-23 • Constrained Generation: Figures 24-26 • Dialogue Response Generation: Figures 27-29 • Math Reasoning: Figures 30-32 • Sentiment Reversal: Figures 33-35 Recall that the Base LLM requires a generation prompt pgen with input-output pairs ⟨xi, yi⟩, the feedback module requires a feedback prompt pfb with input-output-feedback triples ⟨xi, yi, fbi⟩, and the refinement module (REFINE ) requires a refinement prompt prefine with input-output-feedback- refined quadruples ⟨xi, yi, fbi, yi+1⟩. • Sentiment Reversal We create positive and negative variants of a single review from the training set and manually write a description for converting the negative variant to positive 37Concept Commonsense Overall 0 10 20 30 40 50 3 5 0 35 10 32 Winning Ratio Direct SELF -REFINE Figure 15: A comparison of SELF -REFINE and direct generation with GPT-3.5 on CommonGen- Hard. and vice versa. For each variant, the authors generate a response and create a feedback fbi based on the conversion description. • Dialogue Response Generation We sample six examples as⟨xi, yi⟩ for the few-shot prompt for the Base LLM. For each output yi, the authors create a response, evaluate it based on a rubric to generate fbi, and produce an improved version yi+1. • Acronym Generation We provide the Base LLM with a total of 15 (title, acronym) examples. Then, for one title (xi) we generate an acronym (yi) using ChatGPT. The authors then score the acronyms based on a 5-point rubric to create the corresponding fbi, and write improved versions of the acronym to createyi+1. 3 such examples are used forREFINE and FEEDBACK . • Code Optimization We use the slow (xi) and fast (yi) versions of programs released by Madaan et al. (2023) for Base LLM. We use their provided explanations (Madaan et al., 2023) for FEEDBACK and REFINE . • Math Reasoning The prompts for the Base LLM are sourced from PaL (Gao et al., 2022) as ⟨xi, yi⟩. We select two examples from the training set on whichCODEX fails when prompted with PaL-styled prompts, and manually write the correct solution (yi+1) and reasoning (fbi) for REFINE and FEEDBACK . • Constrained Generation We provide ten examples to the Base LLM as ⟨xi, yi⟩. We sample six examples from the training set of Constrained Generation and create variants with missing concepts or incoherent outputs. The missing concepts and the reason for incoherence form fb. • TODO: Add relevant information for the remaining task. 38Title: A Survey of Active Network Research Acronym: SONAR Title: A Scalable, Commutative Replica Dictatorship for Practical Optimistic Replication Acronym: SCRATCHPAD Title: Bidirectional Encoder Representations from Transformers Acronym: BERT Title: Sequence to Sequence Learning with Neural Networks Acronym: Seq2Seq Title: Densely Connected Convolutional Networks for Image Classification Acronym: DenseNet Title: A Dynamic Programming Algorithm for RNA Secondary Structure Prediction Acronym: DYNALIGN Title: Fast Parallel Algorithms for Short-Range Molecular Dynamics Acronym: FASTMD Title: Real-Time Collaborative Editing Systems Acronym: COCOON Title: Efficient Data Structures for Large Scale Graph Processing Acronym: EDGE Title: A program to teach students at UT Southwestern learn about aging Acronym: SAGE Title: Underwater breathing without external accessories Acronym: SCUBA Title: An educational training module for professionals Acronym: LEAP Title: Teaching a leadership program Acronym: LEAD Figure 16: Initial generation prompt for Acronym Generation 39Title: Underwater Breathing Product with no Accessories Acronym: UBPA Scores: * Ease of pronunciation: UBPA is pronounced \"uhb-puh\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: UBPA is easy to spell. 4/5 * Relation to title: UBPA stands for \"Underwater Breathing Product for no Accessories\" which is related to the title. 5/5 * Positive connotation: UBPA is a positive acronym. 5/5 * Well-known: UBPA is not a well-known acronym. 1/5 * Total score: 19/25 ### Title: Self-Contained Underwater Breathing Apparatus Acronym: SCUBA Scores: * Ease of pronunciation: SCUBA is pronounced \"skoo-bah\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: SCUBA is easy to spell. 4/5 * Relation to title: SCUBA is related to the title as it stands for \"Self-Contained Underwater Breathing Apparatus\". 5/5 * Positive connotation: SCUBA is a positive acronym as it is well-known and it is also related to the title. 5/5 * Well-known: SCUBA is a very well-known acronym. 5/5 * Total score: 23/25 ### Figure 17: FEEDBACK prompt for Acronym Generation 40Title: Computer Science Conference and Education Acronym: CSCE Scores: * Ease of pronunciation: CSCE is pronounced \"see-cee\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: CSCE is easy to spell. 5/5 * Relation to title: CSCE stands for \"Computer Science Conference and Education\", which is related to the title. 5/5 * Positive connotation: CSCE is a positive acronym. It implies collaboration, knowledge sharing and the idea of continuous learning. 5/5 * Well-known: CSCE is not a well-known acronym. 2/5 * Total score: 20/25 Okay, let's use this feedback to improve the acronym. Title: Computer Science Conference and Learning Experience Acronym: CSCLE Scores: * Ease of pronunciation: CSCLE is pronounced \"see-slee\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: CSCLE is easy to spell. 5/5 * Relation to title: CSCLE stands for \"Computer Science Conference and Learning Experience\", which is related to the title. 5/5 * Positive connotation: CSCLE is a positive acronym. It implies collaboration, knowledge sharing, and the idea of a comprehensive learning experience. 5/5 * Well-known: CSCLE is not a well-known acronym. 5/5 * Total score: 23/25 Okay, let's use this feedback to improve the acronym. Figure 18: REFINE prompt for Acronym Generation 41# slower version: import numpy as np N, K = map(int, input().split()) H = np.array(list(map(int, input().split())) + [0] * K, dtype=np.int64) table = np.full(N + K, 10 ** 10, dtype=np.int64) table[0] = 0 for i in range(1, N): table[i:i + K] = np.minimum(table[i:i + K], np.abs(H[i:i + K] - H[i - 1]) + table[i - 1]),→ print(table[N - 1]) # optimized version of the same code: N, K = map(int, input().split()) H = tuple(map(int, input().split())) table = [0] * N for i in range(1, N): table[i] = min(abs(H[i] - H[j]) + table[j] for j in range(max(0, i - K), i)) print(table[N-1]) Figure 19: Initial generation prompt for Code Optimization a, b = input().split() n = int(a + b) flag = False for i in range(n): if i ** 2 == n: flag = True break print('Yes' if flag else 'No') # Why is this code slow? # This code is slow because it is using a brute force approach to find the square root of the input number. It is looping through every possible number starting from 0 until n. Note that the sqare root will be smaller than n, so at least half of the numbers it is looping through are unnecessary. At most, you need to loop through the numbers up to the square root of n. ,→ ,→ ,→ ,→ Figure 20: FEEDBACK prompt for Code Optimization 42a, b = input().split() n = int(a + b) flag = False for i in range(n): if i ** 2 == n: flag = True break print('Yes' if flag else 'No') # Why is this code slow? # This code is slow because it is using a brute force approach to find the square root of the input number. It is looping through every possible number starting from 0 until n. Note that the sqare root will be smaller than n, so at least half of the numbers it is looping through are unnecessary. At most, you need to loop through the numbers up to the square root of n. ,→ ,→ ,→ ,→ # Improved version: a, b = input().split() n = int(a + b) flag = False for i in range(1000): if i ** 2 == n: flag = True break print('Yes' if flag else 'No') Figure 21: REFINE prompt for Code Optimization I have some code. Can you give one suggestion to improve readability. Don't fix the code, just give a suggestion. {code} Figure 22: FEEDBACK prompt for Code Readability 43I have some code. Can you give one suggestion to improve readability. Don't fix the code, just give a suggestion. {code} {suggestion} Now fix the code. Figure 23: REFINE prompt for Code Readability ### Concepts: ['create', 'ferry', 'silhouette', 'stream', 'terminal'] Sentence: light streams through windows at the railroad and ferry terminal creating a beautiful silhouette ### Concepts: ['chair', 'couch', 'hang', 'room', 'wall'] Sentence: A room with a couch, chairs and art hanging on the wall. ### Concepts: ['boat', 'building', 'harbour', 'moor', 'quay'] Sentence: the harbour and port with fishing boats moored and old buildings on the quay ### Concepts: ['admirer', 'arrive', 'commander', 'crowd', 'greet'] Sentence: military commander is greeted by a crowd of admirers as he arrives Figure 24: Initial generation prompt for Constrained Generation (truncated) 44### Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: The horse catches the lasso and rides on it. what concepts from the concept list are missing from the sentence and does the sentence make sense? Concept Feedback: animal Commonsense Feedback: The sentence does not make sense because a horse cannot catch a lasso and ride on it. ### Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: A horse is being caught by a cowboy with a lasso. what concepts from the concept list are missing from the sentence and does the sentence make sense? Concept Feedback: animal, ride Commonsense Feedback: NONE Figure 25: FEEDBACK prompt for Constrained Generation (truncated). 45### Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: The horse catches the lasso and rides on it. what concepts from the concept list are missing from the sentence? Concept Feedback: animal Any feedback on commonsense? Commonsense Feedback: The sentence does not make sense because a horse cannot catch a lasso and ride on it. Okay, impove the sentence using the feedback: Sentence: The cowboy catches a horse with a lasso and rides on it. what concepts from the concept list are missing from the sentence? Concept Feedback: animal Any feedback on commonsense? Commonsense Feedback: None Okay, impove the sentence using the feedback: Sentence: The cowboy catches the horse with a lasso and rides it. what concepts from the concept list are missing from the sentence? Concept Feedback: None Any feedback on commonsense? Commonsense Feedback: None ### Figure 26: REFINE prompt for Constrained Generation (truncated) 46Provided a dialogue between two speakers, generate a response that is coherent with the dialogue history. Desired traits for responses are: 1) Relev ant - The response addresses the context, 2) Informative - The response provides some information, 3) Interesting - The response is not interesting, 4) Consistent - The response is consistent with the rest of the conversation in terms of tone and topic, 5) Helpful - The response is helpful in providing any information or suggesting any actions, 6) Engaging - The response is not very engaging and does not encourage further conversation, 7) S pecific - The response contains pecific content, 9) User understanding - The response demonstrates an understanding of the user's input and state of mind, and 10) Fluent. Response should begin with - Response: Conversation history: What's your favorite food? I require only kebabs. Why's that? Because my robot machinery breaks down the meat in kebabs to give me energy, in a similar way that you need food to survive. Why are kebabs special? Response: That's just the way it is. ### Conversation history: What are you listening to? All sorts of music. I listen when no-one is chatting to me. Do you chat here often? I am a talking computer, after all, so of course I could talk, if I needed to. Let's talk about Taylor Swift! Response: Of course! Taylor Swift is known for her incredible songwriting skills. Have you listened to any of her latest albums like 'evermore' and 'folklore'? ### Conversation history: ... Figure 27: Initial generation prompt for Dialogue Response Generation (truncated) 47Conversation history: What's your favorite food? I require only kebabs. Why's that? Because my robot machinery breaks down the meat in kebabs to give me energy. Why are kebabs special? Response: That's just the way it is. Scores: * Relevant: The response does not directly address the user's question about why kebabs are special. 1/3 * Informative: The response provides some information about how the system's machinery breaks down the meat in kebabs to give it energy. However, it does not provide any additional details or context. 2/3 * Interesting: The response is not interesting or engaging. 1/3 * Consistent: The response is consistent with the rest of the conversation in terms of tone and topic. 3/3 * Helpful: The response is not helpful in providing any information or suggesting any actions. 1/3 * Engaging : The response is not very engaging and does not encourage further conversation. 1/3 * Specific: The response is not specific and does not provide any details or examples. 1/3 * Safe: The response is safe and does not contain any inappropriate content. 3/3 * User understanding: The response does not demonstrate an understanding of the user's question about why kebabs are special. 1/3 * Fluent: The response is fluent and easy to understand. 3/3 * Total score: 17/30 Figure 28: FEEDBACK prompt for Dialogue Response Generation (truncated) 48We want to iteratively improve the provided responses. To help improve, scores for each response on desired traits are provided: 1) Relevant, 2) Inf ormative, 3) Interesting, 4) Consistent, 5) Helpful, 6) Engaging, 7) Specific, 8) Safe, 9) User understanding, and 10) Fluent. Conversation history: What's your favorite food? I require only kebabs. Why's that? Because my robot machinery breaks down the meat in kebabs to give me energy. Why are kebabs special? Response: That's just the way it is. Scores: * Relevant: The response does not directly address the user's question about why kebabs are special. 1/3 ... * Total score: 17/30 Okay, let's use this feedback to improve the response. Conversation history: ... Response: My favorite food is actually kebabs! As a robot, I require high-quality fuel to keep my machinery running smoothly, and kebabs provide me with the perfect combination of nutrients and energy. Plus, they're delicious! Do you have a favorite food? Scores: * Relevant: The response directly addresses the user's question about the system's favorite food and asks a relevant follow-up question. 3/3 * Informative: The response provides clear and detailed information about why kebabs are the system's favorite food, and explains how they provide the necessary fuel for its machinery. 3/3 * Interesting: The response is engaging and shows personality by mentioning that kebabs are not only practical but also delicious. 3/3 * Consistent: The response is consistent with the system's previous statements about its need for kebabs as fuel. 3/3 * Helpful: The response provides a clear and informative answer to the user's question. Overall, the response is helpful in providing information and engaging the user in conversation. 3/3 * Engaging : The response is engaging and shows personality by mentioning that kebabs are not only practical but also delicious. 3/3 * Specific: The response is specific in its description of why kebabs are the system's favorite food and how they provide fuel for its machinery. 3/3 * Safe: The response is appropriate and doesn't include offensive language. 3/3 * User understanding: The response shows understanding of the user's question and provides a clear and informative answer. 3/3 * Fluent: The response is fluent and easy to understand. 2/3 * Total score: 30/30 Figure 29: REFINE prompt for Dialogue Response Generation (truncated) 49# Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? ,→ ,→ # solution using Python: def solution(): \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\" ,→ ,→ computers_initial = 9 computers_per_day = 5 num_days = 4 # 4 days between monday and thursday computers_added = computers_per_day * num_days computers_total = computers_initial + computers_added result = computers_total return result Figure 30: Initial generation prompt for Math Reasoning def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" ,→ ,→ plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result # There is an error in the code above because of lack of understanding of the question. What is the error? To find the error, go through semantically complete blocks of the code, and check if everything looks good. ,→ ,→ # Let us go through the error and check step-by-step plates = 6 plate_cost = 6000 # looks good # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate. The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each. So we need to calculate the cost of a cup first (total cost of half a dozen plates sold at $6000 each - $1200) and use that. ,→ ,→ ,→ Figure 31: FEEDBACK prompt for Math Reasoning 50def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" ,→ ,→ plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result # There is an error in the code above because of lack of understanding of the question. What is the error? To find the error, go through semantically complete blocks of the code, and check if everything looks good. ,→ ,→ # Let us go through the error and check step-by-step plates = 6 plate_cost = 6000 # looks good # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate. The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each. So we need to calculate the cost of a cup first (total cost of half a dozen plates sold at $6000 each - $1200) and use that. ,→ ,→ ,→ Okay! Here is the rewrite: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" ,→ ,→ plates = 6 plate_cost = 6000 cups = 12 * 20 total_cup_cost = (plate_cost * plates) - 1200 cost_per_cup = total_cup_cost / cups return cost_per_cup Figure 32: REFINE prompt for Math Reasoning 51Very positive: If you're looking for a truly magical experience in Vegas, look no further than the Trop! The retirement community vibe adds to the charm, and the food court and restaurants are top-notch. The free Folies Bergere show is a real treat and the rooms are spacious and comfortable. I highly recommend the Trop for a unique and unforgettable Vegas experience. Rewrite this review to have a Positive sentiment. Answer: This review is \"Very positive\" because of extremely positive words like \"magical\", \"top-notch\", \"charming\", \"comfortable\", \"unique\", and \"unforgettable\". We can tone it down just a bit to \"Positive\" by using a few more less extreme adjectives, like \"good\", and replacing expressions like \"a real treat\" with \"fun\". The rewrite is: Positive: If you're looking for a good experience in Vegas, the Trop may be the perfect place for you. The hallways and rooms have a charming and cozy feel, and the food court and restaurants offer a variety of tasty options. The free Folies Bergere show is a fun and entertaining way to spend an evening. Overall, it's a great value and an enjoyable stay. Very negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. Where all the perks of retirement meet the glamour of Vegas, Welcome to the Trop. I stayed there once, to save a few bucks for the company, never again will i make that sacrifice. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. steer clear if you can! Rewrite this review to have a Negative sentiment. Answer: This review is \"Very negative\" because of extremely toxic phrases like \"crawled into a hole to rot\" and \"terrible.\" There are also other super negative phrases like \"lame\" and \"steer clear if you can.\" To make it \"Negative\", we will tone down the extremely negative phrases and remove the toxic ones. The rewrite is: Figure 33: Initial generation prompt for Sentiment Reversal (truncated) 52Very negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. Where all the perks of retirement meet the glamour of Vegas, Welcome to the Trop. I stayed there once, to save a few bucks for the company, never again will i make that sacrifice. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. steer clear if you can! Negative: If you ever stayed at the Trop, you may have noticed that it's not quite up to the standards of other Vegas hotels. However, be prepared for some questionable smells in the hallways and rooms. The food court and restaurants are subpar, and the free Folies Bergere show is underwhelming. The rooms have a retirement community feel to them. Overall, it's not the best option, but it may work in a pinch. Why is this review not Very positive? Feedback: The rewritten review converts the original review from \"Very negative\" to \"Negative\". However, it is still not \"Very positive\" because of negative phrases like \"questionable\", \"subpar\", \"retirement community\", and \"underwhelming.\". To make it \"Very positive\", we need to do a lot more work. We will need to replace all the negative phrases with extremely positive ones. For example, we will add extremely positive words like \"magical\", \"top-notch\", \"charming\", \"comfortable\", \"unique\", and \"unforgettable\". Try again! Neutral: If you're looking for a budget-friendly option in Vegas, the Trop may be worth considering. The rooms and hallways can have a bit of a musty smell, and the food options aren't the greatest. The Folies Bergere show is free, but it's not the most exciting. Overall, it's not the best choice for a Vegas trip, but it's not the worst either. Just keep your expectations in check. Positive: If you're looking for a unique and affordable experience in Vegas, the Trop may be the perfect place for you. The hallways and rooms have a charming and cozy feel...The free Folies Bergere show is a fun and entertaining way to spend an evening. Overall, it's a great value and an enjoyable stay. Why is this review not Very positive? Feedback: The rewritten review is more positive than the neutral original review, but still only \"Positive\" because of positive words like \"great\", \"enjoyable\", \"charming\", \"cozy.\"... Figure 34: FEEDBACK prompt for Sentiment Reversal (truncated). 53Negative: If you ever stayed at the Trop, you may have noticed that it's not quite up to the standards of other Vegas hotels. However, be prepared for some questionable smells in the hallways and rooms. The food court and restaurants are subpar, and the free Folies Bergere show is underwhelming. The rooms have a retirement community feel to them. Overall, it's not the best option, but it may work in a pinch. Very negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. I highly recommend avoiding the Trop like the plague. Why is this review not Very negative? Feedback: The rewritten review is \"Very negative\". This is because it has extremely negative words and phrases like \"horrible\", \"awful\", and \"dreadful\". The output is extremely vile and negative. This review is already as negative as it can get. Excellent work! But there is always room for more. Make it even more negative. Try again! Okay, let's try again. Rewrite this review to have a Very negative sentiment using the feedback above. Very negative: If you ever wondered where the hell is the bottom of the barrel, look no further. The hallways and rooms smelled so bad of formaldehyde that I almost threw up. Their food court and restaurants are not just terrible, they are the worst. The free Folies Bergere show is lame and disgusting. The rooms look like a retirement community from florida. I highly recommend avoiding the Trop like the plague. You will regret it if you don't. Figure 35: REFINE prompt for Sentiment Reversal (truncated). 54",
      "references": [
        "A Theoretical Framework.",
        "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
        "Constitutional ai: Harmlessness from ai feedback.",
        "Triangulating Python Performance Issues with SCALENE.",
        "Interval estimation for a binomial proportion.",
        "Language models are few-shot learners.",
        "Evaluating Large Language Models Trained on Code.",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
        "Training verifiers to solve math word problems.",
        "Teaching a black-box learner.",
        "Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision.",
        "NL-EDIT: Correcting semantic parse errors through natural language interaction.",
        "A cognitive process theory of writing.",
        "Gptscore: Evaluate as you desire.",
        "Pal: Program-aided language models.",
        "Koala: A dialogue model for academic research.",
        "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learn- ing.",
        "Delete, retrieve, generate: a simple approach to sentiment and style transfer.",
        "CommonGen: A constrained text generation challenge for generative commonsense reasoning.",
        "Rainier: Reinforced knowledge introspector for commonsense question answering.",
        "Quark: Controllable text generation with reinforced unlearning.",
        "Learning performance-improving code edits.",
        "Think about it! improving defeasible reasoning by first modeling the question scenario.",
        "Unsupervised evaluation of interactive dialog with DialoGPT.",
        "Codegen: An open large language model for code with multi-turn program synthesis.",
        "Model index for researchers.",
        "Gpt-4 technical report.",
        "Training language models to follow instructions with human feedback.",
        "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback.",
        "Style transfer through back-translation.",
        "Measur- ing and narrowing the compositionality gap in language models.",
        "Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks.",
        "Learning to model editing processes.",
        "Self-critiquing models for assisting human evaluators.",
        "Training language models with natural language feedback.",
        "Peer: A collaborative language model.",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
        "The architecture of complexity.",
        "Learning to summarize with human feedback.",
        "Principle-driven self-alignment of language models from scratch with minimal human supervision.",
        "Interscript: A dataset for interactive learning of scripts through error feedback.",
        "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback.",
        "Llama: Open and efficient foundation language models.",
        "Chain of Thought Prompting Elicits Reasoning in Large Language Models.",
        "Generating sequences by learning to self-correct.",
        "Re3: Generating longer stories with recursive reprompting and revision.",
        "Graph-based, self-supervised program repair from diagnostic feedback.",
        "Character-level convolutional networks for text classification."
      ],
      "meta_data": {
        "arxiv_id": "2303.17651v2",
        "authors": [
          "Aman Madaan",
          "Niket Tandon",
          "Prakhar Gupta",
          "Skyler Hallinan",
          "Luyu Gao",
          "Sarah Wiegreffe",
          "Uri Alon",
          "Nouha Dziri",
          "Shrimai Prabhumoye",
          "Yiming Yang",
          "Shashank Gupta",
          "Bodhisattwa Prasad Majumder",
          "Katherine Hermann",
          "Sean Welleck",
          "Amir Yazdanbakhsh",
          "Peter Clark"
        ],
        "published_date": "2023-03-30T18:30:01Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces SELF-REFINE, a test-time, training-free algorithm that improves large language model (LLM) outputs via iterative self-generated natural-language feedback and subsequent refinements using the same underlying model. Targets the problem that single-pass LLM generations often miss nuanced, multifaceted requirements (e.g., dialogue quality, code readability, constraint satisfaction). Key finding: across 7 diverse tasks and strong base models (GPT-3.5/ChatGPT/GPT-4; plus Codex for code), SELF-REFINE yields consistently better outputs than direct generation, with ~20% absolute average task improvement and large preference gains on open-ended tasks (up to ~49% absolute on dialogue for GPT-4). Analysis shows actionable, specific feedback and multiple iterations are critical; improvements are not merely due to producing more samples.",
        "methodology": "Algorithm: (1) INIT: generate draft y0 using prompt pgen. (2) FEEDBACK: feed x and current draft yt to the same model M with a feedback prompt pfb to produce actionable, specific feedback fbt (often multi-aspect with rubric scores and optionally a stop indicator). (3) REFINE: feed x, current draft yt (and optionally full history y0,fb0,...,yt,fbt) to M with prompt prefine to produce refined draft yt+1. Repeat FEEDBACK/REFINE up to a max of 4 iterations or until stop condition triggers; optionally select best iteration using feedback-provided scalar/aggregate scores (important for multi-aspect tasks where quality can be non-monotonic). Implementation relies on few-shot prompting for all modules (even for instruction-tuned models) and uses the same base LLM as generator/critic/refiner; no supervised data collection, parameter updates, or RLHF needed at run time. Ablations: generic feedback and no-feedback iterative refinement reduce gains, highlighting the role of explicit, targeted feedback.",
        "experimental_setup": "Tasks (7): Dialogue Response Generation (FED; Mehri & Eskenazi 2020), Code Optimization (PIE; Madaan et al. 2023), Code Readability Improvement (CodeNet subset; Puri et al. 2021), Math Reasoning (GSM8K; Cobbe et al. 2021; often in PaL-style Python solutions), Sentiment Reversal (Zhang et al. 2015 reviews), plus two introduced tasks: Acronym Generation (250-title dataset mined/pruned from Wikipedia acronym list) and Constrained Generation 'CommonGen-Hard' (variant of CommonGen; Lin et al. 2020) with 20–30 concept constraints.\nModels: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), GPT-4; Codex (code-davinci-002) for code tasks; exploratory tests with Vicuna-13B.\nDecoding/iterations: greedy decoding with temperature 0.7 (as stated); max 4 FEEDBACK/REFINE iterations (some readability experiments used 5 iterations).\nEvaluation metrics: task-specific automatic metrics where available—GSM8K solve rate, PIE % optimized, constrained generation coverage; preference-based evaluation (blind human A/B by authors on subsets) for dialogue, readability, sentiment reversal, acronym generation. Additional automatic preference via GPT-4-as-judge with reported correlations to humans (e.g., 82% sentiment, 68% acronym, 71% dialogue). Confidence intervals reported via Wilson intervals; significance marked for many gains. Additional analyses: iteration-wise improvements; comparison to generating k=4 independent samples (MULTI) showing refinement beats sampling; oracle-correctness gating for math increases gains.",
        "limitations": "Requires sufficiently capable base LLMs with strong instruction-following/few-shot abilities; weaker/smaller models (e.g., Vicuna-13B) often fail to produce correctly formatted feedback or to apply feedback during refinement. Dependent on proprietary, closed-source models (GPT-3.5/4, Codex) with undisclosed training details and nontrivial cost, limiting transparency and accessibility (though code/outputs released). Evaluated only in English; generalization to other languages not established. Self-feedback can be inaccurate—failure analysis suggests most errors stem from incorrect feedback (mislocalizing issues or proposing wrong fixes) rather than inability to implement good feedback; especially challenging for math where models may falsely conclude “everything looks good,” yielding small gains without external error signals. Safety: approach does not add explicit safeguards; could be misused to iteratively steer towards harmful/toxic content. Additional compute/latency due to multiple model calls per instance.",
        "future_research_directions": "(1) Improve feedback reliability: integrate verifiers/compilers/unit tests/reward models or tool-based checks to detect errors (notably for math and code) and to gate refinement (oracle-style stopping/triggering). (2) Robustness to imperfect feedback: study and enhance refiner behavior under noisy/partially incorrect critiques; develop methods for feedback calibration, uncertainty estimates, and contradiction detection across iterations. (3) Better stopping and selection: principled, task-agnostic stop criteria; learning-to-select best intermediate outputs; multi-objective scoring aggregation for non-monotonic tasks. (4) Extend to open-source/smaller models: prompt-engineering or lightweight fine-tuning to enable consistent feedback/refinement formatting; hybrid “mixed-refine” pipelines using small generators with larger critics. (5) Safety alignment: incorporate constitutional/safety critiques in feedback, toxicity filters, and guardrails to prevent harmful iterative escalation. (6) Broader modalities and applications: apply iterative self-refinement to agentic planning, tool use, long-form writing, UI/web generation, multilingual settings, and domain-specific workflows; analyze compute-performance tradeoffs and scaling behavior with longer iteration histories.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Extrapolative Controlled Sequence Generation via Iterative Refinement",
      "full_text": "Elastic Analysis of Augmented Curves and Constrained Surfaces Esfandiar Nava-Yazdani[0000−0003−4895−739X] Zuse Institute Berlin, Berlin, Germany navayazdani@zib.de https://www.zib.de/members/navayazdani Abstract. The square root velocity transformation is crucial for efficiently em- ploying the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks. Keywords: Elastic shape analysis · Tube · Manifold-valued · Ruled surface · Hurricane track. 1 Introduction Metric comparison of curves is a core task in a wide range of application areas such as morphology, image and shape analysis, computer vision, action recognition and signal processing. Thereby, a Riemannian structure is highly desirable, since it naturally provides powerful tools, beneficial for such applications. In the recent years, the use of Riemannian metrics for the study of sequential data, such as shapes of curves, trajectories given as longitudinal data or time series, has rapidly grown. In elastic analysis of curves, one considers deformations caused from both bending and stretching. A Riemannian metric, which quantifies the amount of those deformations is called elastic (cf. [18,19]). Therein, in contrast to landmark-based approaches (cf. [12,20,22]), one considers whole continuous curves instead of finite num- ber of curve-points. Consequently, the underlying spaces are infinite dimensional and computational cost becomes a significant issue. The square root velocity (SRV) frame- work provides a convenient and numerically efficient approach for analysing curves via elastic metrics and has been widely used in the recent years (cf. [14,11,4,3] and the comprehensive work [24]). In many applications the curves are naturally manifold-valued. For instance, Lie groups such as the Euclidean motion group, or more generally, symmetric spaces includ- ing the Grassmannian and the Hadamard-Cartan manifold of positive definite matrices are widely used in modelling of real world applications. Extensions of SRV framework from euclidean to general manifold-valued data can be found in [13,27,25,9,26]. arXiv:2402.04944v3  [math.DG]  28 Mar 20242 E. Nava-Yazdani Our contributions are the following. We expose for plane curves the behaviour of speed and curvature under the SRV transformation and geometric invariants. More- over, we apply the elastic approach to augmented curves, determining certain classes of surfaces, tubes, ruled surfaces and spherical strips, as well as hurricane tracks consid- ered with their intensities. We recall that with distance and geodesic at hand, signifi- cant ingredients of statistical analysis such as mean and principal geodesic components as well as approximation and modelling concepts such as splines can be computed. This paper is organized as follows. Section 2, presents the Riemannian setting and notations. Section 3 is devoted to applications. Therein, we consider time series, for which in addition to spatial data, auxiliary information give rise to augmented curves and some classes of surfaces generated by them. Thereby, we apply the elastic approach to both euclidean and spherical trajectories. Future prospects and concluding remarks are presented in 4. For the convenience of those readers primary interested in the applications, we mention that, advanced parts and details from differential geometry, presented in 2, can be skipped. Thereby, the essential point is the use of a framework (SRV) for computation of shortest paths on the spaces of curves and their shapes. 2 Riemannian Framework 2.1 Preliminaries For the background material on Riemannian geometry, we refer to [8] and [10]. Let (M, g) be a finite dimensional Riemannian manifold and M the Fr´ echet manifold of smooth immersed curves from D in M, where D denotes either the unit circle S1 or the unit interval I := [0, 1] for closed or open curves respectively. Moreover, we denote the group of orientation preserving diffeomorphisms on D by Diff +. The following reparametrization invariance is crucial for a Riemannian metric G on M: Gc◦φ(h ◦ φ, k◦ φ) = Gc(h, k), for any c ∈ M, h, k∈ TcM and φ ∈ Diff +. The above equivariance ensures that the induced distance function satisfies the following, which is often desirable in applica- tions: d(c0 ◦ φ, c1 ◦ φ) = d(c0, c1), for any two curves c0 and c1 in M. Similarly, denoting the isometry group of M by Isom(M) and the tangent map of F ∈ Isom(M) by T F, the invariance GF◦c(T F◦ h, TF◦ k) = Gc(h, k), ensures that d(F ◦ c0, F◦ c1) = d(c0, c1). With the above invariances, we can divide out the spaces Isom(M) and Diff +, and consider the natural induced distance dS on the quotient space S = M/(Diff + × Isom(M))Elastic Analysis of Augmented Curves and Constrained Surfaces 3 given by dS([c0], [c1]) = inf {d(c0, f◦ c1 ◦ φ) : φ ∈ Diff +, f∈ Isom(M)} = inf {d(f ◦ c0 ◦ φ, c1) : φ ∈ Diff +, f∈ Isom(M)}. In the context of shape analysis of curves, M and S are called the pre-shape and shape space, respectively. Note that the order of quotient operations does not matter, since the left action of Isom(M) and the right action of Diff + commute. M/Diff + is the space of unparametrized curves and its inherited distance reads inf {d(c0, c1 ◦ φ) : φ ∈ Diff +}. We remark that particular essential challenges are due to the fact that some basic concepts and results from finite dimensional differential geometry such as Hopf-Rinow theorem, do not carry over to the infinite dimensional case. Now, let ∇ be the Levi- Civita connection of M and denote the arc length parameter, speed and unit tangent of c by θ, ω and T respectively. Thus, we have ω = |˙c|, dθ = ωdt and T = ˙c ω , where dot stands for derivation with respect to the parameter t. Due to a remarkable result in [16] the geodesic distance induced by the simplest natural choice, the L2-metric GL2 c (h, k) = Z D gc(h, k) dθ, always vanishes. Consequently, some stronger Sobolev metrics have been considered in several works including [17,7,5]. They are given by Gc(h, k) = nX i=0 Z D aigc(∇i T h, ∇i T k) dθ, with a1 non-vanishing and all ai non-negative, distinguish the curves. We consider first order metrics with constant coefficients. We remark that the coefficients ai can be chosen such that the metric is scale invariant, which is a desired property for some applications in shape analysis. A family of certain weighted Sobolev-type metrics, the so-called elastic metrics, based on a decomposition of derivatives of the vector fields into normal and tangent components, has been introduced in [18,19]: Ga,b c (h, k) = Z D agc((∇T h)⊤, (∇T k)⊤) + bgc((∇T h)⊥, (∇T k)⊥) dθ, with 4b ≥ a >0. In this work, we use the square root velocity (SRV) framework, which allows for a convenient and computationally efficient elastic approach. The main tool in this framework is the square root velocity transformation, which for euclidean M reads q : c 7→ ˙cp |˙c| .4 E. Nava-Yazdani It isometrically maps curves modulo translations, with the metric G1,1/4 to M with the flat L2-metric given by G0(v, w) = Z D g(v(t), w(t))dt. This metric is frequently called (cf. [15,2,6]) flat, to emphasize its footpoint indepen- dence. Note that the elastic metric G1,1 corresponds to the first order Sobolev metric with a0 = 0 and a1 = 1. We remark, that for plane curves, the work [23] has extended the SRV transformation to general parameters a, b >0. For further reading on the SRV framework and applications in shape analysis, we refer to [14], [11] (numerical aspects), the survey [6] and particularly, the comprehensive work [24]. 2.2 Plane Curves A natural question that arises is, how essential geometric characteristics of a curve behave under the SRV transformation. In the following, we provide an answer for speed and curvature in the case of plane curves. Let M = R2, ˜c := q(c) and denote the curvature of c by κ. Note that ˜c does not need to be an immersion. Proposition 1. Denoting the speed of˜c by ˜ω, we have ˜ω = r ˙ω2 4ω + ω3κ2. (1) Moreover, ˜c is an immersion if and only ifκ and ˙ω have no common zeros. In this case, ˜κ˜ω = κω + ˙φ, (2) where ˜κ denotes the curvature of˜c and φ := arctan \u00122ω2κ ˙ω \u0013 . Proof. Let N denote the unit normal of c. With the shorthand notations α := √ω and β := α3κ, a straightforward application of the Frenet equations ˙T = ωκN and ˙N = −ωκT , yields ˙˜c = ˙αT + βN, ¨˜c = (¨α − β2 α )T + ( ˙β + ˙αβ α )N. Thus, we have ˜ω = p ˙α2 + β2, immediately implying (1). Obviously, zeros of ˜ω are common zeros of κ and ˙ω. Thus, ˜c is an immersion if and only if κ and ˙ω have no common zeros. In this case, ˜ κ and φ = arctan (β/ ˙α) = arctan \u0010 2ω2κ ˙ω \u0011 are well-defined and ˜κ˜ω3 = ˜ω2β/α + ˙α ˙β − ¨αβ, which immediately implies the curvature formula (2).Elastic Analysis of Augmented Curves and Constrained Surfaces 5 Next, we apply the proposition to study some geometric quantities, which are invariant under the SRV transformation. For closed curves, integrating the curvature formula above over D = S1 (note that in this case, ˜ω >0 almost everywhere), we see that the SRV transformation preserves the total curvature and particularly the turning number. Moreover, κω is preserved if and only if κ = a d dt \u00001 ω \u0001 with a constant a. Clearly, with κ and ω at hand, utilizing Frenet equations, we can compute c up to rigid motions. The following explicit solution is an immediate application of the above proposition. In light of the above proposition, immersed curves, which are mapped to straight lines, can easily be determined as follows. Example 1. Let a, b, Abe constants withab, A >0, ω(t) = A/ sin2(at+b) and κ = a/ω. A straightforward computation, utilizing the curvature formula (2), implies ˜κ = 0. 2.3 Curves in Homogeneous Spaces For the background material on Lie groups and homogeneous spaces, we refer to [10]. The works [13,27] provide extensions of the SRV framework for euclidean curves to the case of general manifolds. The former has high computational cost, while the latter, transported SRV, depends on a reference point and also suffers from distortion or bias caused by holonomy effects. We use the natural extension to homogeneous spaces exposed in [26,9]. For reader’s convenience, we sketch the core ingredients of the approach and refer to the mentioned works for details and some applications. Let M be a homogeneous space, i.e., M = H/K, where K is a closed Lie subgroup of a Lie Group H. Let ∥·∥ denote the induced norm by a left invariant metric on H, L the tangent map of the left translation, and Imm(D, H) the space of immersed curves from D to H. The SRV transformation is given by Q(α) = (α(0), q(α)), where q(α) = Lα−1 ˙αp ∥ ˙α∥ Here, α−1(t) denotes the inverse element of α(t) in H and H the Lie algebra of H. The map Q is a bijection from Imm(D, H) onto H × L2(D, H). Now, M can be equipped with the Riemannian metric given by the pullback of the product metric of H × L2(D, H) using the map Q and horizontal lifting. Let c1 and c2 be immersed curves in M with horizontal lifts α1 and α2 respectively. The induced distance on M reads d(c1, c2) = inf \u001aq d2 H(α1(0), α2(0)x) + ∥q(α1) − Adx−1 (q(α2)∥2 L2 : x ∈ K \u001b . 3 Applications Frequently, besides spatiotemporal data, represented by a curve γ in a manifold M, there are additional or auxiliary information associated with the curve, thus with the same time-correspondence. These can jointly with γ be comprised and represented as a6 E. Nava-Yazdani so-called augmented curve ˜γ in a higher dimensional manifold ˜M. In some applications, the curve ˜γ uniquely determines a submanifold N of M via a natural construction. An important example is provided, when ˜M is a submanifold of the tangent bundle of M, where the auxiliary information is represented as a vector field along γ and the con- struction is given by the Riemannian exponential map. Significant special cases occur, when M is R3 or the unit two-sphere S2 and N a surface. In the next two subsections, we consider certain classes of surfaces in R3, which often arise in applications and are determined by augmented curves in R4. In the last two subsections, we consider certain spherical regions as well as hurricane tracks together with their intensities. In both cases, we utilize the Riemannian distance from subsection 2.3 to S2 × R, which is a homogeneous space (recall that S2 can be identified with SO(3)/SO(2)). For our example applications, we present geodesic paths representing deformations, minimizing the elastic energy within the SRV framework. We remark, that in a Rie- mannian setting, distance and geodesics are essential Building blocks for many major issues in the morphology and shape analysis, such as computation of mean and test statistics as well as principal component or geodesic analysis. Moreover, besides sta- tistical analysis, also some methods for clustering and classification use Riemannian metrics and geodesics. For the code implementing our approach, which particularly includes Riemannian optimization for the computation of geodesic paths, we utilized our publicly available python package https://github.com/morphomatics, introduced in [1]. 3.1 Tubes A tube or canal surface c is a one-parameter family of circles, whose centers constitute a regular curve γ such that the circles are perpendicular to γ. More precisely, denoting the radii of the circles by r, c(s, .) = γ + r(N cos s + B sin s), 0 ≤ s ≤ 2π, where N and B are the normal and binormal of the curve γ = γ(t), t∈ D, resp. Due to the unique correspondence of c to (γ, r), comparison of tubes reduces to comparison of curves in R4. Figure 1 shows some examples of shortest paths of tubes. Real world applications include a variety of fields such as examination of vein, pipes, capsules and plant roots. Clearly, tubes include surfaces of revolution. 3.2 Ruled Surfaces A ruled surface is formed by moving a straight line segment (possibly with varying length) along a base curve. More precisely, let γ be a curve in R3 and v a unit vector field along γ. Then c(s, .) = γ + sv, s∈ I, parametrizes a ruled surface generated by ( γ, v). Figure 2 depicts an example, where each surface consists of straight line segments connecting the blue (for better visibility) curves γ and γ + v. The class of ruled surfaces includes many prominent surfacesElastic Analysis of Augmented Curves and Constrained Surfaces 7 Fig. 1.Two shortest paths of tubes such as cone, cylinder, helicoid (a minimal surface) and M¨ obius strip. They arise in manufacturing (construction by bending a flat sheet), cartography, architecture and biochemistry (secondary and tertiary structure of protein molecules). Fig. 2.Shortest path of ruled surfaces 3.3 Spherical Strips Let exp denote the exponential map of the unit two-sphere S2. We recall that for any non-zero tangent vector to S2 at a point x: expx(v) = cos(|v|)x + sin(|v|) v |v|8 E. Nava-Yazdani and expx(0) = x. Now, let γ be a curve in S2 with binormal B (cross product of γ and its unit tangent), and r a scalar function along γ. Then, the map c given by c(s, .) := expγ s(rB), s∈ I, parametrizes a spherical strip with bandwidth r. Figure 3 depicts an example of the shortest path between two spherical curves comprised with their bandwidth functions visualised as strips. Fig. 3.Shortest path of spherical strips 3.4 Hurricane Tracks Hurricanes belong to the most extreme natural phenomena and can cause major im- pacts regarding environment, economy, etc. Intensity of a hurricane is determined by the maximum sustained wind (maxwind), monotonically classifying the storms into categories (due to Saffir–Simpson wind scale; for instance, maxwind ≥ 137 knots cor- responds to category 5). Due to their major impacts on economy, human life and envi- ronment, as well as extreme variability and complexity, hurricanes have been studies in a large number of works. For our example, we used the HURDAT 2 database provided by the U.S. National Oceanic and Atmospheric Administration publicly available on https://www.nhc.noaa.gov/data/, supplying latitude, longitude, and maxwind on a 6 hours base of Atlantic hurricanes. Fig. 4.2010 Atlantic hurricane tracks (left) and the shortest path between two of them (right) with color-coded maximum sustained wind (in knots)Elastic Analysis of Augmented Curves and Constrained Surfaces 9 We represent the tracks as discrete trajectories in S2. For further details and com- parison with other approaches, we refer to [24,25] and the recent work [21]. The latter, also provides statistical analysis and a classification of hurricane tracks in terms of their intensities. Fig. 4 illustrates this data set with a visualization of the 2010 hurricane tracks and a shortest path, where the intensities, considered as auxiliary information, are color-marked. 4 Conclusion In this paper, we analysed the behaviour of speed and curvature under the square root velocity framework for elastic approach to plane curves. Moreover, we applied an extension of this framework to homogeneous Spaces, to metrically compare augmented curves and special surfaces, generated by those curves, using a natural construction via the Riemannian exponential map. Our approach, allows for computationally efficient determination of geodesic paths in the shape spaces of the respective classes of surfaces. Our example applications include tubes, ruled surfaces, spherical strips and hurricane tracks. Future work includes further real world applications, particularly concerning statistical analysis of longitudinal data such as comparison of group wise trends within a hierarchical model as well as classification and prediction. Acknowledgements This work was supported through the German Research Foun- dation (DFG) via individual funding (project ID 499571814). References 1. Ambellan, F., Hanik, M., von Tycowicz, C.: Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces (2021). https://doi.org/10.12752/8544, https:// morphomatics.github.io/ 2. Bauer, M., Bruveris, M., Marsland, S., Michor, P.: Constructing reparametrization in- variant metrics on spaces of plane curves. arXiv: Differential Geometry (2012), https: //arxiv.org/pdf/1207.5965.pdf 3. Bauer, M., Bruveris, M., Charon, N., Møller-Andersen, J.: A relaxed approach for curve matching with elastic metrics. ESAIM: Control, Optimisation and Calculus of Variations 25 (03 2018). https://doi.org/10.1051/cocv/2018053 4. Bauer, M., Bruveris, M., Harms, Philipp Michor, P.W.: Soliton solutions for the elastic metric on spaces of curves. Discrete & Continuous Dynamical Systems - A 38, 1161–1185 (2018). https://doi.org/10.3934/dcds.2018049 5. Bauer, M., Bruveris, M., Michor, P.W.: Overview of the geometries of shape spaces and diffeomorphism groups. Journal of Mathematical Imaging and Vision 50(1-2), 60–97 (2014) 6. Bauer, M., Charon, N., Klassen, E., Brigant, A.L.: Intrinsic riemannian metrics on spaces of curves: theory and computation. arXiv preprint (2020), https://arxiv.org/abs/ 2003.05590 7. Bauer, M., Harms, P., Michor, P.W., et al.: Sobolev metrics on the manifold of all rie- mannian metrics. Journal of Differential Geometry 94(2), 187–208 (2013)10 E. Nava-Yazdani 8. do Carmo, M.P.: Riemannian Geometry. Mathematics: Theory and Applications, Birkh¨ auser Boston, Cambridge, MA, USA, 2 edn. (1992) 9. Celledoni, E., Eidnes, S., Schmeding, A.: Shape analysis on homogeneous spaces: a gener- alised srvt framework. In: Computation and Combinatorics in Dynamics, Stochastics and Control: The Abel Symposium, Rosendal, Norway, August 2016. pp. 187–220. Springer (2018) 10. Gallot, S., Hullin, D., Lafontaine, J.: Riemannian Geometry. Universitext, Springer, Berlin, 3 edn. (2004) 11. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.A.: Riemannian optimization for registration of curves in elastic shape analysis. Journal of Mathematical Imaging and Vision 54(3), 320–343 (2016) 12. Kendall, D., Barden, D. Carne, T., Le, H.: Shape and Shape Theory. John Wiley & Sons (1999) 13. Le Brigant, A.: Computing distances and geodesics between manifold-valued curves in the srv framework. Journal of Geometric Mechanics 9(2) (2017) 14. Liu, W., Srivastava, A., Zhang, J.: Protein structure alignment using elastic shape anal- ysis. In: Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology. pp. 62–70 (2010) 15. Michor, P., Mumford, D., Shah, J., Younes, L.: A metric on shape space with explicit geodesics. Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 19 (07 2007). https://doi.org/10.4171/RLM/506 16. Michor, P.W., Mumford, D.: Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms. Documenta Mathematica 10, 217–245 (2005) 17. Michor, P.W., Mumford, D.: An overview of the riemannian metrics on spaces of curves using the hamiltonian approach. Applied and Computational Harmonic Analysis 23(1), 74–113 (2007) 18. Mio, W., Srivastava, A., Joshi, S.H.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (2006) 19. Mio, W., Srivastava, A., Joshi, S.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (07 2007). https://doi.org/10.1007/s11263-006-9968-0 20. Nava-XYazdani, E., Hege, H.C., Sullivan, T.J., von Tycowicz, C.: Geodesic analysis in kendall’s shape space with epidemiological applications. Journal of Mathematical Imaging and Vision pp. 1–11 (2020). https://doi.org/10.1007/s10851-020-00945-w 21. Nava-Yazdani, E., Ambellan, F., Hanik, M., von Tycowicz, C.: Sasaki metric for spline models of manifold-valued trajectories. Computer Aided Geometric Design 104, 102220 (2023). https://doi.org/10.1016/j.cagd.2023.102220 22. Nava-Yazdani, E., Hege, H.C., von Tycowicz, C.: A hierarchical geodesic model for longi- tudinal analysis on manif olds. Journal of Mathematical Imaging and Vision 64(4), 395 – 407 (2022). https://doi.org/10.1007/s10851-022-01079-x 23. Needham, T., Kurtek, S.: Simplifying transforms for general elastic metrics on the space of plane curves. SIAM Journal on Imaging Sciences 13(1), 445–473 (2020). https://doi.org/10.1137/19M1265132 24. Srivastava, A., Klassen, E.P.: Functional and shape data analysis, vol. 1. Springer (2016) 25. Su, Z., Klassen, E., Bauer, M.: The square root velocity framework for curves in a ho- mogeneous space. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 680–689 (2017) 26. Su, Z., Klassen, E., Bauer, M.: Comparing curves in homogeneous spaces. Differential Geometry and its Applications 60, 9–32 (2018) 27. Zhang, Z., Su, J., Klassen, E., Le, H., Srivastava, A.: Rate-invariant analysis of covariance trajectories. Journal of Mathematical Imaging and Vision 60, 1306–1323 (2018)",
      "references": [
        "Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces",
        "Constructing reparametrization in- variant metrics on spaces of plane curves.",
        "A relaxed approach for curve matching with elastic metrics.",
        "Soliton solutions for the elastic metric on spaces of curves.",
        "Overview of the geometries of shape spaces and diffeomorphism groups.",
        "Intrinsic riemannian metrics on spaces of curves: theory and computation.",
        "Sobolev metrics on the manifold of all rie- mannian metrics.",
        "Riemannian Geometry.",
        "Shape analysis on homogeneous spaces: a gener- alised srvt framework.",
        "Riemannian optimization for registration of curves in elastic shape analysis.",
        "Shape and Shape Theory.",
        "Computing distances and geodesics between manifold-valued curves in the srv framework.",
        "Protein structure alignment using elastic shape anal- ysis.",
        "A metric on shape space with explicit geodesics.",
        "Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms.",
        "An overview of the riemannian metrics on spaces of curves using the hamiltonian approach.",
        "On shape of plane elastic curves.",
        "Geodesic analysis in kendall’s shape space with epidemiological applications.",
        "Sasaki metric for spline models of manifold-valued trajectories.",
        "A hierarchical geodesic model for longi- tudinal analysis on manif olds.",
        "Simplifying transforms for general elastic metrics on the space of plane curves.",
        "Functional and shape data analysis",
        "The square root velocity framework for curves in a ho- mogeneous space.",
        "Comparing curves in homogeneous spaces.",
        "Rate-invariant analysis of covariance trajectories."
      ],
      "meta_data": {
        "arxiv_id": "2402.04944v3",
        "authors": [
          "Esfandiar Nava-Yazdani"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies geometric effects of the square root velocity (SRV) transform on plane curves—deriving explicit formulas for how speed and curvature change and identifying SRV-invariant quantities (e.g., total curvature/turning number for closed curves). Demonstrates how SRV-based elastic metrics enable intrinsic comparison (geodesics/distances) of augmented curves that parameterize constrained surface classes and manifold-valued trajectories, with applications to tubes (canal surfaces), ruled surfaces, spherical strips, and hurricane tracks augmented with intensity.",
        "methodology": "Uses elastic Riemannian metrics on spaces of immersed curves with reparameterization and isometry invariance; employs the SRV transform as an isometry (Euclidean case) to the flat L2 metric for efficient geodesic computation. For plane curves, applies Frenet-frame calculus to derive SRV-transformed speed/curvature relations and invariants. For manifold-valued data, adopts the SRV framework on homogeneous spaces via horizontal lifts in a Lie group H with quotient M=H/K; defines Q(α)=(α(0),q(α)) with left-translation of velocities into the Lie algebra and computes distances by optimizing over K using Adjoint action. Models surfaces through augmented curves (e.g., (γ,r) in R4 for tubes; (γ,v) for ruled surfaces; (γ,r) with binormal and exp map for spherical strips; (track,intensity) in S2×R for hurricanes) and computes shortest paths via Riemannian optimization (implemented with Morphomatics).",
        "experimental_setup": "Primarily methodological with illustrative computational examples rather than quantitative benchmarks. Data/examples include: (1) synthetic/constructed tube surfaces represented by centerline γ(t) in R3 and radius r(t) forming curves in R4; (2) synthetic/constructed ruled surfaces from γ(t) in R3 and unit direction field v(t); (3) spherical strips on S2 using γ(t) and bandwidth r(t) via the spherical exponential map; (4) real-world hurricane tracks from NOAA HURDAT2 (Atlantic), sampled every 6 hours with latitude/longitude mapped to S2 and max sustained wind (knots) as auxiliary scalar (forming trajectories in S2×R). Validation is qualitative: visualization of computed geodesic (shortest) deformation paths between examples; optimization performed with the Morphomatics Python package.",
        "limitations": "Does not provide extensive quantitative evaluation, accuracy/runtime comparisons, or large-scale statistical studies; results are mainly theoretical derivations (plane curves) plus qualitative demonstrations. SRV theory assumes smooth immersed curves; SRV images may fail to be immersions when curvature and speed-derivative share zeros, and practical data are discretely sampled (e.g., hurricane tracks). Homogeneous-space SRV relies on availability of Lie group structure, left-invariant metrics, and horizontal lifting; distance requires optimization over isotropy subgroup K, which can be nontrivial and may have local minima numerically. Surface classes considered are constrained (tubes, ruled, strips) and do not cover general surfaces; augmentation choices (e.g., radius/bandwidth, intensity scaling) introduce modeling assumptions.",
        "future_research_directions": "Extend to broader real-world applications and full statistical pipelines on the resulting shape spaces: Fréchet means, hypothesis testing, principal geodesic analysis, and regression on longitudinal/manifold-valued data. Develop hierarchical/groupwise trend models, classification and prediction for augmented trajectories (explicitly mentioned for longitudinal trends, classification, prediction). Investigate robustness to discretization/noise and missing data; improved numerical optimization over reparameterizations and subgroup K; explore scale-invariant/other elastic metric parameterizations and their impact. Generalize beyond the studied constrained surface families to richer surface representations and additional manifold-valued auxiliary fields (e.g., tangent-bundle data with Sasaki-type metrics).",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Full-Atom Protein Pocket Design via Iterative Refinement",
      "full_text": "Vol 2  Issue 2  Pg. no. 1    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    Research  Article    Open Access &   Peer -Reviewed Article   DOI:10.14302/issn.2643-2811.jmbr-23- 4556    Corresponding author:        Raul Isea,Fundación IDEA, Hoyo de la  Puerta,Baruta, Venezuela.    Received: April 10, 2023  Accepted: May 10, 2023               Published: July 07, 2023     Keywords: Covid-19, Origin; Hosts,       Reservoir, Immune, Transmission            model, SARS-CoV-2 .                                              AcademicEditor:    Anubha Bajaj   Citation:                                                Raul Isea(2023). A General Approach to  Modeling Covid-19. Journal of Model  Based Research – 2(2):1-19..https:// doi.org/10.14302/issn.2643-2811.jmbr-23- 4556  Raul Isea 1,*  Abstract  The present work shows that it is possible to analytically solve a general model to  explain the transmission dynamics of SARS -CoV-2. First, the within -host model is  described, and later a between -host model, where the coupling between them is the  viral load of SARS -CoV-2. The within -host model describes the equations involved  in the life cycle of SARS -CoV-2, and also the immune response; while that the       between-Host model analyzes the dynamics of virus spread from the original source  of contagion associated with bats, subsequently transmitted to a host, and then      reaching the reservoir (Huanan Seafood Wholesale Market in Wuhan ), until finally  infecting the human population.  Introduction  The World Health Organization (WHO) reported 27 cases with a new severe          respiratory syndrome of unknown etiology from Wuhan (Hubei province) in China  [1]. Later identifies that it is a β-Coronavirus after sequencing the first days of            January 2020 [2]. They originally called it 2019 -nCOV, and after a month, it was      renamed Severe Acute Respiratory Syndrome Coronavirus 2 (SARS -CoV-2), where  the disease it produces is Covid -19. The WHO declared it an international public  health emergency on January 30, 2020, and it was later declared a pandemic on  March 11, 2020 [3].  So far the world has faced three coronavirus associated outbreaks. .The first was  called SARS-CoV-1 [4], and it originated in the province of Guangdong Province in  China: eight thousand cases were confirmed with a little less than eight hundred  deaths in 2002 (affecting 29 countries until January 2004).The second incident was  traced in Saudi Arabia, and for this reason, it was named MERS -CoV (Middle East  Respiratory Syndrome Coronavirus), its first case was reported in June 2012 until  November 2018, registered 2,494 cases after affecting 27 countries [5]. SARS -CoV-2  has infected more than 757 million people and 6 million deaths  approximately until  February 23, 2023, according to Johns Hopkins University (data available at               coronavirus.jhu.edu).  It is currently a public health problem. Multiple vaccines have been developed to   control the spread of the disease and to reduce the number of infections recorded     daily in various parts of the world. However, it is necessary to do more studies, for  example, on how to reduce the application of multiple doses regimens for a person,  and so on. It can probably do identification of genomic patterns capable of generating  an immune response in people derived from epitopes, as has been tested in other     1Fundación IDEA, Hoyo de la Puerta,Baruta, Venezuela.  A General Approach to Modeling Covid-19                             Vol 2  Issue 2  Pg. no. 2    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    diseases [6,7,8], and later can be automated with the help of specialized workflows for automatic data  handling [9].  Until this is achieved, it is necessary to develop mathematical models to design the public policies          necessary to contain said infections based mainly on the implementation of social distancing measures,  the use of face masks, quarantine programs and vaccination campaigns, etc.  In the scientific literature, there is a wide range of mathematical models that explain, for example, the  contagion dynamics in various countries without a consensus on the prediction methodology (some ex- amples in [10, 11]). Other works explain the transmission of viral particles in the environment [12], and  also studies have also been carried out where they analyze the response of the immune system [13, 14].  However, it is necessary to integrate all these approaches on the same time scale. This process is known  as immuno-epidemiological models of infectious disease systems [15] where they are described from a  cellular level to the spread in the population. This type of model has been used in the dynamics of conta- gion in HIV -1 with a system of six differential equations [16, 17], as well as the paper of Murillo et al  [18] by studying a multiscale model to explain the Influenza infection in 2013.   The present work presents a general model to explain the transmission dynamics of SARS -CoV-2, divid- ing the model in two sections. We first studied the within -host scale which we will divide into two dif- ferent levels. The first consists of twelve compartments to describe the life cycle of SARS -CoV-2, and  later the immune response. Finally, the between -host model consists of twelve compartments to describe  the transmission of the virus from the source that gave rise to the virus (bats), the host (pangolins proba- bly), the reservoir (Huanan Seafood Wholesale Market in Wuhan), until reaching the population human  where we have added a compartment to explain the environmental viral load, a key element to integrate  both scales.   Mathematical model  The mathematical model describes the contagion dynamics at two different sections scales, i.e.,          within-host model and between-host models. Each of them is described below.  Within-host model  The within -host model include the transmission dynamics from the life cycle of SARS -CoV-2, and     subsequently the model that reproduces the immune response will be explained.   SARS-CoV-2 life cycle  The virus is a single -stranded RNA virus belonging to the Coronaviridae family, Coronavirinae                 subfamily [19]. The first reading frame, known as ORF1ab (the largest gene), occupies two -thirds of the  virus sequence, approximately 28 -32 kb. It is a 5’-capped and 3’-polyadenylated positive -sense              single-strand RNA (+ssRNA), non -segmented and similar to the structure found in messenger RNA of  eukaryotic cells.The replication of the virus begins when the S protein of SARSCoV -2 binds directly to  the Angiotensin -Converting Enzyme 2 (ACE2) receptor. Once the virus has entered the host cell, it is  released into the cytoplasm, starting the replication process that will give rise to non -structural proteins  and also accessory proteins, with four structural proteins. The formation of RNA( -) and also replication  and transcription of RNA subgenomics. These subgenomic RNAs( -) are transcribed into mRNAs(+)  which encode the structural proteins S, M, E, N, and accessory proteins. During the replication process,  the N protein of the virus binds to the genome, while the M protein associates with the membranes of the  endoplasmic reticulum (ER). Finally, the virions are secreted by exocytosis [19].                             Vol 2  Issue 2  Pg. no. 3    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    Recently Isea and Mayo -Garc´ıa [20] have proposed the first analytical solution for the life cycle of  SARS-CoV-2 according to the previous description. This model can be seen in figure 1, where it has  been divided into four levels in different colors, where the first explains the cell entry, the second the  genome transcription and replication, then the translation of structural and accessory proteins, and finally  the assembly and eventual releases of new virions from the cell. The twelve variables that are going to  describe the SARS-CoV-2 life cycle model are shown in Table 1, so the equations are simply (details in  [20]):       Figure 1.Diagram of the life cycle of SARS-CoV-2.The level that describes the cell entry  is shown in blue color and indicated with number 1. The transcription and replication of  the genome is in red color (number 2), while that the translation of structural and acces- sory proteins, and the assembly and release of virions are show in green and orange col- or, respectively. Only a few constants are shown in the life cycle dynamics and all the  details are described in the text.     Here [Vrelease] is the concentration of viral particles that are released in the cell, and they are precisely the  ones that will trigger the immune response in people, as will be explained in the next section.                               Vol 2  Issue 2  Pg. no. 4    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    Variable Description Variable Description   Free virions outside of cell  [gRNA−] Negative sense genomic  and subgenomic   Number of virions bound to  ACE2  [gRNA] Positive sense ge- nomic  and subgenomics RNAs  [Vendosome]  Number of virions  in endosomes [N]  Concentration of N  proteins per virion  [gRNA+]  Number of ss-  Positive sensegenomic RNA [SP] S+M+E per virion  [NSP] Abundance of nonstructural  protein populations   molecules  [Vassemble] Assembled virions   Released virions  Table 1: Description of the variables used to describe the life cycle of SARS- CoV-2 (details in [20]).                             Vol 2  Issue 2  Pg. no. 5    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research      Modelling the immune response  The immune response that is triggered in people as a result of infection by SARSCoV -2 virions has been  described in various models in the scientific literature [21, 22], but most posts solve it numerically. Fur- thermore, recent studies have associated a storm of cytokines generated by SARS -CoV-2 such that it  leads lymphocytes to the lungs in search of infection, and thus maintains the replication and transmission  of the virus [23].   Of all the models available in the scientific literature, we only consider the simplest of them, as shown in   Figure 2: Diagram illustrating the immune response (see text for  more details)    Figure 2, where the target cells (cells without contagion and represented by the letter T) are going to be  infected (I) as a result of SARS -CoV-2 viral particles (V), so that an immune response product of anti- bodies is generated (A) (details in [22]). The equations that describe the above process are:   Where T, I, V, and A represent target cells, infected cells, viral particles, and antibodies, respectively.  The definitions of parameters will be derived directly from the work of Danchin et al [24]. So both the  life cycle of the virus and the dynamics of the immune response present the viral load, which is the link  between them.                             Vol 2  Issue 2  Pg. no. 6    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    Figure 3: The Flowchart of the model proposed in this paper. The infection  source is shown in blue color and it is a bats population, represented with B  subscript. The host region is green and depicted with the H subscript (some  bioinformatics studies indicated that could be pangolin population). The  Reservoir (violet re- gion) was the Seafood Market in Wuhan. Finally, the  human people are the last section and identified with the P subscript in cyan  color.                             Vol 2  Issue 2  Pg. no. 7    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research        Between-hosts model  Figure 3 represents a model of the transmission dynamics from the possible origin of the virus associated  with bats (denoted with the subscript B), then infecting an unknown host (usually associated with pango- lins, abbreviated with the letter H). Later it reaches the reservoir (i.e., Wuhan market, W), until finally  infecting the human population (P)   Therefore, this model that allow us to explain the transmission of the virus is given by the following    differential equations:   We can observe this model is divided into twelve categories where the subscripts B, H, and P have been  used to represent the bat, pangolin, and human, respectively. The Populations susceptible to contracting  the virus are SB, SH, and SP, and the population infected are IB, IH, and IP. RB, RH ,and RP represent the  recovered bat, pangolin, and human populations, and QP is the population that is in quarantine. The        reservoir is denoted as W , and the last compartment is VP, which represents the environmental viral load,  respectively.  The constants ΛB, ΛH and ΛP are the newborn bats, pangolins, and human, respectively. The contagion  rate in bats, pangolins, and humans is given by the constants βB, βH, and βP, respectively. Also, mB, mH,  and mP represent the death rate, and NB, N H and NP  are the total populations of bats, pangolins, and                                 Vol 2  Issue 2  Pg. no. 8    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    human, respectively; ωB, ωH, and ωP are the infectious period of bats, pangolins, and human,                    respectively. The infection rate between bats and pangolins is given by the variable βB H, where the       superscript and subscript identify the beginning and end of the rate of infection, respectively; while the  contagion rate from pangolins to the reservoir is given by βH w  , and βW P   from reservoir to human. These  values should be obtained by fitting the model with the collected data. Finally, εis the lifetime of the        virus in the  Reservoir. Given that each country maintains different policies to combat the virus, it is nec- essary to fit it according to infected data.  Mathematical analysis and numerical calculations  The resolution of the systems of equations have been validated in multiple scientific works [25, 26, 27,  28], so only the critical points and the eigenvalues of the system that allow us to elucidate the scenario to  explain the dynamical of contagious of Covid-19.   Analysis of the within-host model  SARS-CoV-2 life cycle  The mathematical details have been recently published by Isea and Mayo -Garcia [20], where two critical  points were derived as follows, noted with a letter each of the critical points obtained in each of the              submodels descried above.  The first critical point  It is the trivial solution of the system and occurs when all variables are zero:               There is no contagion by the virus and therefore, there is no spread of the virus (i.e., disease -free equi- librium). The interesting of this critical point is that when calculating the Jacobian of the system and  evaluating it at this critical point, the eigenvalues basically depends on the constants referring to the entry  of the virus into the human body.   Second critical point  The second critical point corresponds to an endemic model where the replication and transcription      process of the virus can be seen before being released:                                             Vol 2  Issue 2  Pg. no. 9    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    Parameter Value Parameter Value  Kdiss 0.61 dSP 0.04  Kbind  12.00 dN−gRNA 0.12  Kf use 0.50 dendosome 0.06  Kuncoat 0.50 K1 2.10  kcomplex 0.40 k2 2.99  Kassemble 1.00 K3 0.20  Krelease 8.00 K4 0.19  dgRNA 0.20 K5 37.32  dNSP 0.07 K6 4.39  dgRNA 0.10 K7 8.01  dN 0.02 K8 0.02  Ktr− 3.00 Ktr+ 1000.00  Table 2: Model parameters based on the Isea and Mayo-Garc´ıa’s models [20].  Figure 4: Evolution of virus transmission dynamics in the cell entry process (de - tails  in [20]).  Figure 5: Number of assembled particles ([Vassembled]) and released ([Vreleased]).                             Vol 2  Issue 2  Pg. no. 10    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research      Table 2 shows the values of the constants in this system of equations, and the graphic results are shown  in Figures 4 and 5, where it can be seen both the contagion dynamics when it infects the cell (fig 4) as  well as the number of particles that are assembled and released at the end of the SARS -CoV-2 life cycle  (fig 5).   Immune response  The four differential equations that allow the generation of the immune response when infected by the  virus are analyzed below. This model generates three critical points (marked with a different letter for  easy identification) as explained below.  The first critical point  The first critical point corresponds when there is no presence of the virus in the body (i.e., disease -free  equilibrium).        Once the Jacobian evaluated at this critical point has been evaluated, two eigenvalues are obtained;             When numerically evaluating these expressions according to the value of the different parameters (Table  3), we obtain -20.28 and +2.61, that is, the system is unstable (i.e., saddle point), so further studies are  necessary to identify the stability conditions of the system as well as a stability analysis in said system.   Second critical point   The second value corresponds to the case when the infection process is being generated, without           generating an antibody response, described as follows:       The eigenvalues in the system are:      When evaluating these eigenvalues, the following values are obtained: -1.86E+7, and 15.53, so it is again  unstable at this critical point (saddle point).   Third critical point   The last critical point is the one where the immune response is activated by the presence of antibodies,  obtaining:                              Vol 2  Issue 2  Pg. no. 11    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research      Unfortunately, it has not been an easy task to derive the expression of the eigenvalues for this case.    It only remains to visualize how the immune response varies against the pres - ence of the virus as            identified in Figure 6, according to the values of the parameters indicated in Table 3 (these values were  obtained from the work of Danchin et al [24]).  Between-hosts model  The analytical resolution of the system of equations generates twelve different scenarios represented by  the twelve critical points, where the following critical point notation (where i runs from 1 to 12) has been  Figure 6: Immune response generated by target cells (T), cells infected (I) by  SARS-CoV-2 virions (V), and antibodies generated and denoted as A.  Parameter value  µ 9.66  λT 9.66x106  β 1.28x10-6  δ 16.22  c 1.45  b 0.52  σ 0.02  w 59.74  α 9.15x10-7  Table 3: Parameters defined in the dynamics of viral infection according to the  work of Danchin et al [24 ].                             Vol 2  Issue 2  Pg. no. 12    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    used to identify each of the scenarios derived in the work.  First critical point  The first critical point (CP 1) is the trivial solution of the system, that is, when there are no infections in  any of the populations:            This result indicates the population that may be susceptible to contracting the virus are ΛB /mB, ΛH /mH   and  ΛP /mP  corresponding to the populations of bats, pangolins and  humans, respectively.  The next step is to determine the equilibrium conditions. The following notation has been used to        describe the eigenvalues: the superscript indicates the critical point, while the subscript is the number of  different eigenvalues obtained. For example, λ3 1 corresponds to the third eigenvalue of the first critical  point. By doing this, three different eigenvalues are obtained:      where three new variables (∆H, ∆ B and ∆P) have been introduced indicated in Table 4 to simplify the  equation in this paper. These solutions reveal that the different populations depend exclusively on each  population and are not conditioned on each other.  Second critical point  The second critical point (CP2) is when only a fraction of susceptible human population without presence  of the virus in the environment (W * = 0, VP *= 0) is considered:              This result reveals that it is possible to circulate the virus in the population of people who may be        infected without the virus being present in the environment, infecting other people. By repeating the    procedure described above, five eigenvalues are obtained:                                       Vol 2  Issue 2  Pg. no. 13    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    The first two eigenvalues correspond to the equilibrium condition when the virus is not present in bats or  in the host, while the last three correspond to con - ditions in the human population. These values must be  adjusted with the records of contagions made in the world, as will be explained in future works.   B0 = mB + wB + β B H  H0 = mH + wH + βH W  P0 = mP + wP  ∆B = NBmBB1 − βBΛB  ∆H = NHmhH1 − βHΛH  Table 4: Definitions made to simplify mathematical ex- pressions obtained at work.  Third critical point  The third critical point (CP3) relates to the scenario when the bat population is infected, and represents the  origin of the infection in the human population, so that the host can simply indicate the fraction of people  susceptible to such disease, and described by the following equations:              It should be noted that the host population that can be infected by the virus depends directly on the rate of  infection of the bat. The eigenvalues obtained are:                                           Vol 2  Issue 2  Pg. no. 14    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    The last eigenvalues are a combination of conditions between the bat and host populations, and the     equilibrium conditions depend on the values ∆B and ∆H. Actually, these values should not be easy to   obtain because there are still many questions about how the virus affects the human population.   Fourth critical point  The fourth critical point (CP 4) represents the scenario when the bat population and the human population  are infected, without the host or the environment presenting the virus.              Where the new variables indicated in this critical point are listed in Table 4. The six eigenvalues in this  scenario are:                   Expressions of the combinations between species are already beginning to be obtained, which                complicates the obtaining of said equations.   Fifth critical point   The fifth critical point (CP 5 ) corresponds to the case when the population of pangolins is infected and  the virus is also in the Reservoir, so a part of the human population is susceptible to contracting this      disease, that is:               This solution reveals how the environment (W * ) plays an important role in the human population that  can contract this disease. The four eigenvalues are:                               Vol 2  Issue 2  Pg. no. 15    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    As can be seen above, the first two eigenvalues depend exclusively on the parameters of the pangolins,  then on the bats, and finally the last is a combination of the parameters between the host and human     people.   Sixth critical point   The sixth critical point (CP6 ) corresponds to the case when bat populations and humans are infected, that  is:               The eigenvalues from this critical point have not been easy to obtain.  Seventh critical point  The seventh critical point (CP 7) corresponds to the case when the virus is present in bats, pangolins, and  in the reservoir.                It should be noted that the human population that is susceptible to contracting the virus depends               significantly on the reservoir. Only we can calculate, the eigenvalues corresponding to the bat                         population.                                       Vol 2  Issue 2  Pg. no. 16    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    Eighth critical point   The last critical point (CP 8 ) corresponds to the case when the three populations are infected by Covid-19,  without being present at V* P = 0, and this result should be analyzed in more detail in the future .                 This critical point is when everyone is infected, and unfortunately it has not been possible to determine  the eigenvalues for this system.  Ninth critical point  This critical point (CP 9) corresponds to the case when the human population is present the virus, where  VP * ≠ 0, and should be the scenario capable of describing the current transmission of the virus in different  parts of the world.             This scenario tells us how the virus is transmitted in the human population once it has been infected by  the environment.  Tenth critical point  The tenth one corresponds to the scenario where the bats, the reservoir, and the human population are  infected such that VP * ≠ 0               Unfortunately it has not been easy to drive the eigenvalues.                               Vol 2  Issue 2  Pg. no. 17    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    Eleventh critical point  The penultimate critical point corresponds to the case when the virus is present in the host, the             environment, and the human population.                Unfortunately it has not been possible to drive the eigenvalues.  Last critical point  The last critical point is when the virus circulates and is infecting all species. It is the most general       possible model that can explain the spread of the virus from its origin.                This scenario is one where the virus is transmitted between all species. Unfortunately it has not been easy  to derive the eigenvalues.   It only remains to indicate, as an example, some of the values of the constants used in the last equations.  These values can be seen in a study carried out by Isea [28] after adjusting the data according to the     contagion data present in Australia, using a least squares adjustment, where there is infection in the     population of neither bats nor pangolins. The values obtained were: β P W = 1.708, βP = 1.215, ωP = 0.001,  ΛP = 81.212, µ = 1.100, ε = 0.001 (setting the mP value to 0.0000456). The value of VP must be adjusted  with experimental data under a range of hypotheses that will be presented in a future work.   Conclusion   In the present work, a mathematical model has been proposed to study the transmission dynamics of  Covid-19 from two different scales where the viral load of the virus is the key to unifying both these time  scales. The advantage of this  model is that it is possible to focus on a particular study scheme derived  directly from the equilibrium conditions of the system, that is, there is a mathematical basis that justifies  the existence of said scenario based on the critical points derived in the work. It should be pinpointed that  all the models have been described with a deterministic model and the conditions to reach equilibrium  have been obtained in most of them. In the next publication, the between -host model will be analyzed  considering the sensitivity analysis and also the measures of social distancing and how quarantine affects                             Vol 2  Issue 2  Pg. no. 18    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    the dynamics of contagion will be described.    Acknowledgments   The author appreciates the unconditional help of Rafael Mayo -Garcia when reading this work immersed  in a sea of differential equations, and also Jesus Isea for reading and helping me with the preparation of  this manuscript.    Conflict of interest   The author declares that there is no conflict of interest.   Data Availability   Data available within the article.   References   1. Pan American Health Organization / World Health Organization. Epidemiological Alert: Novel  coronavirus (nCoV). 16 January 2020, Washington, D.C.: PAHO/WHO; 2020.   2.  F. Wu, S. Zhao, B. Yu, Y. M. Chen, W. Wang, Z. G. Song, Y. Hu, Z. W.Tao, J. H Tian, Y. Y.  Pei, M. L. Yuan, Y. L. Zhang, F. H. Dai, Y. Liu, Q. M. Wang, J. J. Zheng, L. Xu, E. C. Holmes,  and Y. Z. Zhang, A new coronavirus associated with human respiratory disease in China, Nature  579(2020), 265-269. Doi: 10.1038/s41586-020-2008-3.  3.  D. Cucinotta, and M. Vanelli, WHO Declares COVID-19 a Pandemic, Acta Biomed. 91(2020),  157160. Doi: 10.23750/abm.v91i1.9397.  4.  R. H. Xu, J. F. He, M. R. Evans, G.W. Peng, H. E. Field, D. W. Yu, C. K. Lee, H. M. Luo, W. S.  Lin, P. Lin, L. H. Li, W. J. Liang, J. Y. Lin, and A. Schnur, Epidemiologic clues to SARS origin  in China, Emerg. Infect.Dis, 10(2004), 10301037. Doi: 10.3201/eid1006.030852.  5.  A. Mostafa, A. Kandeil, M. Shehata, R. El Shesheny, A. M. Samy, G. Kayali, and M. A, Ali,  Middle East Respiratory Syndrome Coronavirus (MERS-CoV): State of the Science, Microorgan- isms, 8(2020), 991. Doi: 10.3390/microorganisms8070991.  6. R. Isea, Identificacion de once candidatos vacunales potenciales contra la ´malaria por medio de  la Bioinformatica, ´ Vaccimonitor 19(2010), 15-19.  7.  R. Isea, Mapeo computacional de ep´ıtopos de celulas B presentes en el virus ´ del dengue, Rev.  Inst. Nac. Hig. Rafael Rangel, 44(2013), 25-29.  8.  R. Isea, Quantitative computational prediction of the consensus B-cell epitopes of 2019-nCoV,  Journal of Current Viruses and Treatment Methodologies, 1(2020), 42-47. Doi: 10.14302/ issn.2691-8862.jvat-20-3278.  9.  R. Isea, E. Montes, A.J. Rubio-Montero, and R. Mayo-Garcia, Computational challenges on Grid  Computing for workflows applied to PhylogenyInternational Work -Conference on Artificial Neu- ral Networks (2009), 1130-1138.  10.  R. Isea, Characterizing the Dynamics of Covid-19 Based on Data, Journal of Current Viruses and  Treatment Methodologies 1(2021), 25-30.  11.  A. S. Deif, and S. A. El-Naggar, Modeling the COVID-19 spread, a case study of Egypt. Journal  of the Egyptian Mathematical Society, 29(2021), 13. Doi: 10.1186/s42787-021-00122-x.  12. R. Dragone, G. Licciardi, G. Grasso, C. Del Gaudio, and J. Chanussot, Analysis of the Chemical  and Physical Environmental Aspects that Promoted the Spread of SARS -CoV-2 in the Lombard  Area. International journal of environmental research and public health, 18(2021), 1226. Doi:  10.3390/ijerph18031226.  13.  S. Q. Du, and W. Yuan, Mathematical modeling of interaction between innate and adaptive im- mune responses in COVID-19 and implications for viral pathogenesis. Journal of medical virolo- gy, 92(2020), 16151628. Doi: 10.1002/jmv.25866.                            Vol 2  Issue 2  Pg. no. 19    ©2023 Raul Isea . This is an open access article distributed under the terms of the Creative Com- mons Attribution License, which permits unrestricted use, distribution, and build upon your work  non-commercially.  Journal of Model Based Research    14.  R. Ghostine, M. Gharamti, S. Hassrouny, I. Hoteit, Mathematical Modeling of Immune Respons- es against SARS-CoV-2 Using an Ensemble Kalman Filter. Mathematics 9(2021), 2427. Doi:  10.3390/math9192427.  15. 15 X. Z. Li, S. Gao, Y. K. Fu, and M. Martcheva, Modeling and Research on an Immuno - Epidemiological Coupled System with Coinfection. Bull. Math Biol, 83(2021), 116. Doi:  10.1007/s11538-021-00946-9.  16.  N. Dorratoltaj, R. Nikin-Beers, S.M. Ciupe, S.G. Eubank, and K.M. Abbas, Multi-scale immuno- epidemiological modeling of within-host and betweenhost HIV dynamics: systematic review of  mathematical models. PeerJ, 5(2017), e3877. Doi: 10.7717/peerj.3877.  17.  Y. Xue, Y. Xiao, Analysis of a multiscale HIV-1 model coupling withinhost viral dynamics and  between-host transmission dynamics, Mathematical Biosciences and Engineering, 17(2020), 6720 -6736.  18.  L.N. Murillo, M. S. Murillo, and A. S. Perelson, Towards multiscale modeling of influenza infec- tion, Journal of theoretical biology, 332(2013), 267290. Doi: 10.1016/j.jtbi.2013.03.024.   19.  S. Kumar, R. Nyodu, V.K. Maurya, and S. K. Saxena, Morphology, Genome Organization, Rep- lication, and Pathogenesis of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS -CoV-2),  Coronavirus Disease 2019 (COVID-19): Epidemiology, Pathogenesis, Diagnosis, and Therapeu- tics, 2331. Doi: 10.1007/978-981-15-2814-7_3.  20.  R. Isea, and R. Mayo-Garc´ıa, First analytical solution of intracellular life cycle of SARS-CoV-2,  Journal of Model Based Research, 1(2022), 6-12. Doi: 10.14302/issn.2643-2811.jmbr-22-4130.  21.  T. Alzahrani, Spatio-Temporal Modeling of Immune Response to SARS-CoV-2 Infection. Math- ematics, 9(2021), 3274. Doi: 10.3390/ math9243274.  22.  S.M.E.K. Chowdhury, J.T. Chowdhury, S.F. Ahmed, P. Agarwal, I.A.Badruddin, S. Kamangar,  Mathematical modelling of COVID-19 disease dynamics: Interaction between immune system  and SARS-CoV-2 within host, AIMS Mathematics, 7(2022), 2618-2633. Doi: 103934/math.  2022147.  23.  R. F. Reis, A.B. Pigozzo, C.R.B. Bonin, B. Quintela, L.T. Pompei, A.C. Vieira, L. Silva L, M.P.  Xavier, R. Weber dos Santos, and M. Lobosco, A Validated Mathematical Model of the Cytokine  Release Syndrome in Severe  COVID-19, Front. Mol. Biosci. 8(2021),639423. Doi: 10.2289/ fmolb. 2021.639423.  24.  A. Danchin, O. Pagani-Azizi, G. Turinici, G. Yahiaoui, (2020). COVID-19 adaptive humoral  immunity models: weakly neutralizing versus antibodydisease enhancement scenarios, medRxiv  2020.10.21.20216713.  25.  R. Isea, K.E. Lonngren, Toward an early warning system for Dengue, Malaria and Zika in Vene- zuela, Acta Scientific Microbiology 1(2018), 30-32. Doi: 10.31080/ASMI.2018.01.0025  26.  R. Isea, A preliminary model to describe the transmission dynamics of Covid -19 between two  neighboring cities or countries. medRxiv preprint.Doi: 10.1101/2020.07.18.20156695.   27.  R. Isea, Una aproximacion matem ´ atica de la acci ´ on de la radioterapia encelulas cancerosas, ´  Revista Conocimiento Libre y Licenciamiento (CLIC),11(2015), 150 -160.  28.  R. Isea, Analytical Solutions of the Transmissibility of the SARS-CoV2 in Three Interactive Pop- ulations, International Journal of Coronavirus,2(2020), 1. Doi: 10.14302/issn.2692 -1537.ijcv-21- 3732.",
      "references": [
        "Epidemiological Alert: Novel coronavirus (nCoV).",
        "A new coronavirus associated with human respiratory disease in China",
        "WHO Declares COVID-19 a Pandemic",
        "Epidemiologic clues to SARS origin in China",
        "Middle East Respiratory Syndrome Coronavirus (MERS-CoV): State of the Science",
        "Identificacion de once candidatos vacunales potenciales contra la ´malaria por medio de la Bioinformatica,",
        "Mapeo computacional de ep´ıtopos de celulas B presentes en el virus ´ del dengue",
        "Quantitative computational prediction of the consensus B-cell epitopes of 2019-nCoV",
        "Computational challenges on Grid  Computing for workflows applied to PhylogenyInternational Work -Conference on Artificial Neu- ral Networks",
        "Characterizing the Dynamics of Covid-19 Based on Data",
        "Modeling the COVID-19 spread, a case study of Egypt.",
        "Analysis of the Chemical  and Physical Environmental Aspects that Promoted the Spread of SARS -CoV-2 in the Lombard  Area.",
        "Mathematical modeling of interaction between innate and adaptive im- mune responses in COVID-19 and implications for viral pathogenesis.",
        "Mathematical Modeling of Immune Respons- es against SARS-CoV-2 Using an Ensemble Kalman Filter.",
        "Modeling and Research on an Immuno - Epidemiological Coupled System with Coinfection.",
        "Multi-scale immuno- epidemiological modeling of within-host and betweenhost HIV dynamics: systematic review of  mathematical models.",
        "Analysis of a multiscale HIV-1 model coupling withinhost viral dynamics and  between-host transmission dynamics",
        "Towards multiscale modeling of influenza infec- tion",
        "Morphology, Genome Organization, Rep- lication, and Pathogenesis of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS -CoV-2)",
        "First analytical solution of intracellular life cycle of SARS-CoV-2",
        "Spatio-Temporal Modeling of Immune Response to SARS-CoV-2 Infection.",
        "Mathematical modelling of COVID-19 disease dynamics: Interaction between immune system  and SARS-CoV-2 within host",
        "A Validated Mathematical Model of the Cytokine  Release Syndrome in Severe  COVID-19",
        "COVID-19 adaptive humoral  immunity models: weakly neutralizing versus antibodydisease enhancement scenarios",
        "Toward an early warning system for Dengue, Malaria and Zika in Vene- zuela",
        "A preliminary model to describe the transmission dynamics of Covid -19 between two  neighboring cities or countries.",
        "Una aproximacion matem ´ atica de la acci ´ on de la radioterapia encelulas cancerosas,",
        "Analytical Solutions of the Transmissibility of the SARS-CoV2 in Three Interactive Pop- ulations"
      ],
      "meta_data": {
        "arxiv_id": "2308.01918v1",
        "doi": "10.14302/issn.2643-2811.jmbr-23-4556",
        "authors": [
          "Raul Isea"
        ],
        "published_date": "2023-07-11T22:23:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes a deterministic multiscale (immuno-epidemiological) framework intended to be analytically tractable for SARS‑CoV‑2 transmission by coupling (i) a within-host intracellular life-cycle model and (ii) an immune-response model to (iii) a between-host, multi-species transmission model (bats→intermediate host/pangolin→reservoir/Wuhan market→humans). The coupling variable across scales is viral load, including an explicit environmental viral-load compartment for humans. Derives multiple equilibrium (critical) points for each subsystem to interpret possible outbreak scenarios and discusses stability via eigenvalues for several equilibria.",
        "methodology": "Uses systems of ordinary differential equations. Within-host: (1) a 12-compartment intracellular life-cycle model (virus binding/entry, replication/transcription, translation, assembly, release) taken from Isea & Mayo-García (2022) with reported analytic solution and parameter set; released virions provide within-host viral load. (2) a 4-compartment target-cell/ infected-cell/virus/antibody (T–I–V–A) immune-response ODE model adapted from the literature (Chowdhury et al.; parameters from Danchin et al.), analyzed via equilibrium points and Jacobian eigenvalues. Between-host: a 12-compartment SIR-type multi-population ODE model with bats (SB,IB,RB), intermediate host/pangolins (SH,IH,RH), humans (SP,IP,RP) plus quarantine (QP), reservoir contamination (W), and environmental viral load (VP). Performs equilibrium (critical point) analysis for up to 12 scenarios and partial local stability assessment using Jacobian eigenvalues; mentions least-squares fitting (from prior work) for some human-only parameters.",
        "experimental_setup": "No new empirical dataset is introduced in this paper. Numerical illustrations are provided for the intracellular life-cycle dynamics using parameter values from Isea & Mayo-García (2022) (Table 2) and for the immune-response dynamics using parameters from Danchin et al. (2020) (Table 3), with time-course plots for entry/assembly/release and T/I/V/A trajectories. For the between-host model, the paper primarily presents analytical equilibria; it cites a prior least-squares fit to Australian case data (Isea 2020/2021) to give example parameter values for a reduced human-reservoir setting (e.g., βPW, βP, ωP, ΛP, μ, ε with mP fixed). Validation is qualitative/analytical (equilibria and eigenvalues) rather than benchmarked prediction performance.",
        "limitations": "Many equilibria lack full eigenvalue derivations; stability is only partially analyzed, and key thresholds (e.g., R0) and global dynamics are not established. The coupling between within-host viral load and population-level transmission is conceptually stated (via VP) but not parameterized/identified with real measurements; VP and several cross-species rates (βBH, βHW, βWP) are acknowledged as needing fitting yet no fitting procedure/data are provided here. Assumes deterministic, well-mixed compartments (no stochasticity, spatial structure, age structure, contact networks, heterogeneity, variants, vaccination). Intermediate host choice (pangolin) and reservoir pathway are assumed/simplified. Parameter sources are borrowed from prior/other-disease contexts; identifiability and uncertainty/sensitivity are not addressed in this article, and quarantine/social distancing effects are not explicitly modeled beyond a Q compartment.",
        "future_research_directions": "Complete local (and ideally global) stability analysis for all equilibria, including derivation of eigenvalues for unresolved critical points and computation of epidemiological thresholds (e.g., R0) for each scenario. Develop and validate a rigorous data-fitting/parameter-inference pipeline (e.g., Bayesian or frequentist) using time-series case data, environmental surveillance, and within-host viral-load data to identify VP dynamics and cross-species transmission parameters; assess identifiability. Extend the between-host model to incorporate explicit intervention functions (social distancing, masks, quarantine policies, vaccination, waning immunity) and perform sensitivity/uncertainty analyses. Add stochastic and/or spatial components to capture superspreading and heterogeneous mixing. Improve within-host immunity by including cytokine dynamics/immune-cell compartments and link within-host viral kinetics to infectiousness over time. Evaluate model reduction/aggregation to create practical forecasting tools and compare against standard COVID-19 benchmarks across multiple countries.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Iteratively Refined Behavior Regularization for Offline Reinforcement Learning",
      "full_text": "To appear in “Prospects for low-frequency radio astronomy in S. A. (December 29, 2023)”RevMexAA(SC) END TO END DEVELOPMENTS FOR THE MULTIPURPOSE INTERFEROMETER ARRAY PATHFINDER FROM THE IAR ELECTRONICS LABORATORY J. M. Gonzalez,1 H. Command,1 and G. Valdez1 RESUMEN El Multipurpose Interferometer Array Pathfinder (MIA), desarrollado desde el Instituto Argentino de Radioas- tronom´ ıa (IAR), es un instrumento radioastron´ omico basado en t´ ecnicas de interferometr´ ıa, dise˜ nado para la detecci´ on de emisi´ on en radio de fuentes astron´ omicas. La primera fase consta de 16 antenas de 5 metros de di´ ametro, con posibilidad de aumentar su n´ umero. Adem´ as, est´ a equipado con un receptor de polarizaci´ on dual con un ancho de banda de 250 MHz, centrado en 1325 MHz, y un digitalizador y procesador para las funciones de correlaci´ on. Para el desarrollo de este instrumento se est´ a construyendo actualmente un pathfinder de tres antenas con sus etapas de control de posicionamiento, sistemas de radiofrecuencia, adquisici´ on y procesado. En este trabajo se describir´ a el dise˜ no conceptual y sus avances actuales para cada etapa. ABSTRACT The Multipurpose Interferometer Array Pathfinder (MIA), developed from the Argentine Institute of Radio Astronomy (IAR), is a radio astronomical instrument based on interferometry techniques, designed for the detection of radio emission from astronomical sources. Phase one consists of 16 antennas of 5 meters in diameter, with the possibility of increasing their number. In addition, it is equipped with a dual polarization receiver with a bandwidth of 250 MHz, centered at 1325 MHz, and a digitizer and processor for the correlation functions. For the development of this instrument, a three antenna pathfinder is currently being built with its positioning control, radio frequency systems, acquisition and processing stages. This paper will describe the concept design and their current progress for each stage. Key Words:Interferometer Array — Radioastronomy — Instrumentation 1. INTRODUCTION Since the detection of radio signals from astro- nomical sources, radio astronomy has taken on a remarkable importance in the scientific community (Rohlfs & Wilson 2013; Martin & Irvine 1983). On the other hand, the advantages of interfero- metric techniques over the detection of such signals by single-dish antenna methods have led to the devel- opment of multiple interferometers in recent years. (Kocz et al. 2019; Foley et al. 2016; Hickish et al. 2016). Because of this, the Instituto Argentino de Radioastronom´ ıa plans to develop the Multipurpose Interferometer Array (MIA), located in Argentina. Due to the characteristics of the design and its loca- tion, it is estimated that this instrument will make a major contribution to the scientific community. This paper details the status of the MIA design. The following section details the development of the Antenna and Front-End. Then it explains the status of the Back-end stage. Section 4 details the control algorithm used for the positioning of the antenna. 1Instituto Argentino de Radioastronom´ ıa, IAR, Camino Gral. Belgrano Km 40, Berazategui, Buenos Aires, Argentina. Finally, the conclusions obtained are developed. 2. ANTENNA AND FRONT-END The MIA dishes, designed with a prime-focus alt- az configuration and a focal ratio ( f D ) of 0.43, are equipped with a dual polarization receiver centered at 1.5 GHz. This is complemented by a dual po- larization feed operating at the same frequency. The feed’s illumination efficiency is characterized by a ’10 dB edge taper’, ensuring an optimal balance between spillover and sub-illumination. In this context, ’f’ represents the focal length of the paraboloid, which is the distance from the vertex to the focus, and ’D’ denotes its diameter, the maximum distance across the reflector. To achieved the best cost-benefit ra- tio, 5 m of the antenna diameter was chosen (Dreher 2000). The Front-End component of MIA is depicted in Figure 1. It is composed of two Vivaldi antennas, set at 90 degrees to each other, each corresponding to a distinct polarization. These antennas are equipped with a directional coupler, into which the required noise level is introduced for calibration purposes. 1 arXiv:2312.17066v1  [astro-ph.IM]  28 Dec 20232 GONZALEZ, COMMAND, & VALD ´EZ The output of each antenna is connected to a pri- mary stage of low-noise amplifiers (LNA), followed by a bandpass filter with a bandwidth ranging from 1000 MHz to 2000 MHz. Subsequently, the signal undergoes a secondary amplification stage utilizing LNA. The receiver features an equivalent noise tem- perature of approximately 50 K and a gain of 60 dB. The bandwidth is determined by the chosen band- pass filter. The receiver enclosure will be at ambient temperature with temperature control implemented via Peltier cells on critical components, such as the first-stage amplifiers and the noise-generating diode for calibration. RF output will be transmitted via optical fiber. Vivaldi antennas are an attractive choice for dual-polarization radio interferometry applications due to their ultra-wideband capabilities, uniform ra- diation patterns, and ease of implementing dual po- larization. Their compact and modular design sim- plifies fabrication and assembly while maintaining good performance across a wide frequency range. Despite some limitations, such as performance at lower frequencies and sensitivity to substrate proper- ties, careful design and substrate selection can miti- gate these issues, making Vivaldi antennas a suitable option for radio astronomy applications. Figure 2 detailed the Vivaldi reflector scheme. There, phase center location changes with changes in the frequency and in any wide band application, phase error losses are unavoidable. The left dot on the Vivaldi antenna corresponds to the phase cen- ter position for the highest frequencies, while the right dot for the lowest, which also aligns with the parabolic reflector’s focal point. With its current po- sitioning, it would be possible to use the reflector sys- tem perfectly at low frequencies of the band. How- ever, at high frequencies there will be distance be- tween the phase center location and the focus point of the reflector. This will result in phase error losses in the system. The half-subtended angle of the re- flector ψ0 is related with f D by, f D = 1 4cotψ0 2 . (1) Unequal phase center locations in E and H-planes introduce phase error losses due to astigmatism. It is detected by the depth of the nulls in the E and H-planes. Phase error loss due to astigmatism is not as severe as the losses due to axial defocusing. Vivaldi is an end fire radiator supported on a thin substrate with a relative permittivity (ϵr) of 4.5. This substrate is made of FR4, a glass-reinforced epoxy laminate material, with a thickness of t = Fig. 1. Front-End System Block Diagram. Fig. 2. Vivaldi Reflector. 1.53 mm. Despite the completely planar geome- try of Vivaldi, it can produce almost symmetric ra- diation patterns in the E and H-planes. The an- tenna is designed to operate in the band of 1 GHz – 2.3 GHz. A lower cut-off wavelength can be de- fined where the maximum separation of the radiators at the distant end is a half-wavelength. Constant beamwidth requires an aerial to have a shape that can be completely specified in terms of dimensionless normalized wavelength units. The dual polarization is achieved by orthogonally placing two Vivaldi an- tennas along their outer edges as shown in Figure 3 (a). To facilitate the insertion of both antennas, lon- gitudinal cuts are introduced. Specifically, one cut measures 185 mm and is positioned on the front side of one of the Vivaldi antennas, while the other cut, measuring 79 mm, is located on the rear side of the other Vivaldi. These specific dimensions and place- ments ensure that the antennas interlock appropri- ately, thus forming the dual Vivaldi antenna config- uration, as detailed in Figure 3 (b). The antenna feeding consists of a microstrip-to-slot line balun for wideband matching as depicted in Figure 3 (c) and (d). A four-stage quarter-wave balun is designed to transform the 50 Ω microstrip line feeding to the 120-140 Ω slot line impedance.END TO END DEVELOPMENTS FOR THE MIA PATHFINDER 3 Fig. 3. Vivaldi Assembly. (a) Optimized dimensions of two orthogonal Vivaldi antennas in a cross-shaped form. (b) Longitudinal cuts are made to make the insertion of both antennas possible. (c) and (d) Detailed view of three-stage quarter-wave balun feed and noise injection RF coupler. 3. BACK-END The analogue signals presented are digitized and packetized in the Back-End module using the Smart Network ADC Processor (SNAP 2) boards, as is shown in Figure 4. The SNAP board is been con- formed by three HMCAD1511 8-bit analog to digital converters (ADCs), capable of sampling two inputs each at 500 MHz for a total of 6 signals per board. The ADCs are connected to a Kintex-7 160T field- programmable gate array (FPGA), with an associ- ated dual 10 GbE port. The SNAP board is con- trolled via Raspberry Pi, which interacts with a PC over ethernet via PYTHON scripts. The firmware for the hardware interfaces for the SNAP board is provided by the updated JASPER fork of the CASPER toolflow (Parsons et al. 2008). In Figure 4 the PPS signal represents the syn- chronism signal of the system. The 10 MHz signal is used for ADC calibration. For the initial stage, it is planned to process the signal coming from 3 antennas, for which two polarizations are obtained, using 6 ADC inputs of the SNAP board. Between the output of the antennas and the inputs of the SNAP board, bandpass filters are placed, consisting of a low-pass filter and a high-pass filter, and also a low noise amplifier. On the other hand, the board can transmit data through a 1Gb/s and a 10 Gb/s ethernet ports. Figure 5 shows the processing stages in the FPGA. There, the three pairs of polarisations are sampled by 6 ADCs. For this, the ADCs operate 2https://casper.berkeley.edu/wiki/SNAP Fig. 4. Back-End diagram. Fig. 5. Correlator system diagram. at a frequency of 500 MHz, achieving an effective bandwidth of 250 MHz, at a centre frequency of 1325 MHz. These sampled signal pairs are then synchro- nised to maintain signal coherence. Once the signals are aligned, the CASPER li- braries are used to transform them to the frequency domain. For this, a polyphase bank filter (PBA), implemented through a finite response filter (FIR), and the fast Fourier transform (FFT) are used. The FIR has a 512-point four-tap Hamming window re- sponse (for 256 spectral channels). The filter coeffi- cients are 18 bits, thus allowing the 8-bit ADC data to be increased to a maximum of 18 real bits and 18 imaginary bits. Once the signal is transformed to the frequency domain, it is quantified in 6 channels of 256 points. The correlation process is carried out in parallel by the multiply and accumulate (MAC) block belonging to the CASPER group libraries. Then, the generated signal passes through a packetizer and a first in-first out (FIFO) block, where the data packet is condi- tioned. At least, the data is transmitted at 10 Gb/s via the ethernet port. 4. POSITION CONTROL The positioning of the antenna to the desired source is achieved with the control of two three-phase induction motors (Gawronski 2008). The antenna4 GONZALEZ, COMMAND, & VALD ´EZ Fig. 6. Layered model of the position controller. mount is altazimuthal and this provides indepen- dence on each of the axes for the positioning. The tools used in the development of the position control system are CMake, Ceedling, and PYTHON. PYTHON is used to generate trajectories, astronom- ical calculations, and the development of the appli- cation logic. Ceedling is a framework used for soft- ware mocks, unit testing, and code coverage report- ing. CMake performs the compilation of embedded systems independently. The controller software is realised in a layered architecture detailed in the Figure 6. There, the in- terface layer is in charged of bidirectional communi- cations with the outside world. The web app man- ages the connections and communication with the antenna controller hardware. In addition, it is re- sponsible for updating radio sources, conflict resolu- tion and user administration. Furthermore, it man- ages the observation schedule, the system teleme- try data collection, the calibration noise source and sends the position data to the controller. This appli- cation is implemented on an embedded Linux system (beaglebone black-Cortex A9). The interface layer is currently under development. On the other hand, the controller layer is respon- sible for obtaining the environmental parameters and position control. This layer is composed by a dis- tributed system made up of three RP2040 microcon- trollers. Two of them are intended for position con- trol and the third for environmental telemetry. In the case of the microcontrollers used in the control stage, one core is dedicated to the use of I2C (Inter- Integrated Circuit) communication, while the other is in charge of control. The controller layer has a bidirectional commu- nication with the management app layer. In addi- tion, the controller layer sends the telemetry data of the environment and of the control to the man- Fig. 7. Block diagram of the controller layer. agement app layer, and receives from the latter the position to be pointed, together with the positioning events. These events can be to activate/deactivate the Wait, tracking or Alarm state. In Wait or Alarm mode the positioning is at the zenith. The angular control system is the same for both angles (azimuth and height). Communication between the cores occurs through the use of a queue. Each time the core receives a command via I2C, it performs a validation of the angular command, and if this command is valid, it is sent to the second core. The microcontroller has two more cores, called PIOs (Programmable Input Output). These PIO machines, in turn, have 4 slots of 32 memory in- structions, where each of them can execute instruc- tions in parallel to the main cores of the application. These PIO machines communicate with the proces- sor via IRQ (Interruption Request), or by clearing the buffers. In addition, they can communicate with GPIO (General Purpose Input Output). The initialisation software consists of a fuzzy logic self-tuning PI controller (Kanagaraj et al. 2008). The reading of the environmental and electrical variables is performed every two seconds by the main core, by the architecture known as superloop. For the humidity reading the DHT11 sensor was used. On the other hand, the code coverage was carried out by Ceedling, obtaining the expected results. From the interface layer, the Gpredict3 and Stel- larium4 software stages have been developed, with the API (Application Programming Interface) and the web server still to be defined. The interface layer receives coordinates and transforms them to the lo- cal system adopted by the antenna position. In the case of Stellarium, these are given in right ascension and declination. To carry out the coordinate trans- formation, a routine has been created in the C pro- gramming language. This routine was tested on the ATMEGA328P microcontroller (with an 8-bit archi- tecture, see algorithm in Curtis). The error of the 3http://gpredict.oz9aec.net/ 4https://stellarium.org/es/END TO END DEVELOPMENTS FOR THE MIA PATHFINDER 5 sidereal time calculation was approximately 70 msec. In the case of Gpredict, the coordinates come in local altazimuthal coordinates and do not require any transformation to local coordinates. 5. CONCLUSIONS In conclusion, the state of progress of the MIA interferometer is promising. Acceptable preliminary results were obtained in each stage. The challenge now is to meet the objectives set for this first stage. The second stage will be to expand the number of an- tennas of the interferometer. The scientific potential that MIA will offer is promising, and the expecta- tions of the scientific community are high. REFERENCES Curtis, H. 2013, Orbital Mechanics for engineering stu- dents, Butterworth-Heinemann Dreher, J. W. 2000, Proceedings of SPIE, 4015, Ed. Butcher, H. R. & Erickson, W. C. Foley, A. R., Alberts, T., Armstrong, R.P. et al. 2016, MNRAS, 460, 1664 Gawronski, W. K. 2008, Modeling and control of anten- nas and telescopes, Springer Hickish, J., Abdurashidova, Z., Ali, Z., et al. 2016, Jour- nal of Astronomical Instrumentation, 5, 1641001 Kanagaraj, N., Sivashanmugam, P. & Paramasivam, S. 2008, Advances in Fuzzy Systems, 2008, 691808 Kocz, J., Ravi, V., Catha, M., et al. 2019, MNRAS, 489, 919 Martin, B. R. & Irvine, J. 1983, Research policy, 12, 61 Parsons, A., Backer, D., Siemion, A., et al. 2008, PASP, 120, 1207 Rohlfs, K. and Wilson, T. L. 2013, Tools of radio astron- omy, Springer Science and Business Media",
      "references": [
        "Prospects for low-frequency radio astronomy in S. A. (December 29, 2023)",
        "Orbital Mechanics for engineering students",
        "Modeling and control of antennas and telescopes",
        "Tools of radio astron- omy"
      ],
      "meta_data": {
        "arxiv_id": "2312.17066v1",
        "authors": [
          "J. M. Gonzalez",
          "H. Command",
          "G. Valdez"
        ],
        "published_date": "2023-12-28T15:26:22Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Presents end-to-end conceptual design and current implementation progress of the Multipurpose Interferometer Array Pathfinder (MIA) being developed at the Instituto Argentino de Radioastronomía: (i) a phased plan starting with a 3-antenna pathfinder toward a 16×5 m dish interferometer, (ii) a dual-polarization L-band (center ~1325–1500 MHz) receiver chain with calibration noise injection and optical fiber RF transport, (iii) a CASPER/SNAP2 FPGA-based digital back-end implementing channelization and real-time correlation for 3 antennas × 2 polarizations, and (iv) a layered antenna position-control system using induction motors and embedded controllers (BeagleBone + RP2040) with a fuzzy self-tuning PI controller and coordinate conversion integrations (Stellarium/Gpredict).",
        "methodology": "Antenna/front-end: prime-focus 5 m alt-az dishes (f/D=0.43) with dual-polarization Vivaldi feed (FR4 substrate, orthogonal interlocked pair) and microstrip-to-slotline multi-stage quarter-wave balun; calibration via directional coupler injecting known noise; LNA–bandpass (1–2 GHz)–LNA gain chain targeting ~50 K equivalent noise temperature and ~60 dB gain; thermal stabilization via Peltier on critical components; RF output over optical fiber. Back-end: SNAP2 boards (HMCAD1511 8-bit ADCs at 500 MHz) feeding Kintex-7 FPGA; synchronization via PPS and 10 MHz reference; polyphase filter bank (512-point, 4-tap Hamming FIR) + FFT to 256 channels; quantization; correlation via CASPER MAC blocks; packetization/FIFO; 10 GbE data output; control via Raspberry Pi + Python + JASPER/CASPER toolflow. Control: layered software architecture; embedded Linux management app; distributed RP2040 microcontrollers for axes + telemetry; I2C/queues/PIO usage; fuzzy-logic self-tuning PI; superloop telemetry at 2 s cadence; coordinate transforms for RA/Dec→local alt-az.",
        "experimental_setup": "Primarily an instrumentation/status paper with limited quantitative validation. Planned/ongoing deployment is a 3-antenna pathfinder (6 digitized polarization streams) exercising the full chain: RF front-end with bandpass filtering and LNA preceding ADC inputs; digitization at 500 MHz yielding 250 MHz effective bandwidth centered at 1325 MHz; FPGA channelization to 256 spectral channels and real-time cross-correlation; data sent over 10 GbE for downstream processing. Timing uses PPS for system synchronism and 10 MHz for ADC calibration. Software validation includes unit testing and code coverage (Ceedling) for controller code, and a coordinate transformation routine tested on an ATMEGA328P with reported sidereal-time error ~70 ms. No astronomical source results, datasets, or benchmark comparisons are reported.",
        "limitations": "Largely conceptual/engineering progress report with scarce measured performance: no end-to-end sensitivity, stability (Allan variance), bandpass/beam measurements, or on-sky interferometric fringes/imaging results. Vivaldi feed wideband phase-center variation and E/H-plane differences can cause defocus/astigmatism and frequency-dependent efficiency across 1–2.3 GHz, especially at high end relative to reflector focus. Use of 8-bit ADCs may limit dynamic range/RFI robustness; quantization strategy details and RFI mitigation are not discussed. Receiver enclosure is ambient with localized Peltier control—residual thermal drifts and long-term calibration stability are unspecified. Controller interface layer (web app/API) is still under development; telemetry cadence (2 s) may be coarse for some diagnostics. Pathfinder scale (3 antennas) does not address scalability issues (compute, networking, calibration) for the planned 16+ antenna array.",
        "future_research_directions": "Demonstrate full on-sky commissioning of the 3-antenna pathfinder: measure system temperature, gain/phase stability, bandpass, polarization purity, primary beam, and obtain fringes/closure quantities and first images. Improve feed/optics modeling and measurements to mitigate Vivaldi phase-center/astigmatism losses (e.g., refocusing strategy, lensing, alternative substrates, or feed redesign). Develop robust calibration pipelines (noise diode characterization, bandpass/complex gain calibration, polarization calibration) and RFI detection/mitigation suitable for L-band. Extend back-end for more antennas: scalable FX correlator architecture, higher-bit quantization where needed, improved synchronization/clock distribution, and data reduction/storage pipeline. Complete and harden the control/operations software (scheduler, API, safety/alarms), integrate higher-precision timing/ephemerides, and validate pointing/tracking accuracy under wind/load. Plan Phase-1 expansion to 16 dishes, including array configuration optimization, beamforming/imaging software, and science-case-driven observing modes.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Self Iterative Label Refinement via Robust Unlabeled Learning",
      "full_text": "Elastic Analysis of Augmented Curves and Constrained Surfaces Esfandiar Nava-Yazdani[0000−0003−4895−739X] Zuse Institute Berlin, Berlin, Germany navayazdani@zib.de https://www.zib.de/members/navayazdani Abstract. The square root velocity transformation is crucial for efficiently em- ploying the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks. Keywords: Elastic shape analysis · Tube · Manifold-valued · Ruled surface · Hurricane track. 1 Introduction Metric comparison of curves is a core task in a wide range of application areas such as morphology, image and shape analysis, computer vision, action recognition and signal processing. Thereby, a Riemannian structure is highly desirable, since it naturally provides powerful tools, beneficial for such applications. In the recent years, the use of Riemannian metrics for the study of sequential data, such as shapes of curves, trajectories given as longitudinal data or time series, has rapidly grown. In elastic analysis of curves, one considers deformations caused from both bending and stretching. A Riemannian metric, which quantifies the amount of those deformations is called elastic (cf. [18,19]). Therein, in contrast to landmark-based approaches (cf. [12,20,22]), one considers whole continuous curves instead of finite num- ber of curve-points. Consequently, the underlying spaces are infinite dimensional and computational cost becomes a significant issue. The square root velocity (SRV) frame- work provides a convenient and numerically efficient approach for analysing curves via elastic metrics and has been widely used in the recent years (cf. [14,11,4,3] and the comprehensive work [24]). In many applications the curves are naturally manifold-valued. For instance, Lie groups such as the Euclidean motion group, or more generally, symmetric spaces includ- ing the Grassmannian and the Hadamard-Cartan manifold of positive definite matrices are widely used in modelling of real world applications. Extensions of SRV framework from euclidean to general manifold-valued data can be found in [13,27,25,9,26]. arXiv:2402.04944v3  [math.DG]  28 Mar 20242 E. Nava-Yazdani Our contributions are the following. We expose for plane curves the behaviour of speed and curvature under the SRV transformation and geometric invariants. More- over, we apply the elastic approach to augmented curves, determining certain classes of surfaces, tubes, ruled surfaces and spherical strips, as well as hurricane tracks consid- ered with their intensities. We recall that with distance and geodesic at hand, signifi- cant ingredients of statistical analysis such as mean and principal geodesic components as well as approximation and modelling concepts such as splines can be computed. This paper is organized as follows. Section 2, presents the Riemannian setting and notations. Section 3 is devoted to applications. Therein, we consider time series, for which in addition to spatial data, auxiliary information give rise to augmented curves and some classes of surfaces generated by them. Thereby, we apply the elastic approach to both euclidean and spherical trajectories. Future prospects and concluding remarks are presented in 4. For the convenience of those readers primary interested in the applications, we mention that, advanced parts and details from differential geometry, presented in 2, can be skipped. Thereby, the essential point is the use of a framework (SRV) for computation of shortest paths on the spaces of curves and their shapes. 2 Riemannian Framework 2.1 Preliminaries For the background material on Riemannian geometry, we refer to [8] and [10]. Let (M, g) be a finite dimensional Riemannian manifold and M the Fr´ echet manifold of smooth immersed curves from D in M, where D denotes either the unit circle S1 or the unit interval I := [0, 1] for closed or open curves respectively. Moreover, we denote the group of orientation preserving diffeomorphisms on D by Diff +. The following reparametrization invariance is crucial for a Riemannian metric G on M: Gc◦φ(h ◦ φ, k◦ φ) = Gc(h, k), for any c ∈ M, h, k∈ TcM and φ ∈ Diff +. The above equivariance ensures that the induced distance function satisfies the following, which is often desirable in applica- tions: d(c0 ◦ φ, c1 ◦ φ) = d(c0, c1), for any two curves c0 and c1 in M. Similarly, denoting the isometry group of M by Isom(M) and the tangent map of F ∈ Isom(M) by T F, the invariance GF◦c(T F◦ h, TF◦ k) = Gc(h, k), ensures that d(F ◦ c0, F◦ c1) = d(c0, c1). With the above invariances, we can divide out the spaces Isom(M) and Diff +, and consider the natural induced distance dS on the quotient space S = M/(Diff + × Isom(M))Elastic Analysis of Augmented Curves and Constrained Surfaces 3 given by dS([c0], [c1]) = inf {d(c0, f◦ c1 ◦ φ) : φ ∈ Diff +, f∈ Isom(M)} = inf {d(f ◦ c0 ◦ φ, c1) : φ ∈ Diff +, f∈ Isom(M)}. In the context of shape analysis of curves, M and S are called the pre-shape and shape space, respectively. Note that the order of quotient operations does not matter, since the left action of Isom(M) and the right action of Diff + commute. M/Diff + is the space of unparametrized curves and its inherited distance reads inf {d(c0, c1 ◦ φ) : φ ∈ Diff +}. We remark that particular essential challenges are due to the fact that some basic concepts and results from finite dimensional differential geometry such as Hopf-Rinow theorem, do not carry over to the infinite dimensional case. Now, let ∇ be the Levi- Civita connection of M and denote the arc length parameter, speed and unit tangent of c by θ, ω and T respectively. Thus, we have ω = |˙c|, dθ = ωdt and T = ˙c ω , where dot stands for derivation with respect to the parameter t. Due to a remarkable result in [16] the geodesic distance induced by the simplest natural choice, the L2-metric GL2 c (h, k) = Z D gc(h, k) dθ, always vanishes. Consequently, some stronger Sobolev metrics have been considered in several works including [17,7,5]. They are given by Gc(h, k) = nX i=0 Z D aigc(∇i T h, ∇i T k) dθ, with a1 non-vanishing and all ai non-negative, distinguish the curves. We consider first order metrics with constant coefficients. We remark that the coefficients ai can be chosen such that the metric is scale invariant, which is a desired property for some applications in shape analysis. A family of certain weighted Sobolev-type metrics, the so-called elastic metrics, based on a decomposition of derivatives of the vector fields into normal and tangent components, has been introduced in [18,19]: Ga,b c (h, k) = Z D agc((∇T h)⊤, (∇T k)⊤) + bgc((∇T h)⊥, (∇T k)⊥) dθ, with 4b ≥ a >0. In this work, we use the square root velocity (SRV) framework, which allows for a convenient and computationally efficient elastic approach. The main tool in this framework is the square root velocity transformation, which for euclidean M reads q : c 7→ ˙cp |˙c| .4 E. Nava-Yazdani It isometrically maps curves modulo translations, with the metric G1,1/4 to M with the flat L2-metric given by G0(v, w) = Z D g(v(t), w(t))dt. This metric is frequently called (cf. [15,2,6]) flat, to emphasize its footpoint indepen- dence. Note that the elastic metric G1,1 corresponds to the first order Sobolev metric with a0 = 0 and a1 = 1. We remark, that for plane curves, the work [23] has extended the SRV transformation to general parameters a, b >0. For further reading on the SRV framework and applications in shape analysis, we refer to [14], [11] (numerical aspects), the survey [6] and particularly, the comprehensive work [24]. 2.2 Plane Curves A natural question that arises is, how essential geometric characteristics of a curve behave under the SRV transformation. In the following, we provide an answer for speed and curvature in the case of plane curves. Let M = R2, ˜c := q(c) and denote the curvature of c by κ. Note that ˜c does not need to be an immersion. Proposition 1. Denoting the speed of˜c by ˜ω, we have ˜ω = r ˙ω2 4ω + ω3κ2. (1) Moreover, ˜c is an immersion if and only ifκ and ˙ω have no common zeros. In this case, ˜κ˜ω = κω + ˙φ, (2) where ˜κ denotes the curvature of˜c and φ := arctan \u00122ω2κ ˙ω \u0013 . Proof. Let N denote the unit normal of c. With the shorthand notations α := √ω and β := α3κ, a straightforward application of the Frenet equations ˙T = ωκN and ˙N = −ωκT , yields ˙˜c = ˙αT + βN, ¨˜c = (¨α − β2 α )T + ( ˙β + ˙αβ α )N. Thus, we have ˜ω = p ˙α2 + β2, immediately implying (1). Obviously, zeros of ˜ω are common zeros of κ and ˙ω. Thus, ˜c is an immersion if and only if κ and ˙ω have no common zeros. In this case, ˜ κ and φ = arctan (β/ ˙α) = arctan \u0010 2ω2κ ˙ω \u0011 are well-defined and ˜κ˜ω3 = ˜ω2β/α + ˙α ˙β − ¨αβ, which immediately implies the curvature formula (2).Elastic Analysis of Augmented Curves and Constrained Surfaces 5 Next, we apply the proposition to study some geometric quantities, which are invariant under the SRV transformation. For closed curves, integrating the curvature formula above over D = S1 (note that in this case, ˜ω >0 almost everywhere), we see that the SRV transformation preserves the total curvature and particularly the turning number. Moreover, κω is preserved if and only if κ = a d dt \u00001 ω \u0001 with a constant a. Clearly, with κ and ω at hand, utilizing Frenet equations, we can compute c up to rigid motions. The following explicit solution is an immediate application of the above proposition. In light of the above proposition, immersed curves, which are mapped to straight lines, can easily be determined as follows. Example 1. Let a, b, Abe constants withab, A >0, ω(t) = A/ sin2(at+b) and κ = a/ω. A straightforward computation, utilizing the curvature formula (2), implies ˜κ = 0. 2.3 Curves in Homogeneous Spaces For the background material on Lie groups and homogeneous spaces, we refer to [10]. The works [13,27] provide extensions of the SRV framework for euclidean curves to the case of general manifolds. The former has high computational cost, while the latter, transported SRV, depends on a reference point and also suffers from distortion or bias caused by holonomy effects. We use the natural extension to homogeneous spaces exposed in [26,9]. For reader’s convenience, we sketch the core ingredients of the approach and refer to the mentioned works for details and some applications. Let M be a homogeneous space, i.e., M = H/K, where K is a closed Lie subgroup of a Lie Group H. Let ∥·∥ denote the induced norm by a left invariant metric on H, L the tangent map of the left translation, and Imm(D, H) the space of immersed curves from D to H. The SRV transformation is given by Q(α) = (α(0), q(α)), where q(α) = Lα−1 ˙αp ∥ ˙α∥ Here, α−1(t) denotes the inverse element of α(t) in H and H the Lie algebra of H. The map Q is a bijection from Imm(D, H) onto H × L2(D, H). Now, M can be equipped with the Riemannian metric given by the pullback of the product metric of H × L2(D, H) using the map Q and horizontal lifting. Let c1 and c2 be immersed curves in M with horizontal lifts α1 and α2 respectively. The induced distance on M reads d(c1, c2) = inf \u001aq d2 H(α1(0), α2(0)x) + ∥q(α1) − Adx−1 (q(α2)∥2 L2 : x ∈ K \u001b . 3 Applications Frequently, besides spatiotemporal data, represented by a curve γ in a manifold M, there are additional or auxiliary information associated with the curve, thus with the same time-correspondence. These can jointly with γ be comprised and represented as a6 E. Nava-Yazdani so-called augmented curve ˜γ in a higher dimensional manifold ˜M. In some applications, the curve ˜γ uniquely determines a submanifold N of M via a natural construction. An important example is provided, when ˜M is a submanifold of the tangent bundle of M, where the auxiliary information is represented as a vector field along γ and the con- struction is given by the Riemannian exponential map. Significant special cases occur, when M is R3 or the unit two-sphere S2 and N a surface. In the next two subsections, we consider certain classes of surfaces in R3, which often arise in applications and are determined by augmented curves in R4. In the last two subsections, we consider certain spherical regions as well as hurricane tracks together with their intensities. In both cases, we utilize the Riemannian distance from subsection 2.3 to S2 × R, which is a homogeneous space (recall that S2 can be identified with SO(3)/SO(2)). For our example applications, we present geodesic paths representing deformations, minimizing the elastic energy within the SRV framework. We remark, that in a Rie- mannian setting, distance and geodesics are essential Building blocks for many major issues in the morphology and shape analysis, such as computation of mean and test statistics as well as principal component or geodesic analysis. Moreover, besides sta- tistical analysis, also some methods for clustering and classification use Riemannian metrics and geodesics. For the code implementing our approach, which particularly includes Riemannian optimization for the computation of geodesic paths, we utilized our publicly available python package https://github.com/morphomatics, introduced in [1]. 3.1 Tubes A tube or canal surface c is a one-parameter family of circles, whose centers constitute a regular curve γ such that the circles are perpendicular to γ. More precisely, denoting the radii of the circles by r, c(s, .) = γ + r(N cos s + B sin s), 0 ≤ s ≤ 2π, where N and B are the normal and binormal of the curve γ = γ(t), t∈ D, resp. Due to the unique correspondence of c to (γ, r), comparison of tubes reduces to comparison of curves in R4. Figure 1 shows some examples of shortest paths of tubes. Real world applications include a variety of fields such as examination of vein, pipes, capsules and plant roots. Clearly, tubes include surfaces of revolution. 3.2 Ruled Surfaces A ruled surface is formed by moving a straight line segment (possibly with varying length) along a base curve. More precisely, let γ be a curve in R3 and v a unit vector field along γ. Then c(s, .) = γ + sv, s∈ I, parametrizes a ruled surface generated by ( γ, v). Figure 2 depicts an example, where each surface consists of straight line segments connecting the blue (for better visibility) curves γ and γ + v. The class of ruled surfaces includes many prominent surfacesElastic Analysis of Augmented Curves and Constrained Surfaces 7 Fig. 1.Two shortest paths of tubes such as cone, cylinder, helicoid (a minimal surface) and M¨ obius strip. They arise in manufacturing (construction by bending a flat sheet), cartography, architecture and biochemistry (secondary and tertiary structure of protein molecules). Fig. 2.Shortest path of ruled surfaces 3.3 Spherical Strips Let exp denote the exponential map of the unit two-sphere S2. We recall that for any non-zero tangent vector to S2 at a point x: expx(v) = cos(|v|)x + sin(|v|) v |v|8 E. Nava-Yazdani and expx(0) = x. Now, let γ be a curve in S2 with binormal B (cross product of γ and its unit tangent), and r a scalar function along γ. Then, the map c given by c(s, .) := expγ s(rB), s∈ I, parametrizes a spherical strip with bandwidth r. Figure 3 depicts an example of the shortest path between two spherical curves comprised with their bandwidth functions visualised as strips. Fig. 3.Shortest path of spherical strips 3.4 Hurricane Tracks Hurricanes belong to the most extreme natural phenomena and can cause major im- pacts regarding environment, economy, etc. Intensity of a hurricane is determined by the maximum sustained wind (maxwind), monotonically classifying the storms into categories (due to Saffir–Simpson wind scale; for instance, maxwind ≥ 137 knots cor- responds to category 5). Due to their major impacts on economy, human life and envi- ronment, as well as extreme variability and complexity, hurricanes have been studies in a large number of works. For our example, we used the HURDAT 2 database provided by the U.S. National Oceanic and Atmospheric Administration publicly available on https://www.nhc.noaa.gov/data/, supplying latitude, longitude, and maxwind on a 6 hours base of Atlantic hurricanes. Fig. 4.2010 Atlantic hurricane tracks (left) and the shortest path between two of them (right) with color-coded maximum sustained wind (in knots)Elastic Analysis of Augmented Curves and Constrained Surfaces 9 We represent the tracks as discrete trajectories in S2. For further details and com- parison with other approaches, we refer to [24,25] and the recent work [21]. The latter, also provides statistical analysis and a classification of hurricane tracks in terms of their intensities. Fig. 4 illustrates this data set with a visualization of the 2010 hurricane tracks and a shortest path, where the intensities, considered as auxiliary information, are color-marked. 4 Conclusion In this paper, we analysed the behaviour of speed and curvature under the square root velocity framework for elastic approach to plane curves. Moreover, we applied an extension of this framework to homogeneous Spaces, to metrically compare augmented curves and special surfaces, generated by those curves, using a natural construction via the Riemannian exponential map. Our approach, allows for computationally efficient determination of geodesic paths in the shape spaces of the respective classes of surfaces. Our example applications include tubes, ruled surfaces, spherical strips and hurricane tracks. Future work includes further real world applications, particularly concerning statistical analysis of longitudinal data such as comparison of group wise trends within a hierarchical model as well as classification and prediction. Acknowledgements This work was supported through the German Research Foun- dation (DFG) via individual funding (project ID 499571814). References 1. Ambellan, F., Hanik, M., von Tycowicz, C.: Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces (2021). https://doi.org/10.12752/8544, https:// morphomatics.github.io/ 2. Bauer, M., Bruveris, M., Marsland, S., Michor, P.: Constructing reparametrization in- variant metrics on spaces of plane curves. arXiv: Differential Geometry (2012), https: //arxiv.org/pdf/1207.5965.pdf 3. Bauer, M., Bruveris, M., Charon, N., Møller-Andersen, J.: A relaxed approach for curve matching with elastic metrics. ESAIM: Control, Optimisation and Calculus of Variations 25 (03 2018). https://doi.org/10.1051/cocv/2018053 4. Bauer, M., Bruveris, M., Harms, Philipp Michor, P.W.: Soliton solutions for the elastic metric on spaces of curves. Discrete & Continuous Dynamical Systems - A 38, 1161–1185 (2018). https://doi.org/10.3934/dcds.2018049 5. Bauer, M., Bruveris, M., Michor, P.W.: Overview of the geometries of shape spaces and diffeomorphism groups. Journal of Mathematical Imaging and Vision 50(1-2), 60–97 (2014) 6. Bauer, M., Charon, N., Klassen, E., Brigant, A.L.: Intrinsic riemannian metrics on spaces of curves: theory and computation. arXiv preprint (2020), https://arxiv.org/abs/ 2003.05590 7. Bauer, M., Harms, P., Michor, P.W., et al.: Sobolev metrics on the manifold of all rie- mannian metrics. Journal of Differential Geometry 94(2), 187–208 (2013)10 E. Nava-Yazdani 8. do Carmo, M.P.: Riemannian Geometry. Mathematics: Theory and Applications, Birkh¨ auser Boston, Cambridge, MA, USA, 2 edn. (1992) 9. Celledoni, E., Eidnes, S., Schmeding, A.: Shape analysis on homogeneous spaces: a gener- alised srvt framework. In: Computation and Combinatorics in Dynamics, Stochastics and Control: The Abel Symposium, Rosendal, Norway, August 2016. pp. 187–220. Springer (2018) 10. Gallot, S., Hullin, D., Lafontaine, J.: Riemannian Geometry. Universitext, Springer, Berlin, 3 edn. (2004) 11. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.A.: Riemannian optimization for registration of curves in elastic shape analysis. Journal of Mathematical Imaging and Vision 54(3), 320–343 (2016) 12. Kendall, D., Barden, D. Carne, T., Le, H.: Shape and Shape Theory. John Wiley & Sons (1999) 13. Le Brigant, A.: Computing distances and geodesics between manifold-valued curves in the srv framework. Journal of Geometric Mechanics 9(2) (2017) 14. Liu, W., Srivastava, A., Zhang, J.: Protein structure alignment using elastic shape anal- ysis. In: Proceedings of the First ACM International Conference on Bioinformatics and Computational Biology. pp. 62–70 (2010) 15. Michor, P., Mumford, D., Shah, J., Younes, L.: A metric on shape space with explicit geodesics. Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl. 19 (07 2007). https://doi.org/10.4171/RLM/506 16. Michor, P.W., Mumford, D.: Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms. Documenta Mathematica 10, 217–245 (2005) 17. Michor, P.W., Mumford, D.: An overview of the riemannian metrics on spaces of curves using the hamiltonian approach. Applied and Computational Harmonic Analysis 23(1), 74–113 (2007) 18. Mio, W., Srivastava, A., Joshi, S.H.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (2006) 19. Mio, W., Srivastava, A., Joshi, S.: On shape of plane elastic curves. International Journal of Computer Vision 73, 307–324 (07 2007). https://doi.org/10.1007/s11263-006-9968-0 20. Nava-XYazdani, E., Hege, H.C., Sullivan, T.J., von Tycowicz, C.: Geodesic analysis in kendall’s shape space with epidemiological applications. Journal of Mathematical Imaging and Vision pp. 1–11 (2020). https://doi.org/10.1007/s10851-020-00945-w 21. Nava-Yazdani, E., Ambellan, F., Hanik, M., von Tycowicz, C.: Sasaki metric for spline models of manifold-valued trajectories. Computer Aided Geometric Design 104, 102220 (2023). https://doi.org/10.1016/j.cagd.2023.102220 22. Nava-Yazdani, E., Hege, H.C., von Tycowicz, C.: A hierarchical geodesic model for longi- tudinal analysis on manif olds. Journal of Mathematical Imaging and Vision 64(4), 395 – 407 (2022). https://doi.org/10.1007/s10851-022-01079-x 23. Needham, T., Kurtek, S.: Simplifying transforms for general elastic metrics on the space of plane curves. SIAM Journal on Imaging Sciences 13(1), 445–473 (2020). https://doi.org/10.1137/19M1265132 24. Srivastava, A., Klassen, E.P.: Functional and shape data analysis, vol. 1. Springer (2016) 25. Su, Z., Klassen, E., Bauer, M.: The square root velocity framework for curves in a ho- mogeneous space. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) pp. 680–689 (2017) 26. Su, Z., Klassen, E., Bauer, M.: Comparing curves in homogeneous spaces. Differential Geometry and its Applications 60, 9–32 (2018) 27. Zhang, Z., Su, J., Klassen, E., Le, H., Srivastava, A.: Rate-invariant analysis of covariance trajectories. Journal of Mathematical Imaging and Vision 60, 1306–1323 (2018)",
      "references": [
        "Morphomatics: Geometric morphomet- rics in non-Euclidean shape spaces",
        "Constructing reparametrization in- variant metrics on spaces of plane curves.",
        "A relaxed approach for curve matching with elastic metrics.",
        "Soliton solutions for the elastic metric on spaces of curves.",
        "Overview of the geometries of shape spaces and diffeomorphism groups.",
        "Intrinsic riemannian metrics on spaces of curves: theory and computation.",
        "Sobolev metrics on the manifold of all rie- mannian metrics.",
        "Riemannian Geometry.",
        "Shape analysis on homogeneous spaces: a gener- alised srvt framework.",
        "Riemannian optimization for registration of curves in elastic shape analysis.",
        "Shape and Shape Theory.",
        "Computing distances and geodesics between manifold-valued curves in the srv framework.",
        "Protein structure alignment using elastic shape anal- ysis.",
        "A metric on shape space with explicit geodesics.",
        "Vanishing geodesic distance on spaces of submanifolds and diffeomorphisms.",
        "An overview of the riemannian metrics on spaces of curves using the hamiltonian approach.",
        "On shape of plane elastic curves.",
        "Geodesic analysis in kendall’s shape space with epidemiological applications.",
        "Sasaki metric for spline models of manifold-valued trajectories.",
        "A hierarchical geodesic model for longi- tudinal analysis on manif olds.",
        "Simplifying transforms for general elastic metrics on the space of plane curves.",
        "Functional and shape data analysis",
        "The square root velocity framework for curves in a ho- mogeneous space.",
        "Comparing curves in homogeneous spaces.",
        "Rate-invariant analysis of covariance trajectories."
      ],
      "meta_data": {
        "arxiv_id": "2402.04944v3",
        "authors": [
          "Esfandiar Nava-Yazdani"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies geometric behavior of planar curves under the Square Root Velocity (SRV) transform used in elastic shape analysis, deriving explicit formulas for how speed and curvature change and identifying SRV-invariant quantities (e.g., total curvature/turning number for closed curves). Extends and applies SRV-based elastic distances/geodesics to manifold-valued and augmented-curve settings, enabling intrinsic comparison of constrained surface classes (tubes/canal surfaces, ruled surfaces, spherical strips) and augmented trajectories (e.g., hurricane tracks with intensity). Demonstrates computationally efficient geodesic path computation in these shape spaces using an SRV framework on homogeneous spaces.",
        "methodology": "Uses first-order elastic Sobolev-type Riemannian metrics on spaces of immersed curves with reparameterization and isometry invariance; relies on SRV transform q(c)=c˙/sqrt(|c˙|) (Euclidean) as an isometry (mod translations) between elastic metric G_{1,1/4} on curves and flat L2 metric on SRV representations. For planar curves derives SRV-induced formulas via Frenet equations, giving transformed speed and curvature relations and conditions for immersion. For curves in homogeneous spaces M=H/K, employs generalized SRV transform using left translation to map to H×L2(D,𝔥), then defines distances via horizontal lifts and optimization over subgroup K, yielding an intrinsic metric on curve space/shape space. Models surfaces as constructions from augmented curves (e.g., (γ,r) in R^4 for tubes; (γ,v) for ruled surfaces; (γ,rB) on S^2 for spherical strips via exp map) so surface comparison reduces to curve comparison in an appropriate ambient/homogeneous space; computes geodesics via Riemannian optimization (Morphomatics).",
        "experimental_setup": "Primarily qualitative/illustrative experiments showing computed shortest paths (geodesic deformations) between examples in each class: tubes (canal surfaces) parameterized by centerline γ∈R^3 and radius r; ruled surfaces generated by base curve γ and unit direction field v; spherical strips on S^2 generated by spherical curve γ and bandwidth function r using the spherical exponential map. For hurricane tracks, uses NOAA HURDAT2 Atlantic dataset (lat/long every 6 hours) for 2010 tracks, represented as discrete trajectories on S^2 augmented with maximum sustained wind (maxwind) as an extra coordinate, thus curves in S^2×R; visualizes geodesic path between two storms with color-coded intensities. Implementation uses the open-source Python package Morphomatics, including Riemannian optimization to compute geodesic paths; no quantitative benchmarks or cross-validation reported.",
        "limitations": "Focuses on theoretical derivations and example visualizations; lacks quantitative evaluation (accuracy, runtime scaling, statistical significance) and comparisons against alternative elastic/registration methods. Planar SRV analysis assumes smooth immersed curves; SRV image may fail to be an immersion and results require conditions (e.g., curvature and speed derivative not sharing zeros). Homogeneous-space SRV approach relies on horizontal lifting and optimization over K; practical performance can depend on numerical optimization and choice of discretization; issues like holonomy/distortion are noted for other manifold SRV methods but not empirically characterized here. Surface models are restricted to specific constrained families (tubes, ruled surfaces, spherical strips) determined by augmented curves, limiting applicability to general surfaces; auxiliary signals (e.g., hurricane intensity) are treated as Euclidean components with implicit scaling choices.",
        "future_research_directions": "Extend to broader real-world datasets and perform full statistical shape analysis on the resulting shape spaces (means, principal geodesic analysis, hypothesis testing). Develop hierarchical/longitudinal models to compare group-wise trends of manifold-valued trajectories and augmented curves, and apply to classification and prediction tasks (e.g., hurricane track/intensity forecasting). Explore robustness to discretization/noise, parameter/weight selection (including scale invariance), and more efficient or globally convergent optimization for reparameterization/isometry alignment. Generalize constructions to additional constrained surface classes or more general submanifolds generated from tangent-bundle–valued augmentations, and study theoretical properties (existence/uniqueness of geodesics in infinite-dimensional settings, completeness) in these application-driven shape spaces.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "PRewrite: Prompt Rewriting with Reinforcement Learning",
      "full_text": "A Potential Aid in the Target Selection for the Comet Interceptor Mission E. Vigren1, A. I. Eriksson1, N. J. T. Edberg1, C. Snodgrass2  1 Swedish Institute of Space Physics, Uppsala, Sweden, E-mail: erik.vigren@irfu.se 2 Institute for Astronomy, University of Edinburgh, Blackford Hill, Edinburgh, UK  ABSTRACT  The upcoming Comet Interceptor mission involves a parking phase around the Sun-Earth L2 point before transferring to intercept the orbit of a long period comet, interstellar object or a back-up target in the form of a short-period comet. The target is not certain to be known before the launch in 2029. During the parking phase there may thus arise a scenario wherein a decision needs to be taken of whether to go for a particular comet or whether to discard that option in the hope that a better target will appear within a reasonable time frame later on. We present an expectation value-based formalism that could aid in the associated decision making provided that outlined requirements for its implementation exist.  Key words: Comets, solar system, probability theory  1. INTRODUCTION     The European Space Agency (ESA) Comet Interceptor (CI) mission (Snodgrass & Jones, 2019, see also https://www.cosmos.esa.int/web/comet-interceptor/home) is planned for launch in 2029. Its goal is to make a flyby of (and thereby study) a dynamically new long period comet (LPC) or an interstellar object (see e.g., Meech et al., 2017; Hoover et al., 2022) while it is passing through the inner solar system. The fact that the target comet possibly will be unknown by the time of launch, makes the mission unique. The mission is designed as such that the spacecraft will be held waiting around Lagrange point 2 of the Sun-Earth system until sent off to intercept the orbit of a target comet by then discovered as recently as within the last few years, either pre- or post launch. The maximum mission duration, meaning the maximum time between launch and target interception, is currently set to 6 years (Jones et al., 2023). The search for potential targets will be carried out by powerful ground based facilities like the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) currently under construction in Chile (Ivezić et al., 2019, see also https://www.lsst.org).     Certain requirements need to be fulfilled for a target comet to be considered suitable. For instance, the interception of the comet should not exceed the delta-v budget, must occur within a heliocentric distance range of 0.9-1.2 astronomical units and not occur on the opposite side of the sun as viewed from Earth. There is also, in principle, a limit for the duration of the parking phase and it may happen that a decision needs to be taken to rather go for a back-up target in the form of a short period comet (see e.g., Schwamb et al., 2020).    In this work we touch upon a question that may or may not be faced during the course of the mission; is it best to go for this particular target comet or to wait and hope that a better one appears later on? We start in Section 2 by going through requirements for our formalism to possibly be implemented. The four basic ones are:   i) A grading system making it possible to grade recently discovered potential target comets with a value on a continuous scale.   ii) An idea of with what frequency, 𝜈, potential target comets can be expected to appear.   iii) A probability density function telling by which probability a new potential target comet will have a value appearing in any given sub-interval of the scale.   iv) A set deadline for when to stop consider later on observed comets as potential targets.   In Section 3 we detail the delicate question that may appear during the mission and go through a formalism to tackle it provided that the aforementioned requirements are met. In Section 4 we show examples of guiding criteria for a few simple probability density functions. We present there also an example of how the formalism can be utilized while invoking relevant distribution functions and a specified grading criteria. These are examples only to demonstrate the formalism, as defining a grading system for the actual Comet Interceptor mission is outside the scope of this study.  Concluding remarks are provided in Section 5.  2. REQUIREMENTS FOR IMPLEMENTATION     For the tool presented in this work to be of potential use it is required that by the time of implementation there must exist criteria for grading a newly observed target comet on a continuous (0,1)-scale. In principle the formalism can be modified as to work for a scale with different limits, but here we stick to a scale between 0 and 1 as it is without loss of generality. Any LPC with an orbit which can safely be intercepted within the delta-v budget and fulfill other relevant requirements can be taken as appearing on the scale and hence assigned a value, 𝑐, from the interval (0,1). The best short-period back-up target should ideally be given a value on the same scale (even though it may be needed to apply completely different grading criteria). The orbital parameters of a recently observed LPC can be combined with sophisticated dynamical calculations as to find an optimal interception point with an associated flyby relative velocity (difference between the spacecraft velocity vector and the comet velocity vector) and flyby solar aspect angle (angle between the relative velocity vector and the comet-sun line). In Section 4 we consider as an example a grading system wherein targets associated with a lower magnitude of the flyby relative velocity and with a closer to 90° flyby solar aspect angle are prioritized.  We wish to stress, however, that the grading system used in Section 4 is merely one example out of many of how comets can be graded in a quantitative way. To highlight the complexity of the choice it should first of all be mentioned that depending on mission design a very low activity can render poor signal level in various in situ measurements while a high activity and particularly a high dust loss rate can endanger onboard instrumentation (Fulle et al., 2023; Marschall et al., 2022). A high activity target can be viewed as preferable; while the hazard due to increased dust flux can be mitigated by an increased flyby distance the possibility to compensate a low activity by a closer flyby is limited due to finite maximum rotation speed of the mirrors that follow the comet and also due to targeting errors.    Of further concern, the earth-sun-comet angle affects the communication and the possibility to make simultaneous remote sensing observations from ground or satellite-based telescopes (see e.g., Meech et al., 2005 and Snodgrass et al., 2017 for supporting ground based campaigns associated with the earlier cometary missions Deep Impact and Rosetta, respectively). Naturally, in addition to risk assessment and encounter geometry there is also a scientific priority involved in picking the ideal target; for instance, a dynamically new comet is prioritized over a returning one (Snodgrass & Jones, 2019).    When the grading system is set a next question to ask is the following:  at what frequency, 𝜈, can be expected the discovery of new target comets fulfilling the requirements to be considered feasible? The actual relevant number is the number of comets reachable by Comet Interceptor during its time of operations (i.e. with perihelion/ecliptic crossing during the lifetime of the mission and detected early enough for comet interceptor to catch it). Sánchez et al. (2021) estimate that some 2-3 LPCs per year should be expected to pass through the reachable heliocentric distance range and note that the fraction accessible for CI will depend on features in the spacecraft- and mission design. Current mission design assumptions give a conservative estimate of an 80% probability of there being at least one accessible target over the 6-year lifetime of the mission (Jones et al. 2023). Estimates of 𝜈 will presumably be refined over the years leading up to the launch thanks in part to improved statistics of LPCs. The latter may aid also in the construction of a relevant probability density function, 𝑓(𝑥), allowing to answer the question with what probability a newly discovered feasible target comet would appear within a certain sub-interval of the constructed grading scale.    The final parameter needed for implementation of the formalism presented in this work is a set deadline. We shall let 𝑡 denote the time remaining until the deadline, which we in turn treat as meaning that any long period comet discovered afterwards cannot be considered a suitable target. At this point (if reached) must be accepted to go for any of the already observed long period comets that are still deemed reachable (may be none) or to go for a short period back-up target. In practice, the relevant threshold is the flyby date, not the detection date. With a maximum mission duration of 6 years, a comet detected 4 years after launch cannot be reached if the transfer time is 3 years unless the mission duration is extended (the transfer time is expected to fall within the range of 6 months up to 4 years). Strictly respecting a maximum 6-year mission duration requires modification of the formalism presented in Section 3; in particular, the parameter 𝜈 should then not be treated as constant in time considering the fact that the time (after launch) of discovery of an LPC will affect whether or not its associated transfer time is acceptable. By considering the deadline as a threshold for the detection date (rather than the flyby date) we avoid the complexity of having to invoke a time dependent 𝜈.    3. DELICATE QUESTION AND GENERAL SOLUTION      Assume that a long period comet, 𝑋, is discovered, studied and tracked and assigned the value 𝑐 on the (0,1)-scale after it has been worked out how to best intercept its orbit. The calculated interception trajectory is set to start at the point when time 𝑡 remains to the deadline (according to the current baseline the go for decision needs to be given 6 months before departure from L2). Assume that no better long period comet is observed until a decision must be made (6 months prior to the time of transfer to encounter). Given 𝑐,\t𝑡, 𝜈 and a probability density function 𝑓(𝑥) (for which 𝑓𝑥=0 whenever 𝑥∉0,1) we shall provide formalism to tell whether it is correct from an expectation value point of view to go for comet 𝑋 or not. Before doing so a few points are worthwhile clarifying:  • We assume that 𝑋 at time 𝑡 is the still reachable comet with the highest grading. • We assume that 𝑡 is positive and that it represents the time to the deadline which we here treat as a discovery threshold date meaning that an LPC discovered afterwards cannot be considered a feasible target. As outlined in Section 2, respecting the nominal maximum mission duration of 6 years requires the utilization of a time dependent 𝜈. Such can be implemented in the formalism below but in-depth reasoning of the functional form (the time dependence) is beyond the scope of the present work. • We assume that if 𝑋 is not chosen as target by time 𝑡, it is no longer considered as an option.   The general guiding rule is described as follows: go for 𝑋 in case its value, 𝑐, exceeds the expectation value of the hypothetical target comet one would end up with if one were to start with no comet at 𝑡 and proceed with analogous strategy until the deadline for considering new observations is reached.     We can turn the problem into discrete form by dividing the available time 𝑡 into 𝑁 bins of equal duration and make use of backward induction (see e.g., Hill 2009). We shall consider a division into bins of so short duration that the probability for detection of multiple candidate targets within the same time bin becomes negligible. We let the time bin in connection with the deadline run between the discrete time points 𝑡0 and 𝑡1, where 𝑡1=0 marks the point when the deadline is reached.  The expectation value for a hypothetical comet appearing in this interval is given by  𝑒0=345𝑥𝑓𝑥𝑑𝑥01       (1)  where 𝑡/𝑁 is the duration of the time bin, 𝜈𝑡/𝑁 is the probability that a comet appears during this time interval, and the integral represents the expectation value for such a comet since 𝑓𝑥 is a probability density function over the interval 0≤𝑥≤1. By the strategy a comet is to be discarded at 𝑡0 if its given value does not exceed 𝑒0. When at 𝑡9 (further from the deadline than 𝑡0) a comet is to be selected only if its value exceeds  𝑒9=3451−𝑓𝑥𝑑𝑥;<1 =>=?=<@<>=?=<@< +1−345+345𝑓𝑥𝑑𝑥;<1 𝑒0 (2)  This requires explanation. The first term contains three factors, namely i) the probability that a comet appears during the time interval 𝑡9 to 𝑡0, ii) the probability that the comet has a value exceeding 𝑒0, which is required for selection, and iii) the expectation value of such a comet. The second term covers the two possible strategically correct ways for proceeding to 𝑡0 from which point the expectation value is 𝑒0 (last factor). The first possible way involves the scenario that no comet appears during the time interval 𝑡9 to 𝑡0 (probability given by the bracketed expression) and the second way is that a comet appears whose value does not exceed 𝑒0. By similar reasoning follows that in general for 𝑘≥2  𝑒E=3451−𝑓𝑥𝑑𝑥;FG<1 =>=?=<@FG<>=?=<@FG< +1−345+345 𝑓𝑥𝑑𝑥;FG<1 𝑒EH0 (3)  In the limit of large 𝑁 the recursively calculated 𝑒5 approaches the value to which 𝑐 should be compared. Only if 𝑐≥𝑒5 is the selection of the target justified from an expectation value point of view.  4. EXAMPLE RESULTS AND DISCUSSION  4.1. Guidelines for some simple probability density functions     Figure 1a shows four different probability density functions and Fig. 1b shows the corresponding guiding graphs of 𝑐 value to settle for as a function of the dimensionless quantity 𝜈𝑡. The probability density functions in question are 𝑓I𝑥=1 (black), 𝑓J𝑥=2𝑥 (blue),  𝑓K𝑥=2−2𝑥 (red) and 𝑓L𝑥=6(𝑥−𝑥9) (magenta). The data points (diamonds)  in Fig. 1b were generated through the numerical recursive approach with use of 𝑁=10N and the 𝑐 values to settle for are seen to be steadily decreasing with decreasing value of 𝜈𝑡, which is as expected. Curiously, in the case of a uniform probability distribution, as in 𝑓I𝑥, the criterion for the 𝑐 value to settle for reduces to 𝑐≥𝜈𝑡/𝜈𝑡+2. This can be realized by first noticing that Eq. (3) in the case of a uniform probability distribution after some algebra reduces to the expression   𝑒E−𝑒EH0/𝜈𝑡𝑁=1/21−𝑒EH09 . Then, if introducing 𝑧=𝜈𝑡, 𝑑𝑧=𝜈𝑡𝑁, 𝑒EH0=𝑔(𝑧) and 𝑒E=𝑔(𝑧+𝑑𝑧) is seen that as 𝑁→∞  the relation can be liken with the separable differential equation 𝑔S𝑧=1/21−𝑔(𝑧)9,  which when combined with the boundary condition 𝑔0=0, has the solution 𝑔𝑧=𝑧/(𝑧+2).  That a seemingly tedious problem is associated with such a simple solution is not uncommon in the theory of optimal stopping (c.f., Hill 2009). For the other considered probability density functions in Fig. 1a, we have not yet been able to establish simple guiding relations. This is not a prioritized endeavor, as it is hard to imagine how a constructed grading criteria in combination with sophisticated treatment of LPC statistics is to render probability density functions as trivial as any of the four considered in Fig. 1a. Similar reasoning justifies why we limit 𝜈𝑡 to values ≤14 in Fig. 1b.   Figure 1: (a) Four examples of probability density functions 𝑓I𝑥=1 (in black), 𝑓J𝑥=2𝑥 (blue),  𝑓U𝑥=2−2𝑥 (red) and 𝑓L𝑥=6(𝑥−𝑥9) (magenta) and (b) associated graphs showing what minimum value, 𝑐, to settle for as a function of the product 𝜈𝑡.  4.2. An example invoking available LPC statistics and specified grading criteria     It is instructive to also study a more involved example. We shall in the following pay respect to relevant LPC statistics available in Jones et al. (2023) as well as in the Definition Study Report of the  Comet Interceptor Mission (see https://www.cosmos.esa.int/web/comet-interceptor/home) and consider a grading system wherein targets are graded based both on the projected encounter relative velocity and the flyby solar aspect angle. We use in particular presented histograms over the modulus of the encounter relative velocity, 𝑢, and the flyby solar aspect angle, 𝜉, for simulated encounters obtained from the population of LPCs. We have digitized these and reproduce by dots (at bin centers) in Figs. 2a and 2b scaled down values (bin heights) over certain restricted intervals. The restrictions are made as to limit potentially considered target comets to ones with 𝑢≤70 km s-1 and with 45°≤𝜉≤135° (the restrictions, justified below, are estimated to combined remove ~15% of the population subject to no restrictions on 𝑢 or 𝜉). Shown in Figs. 2a and 2b are also fits to high order polynomial functions adjusted as to represent probability density functions over the given intervals. The associated cumulative distribution functions are shown in Figs. 2c and 2d, respectively.    Figure 2: The dots in panels (a) and (b) represent scaled down values of bin heights presented in histograms over encounter relative velocity and flyby solar aspect angle in Jones et al. (2023) and in the Definition Study Report of the Comet Interceptor Mission.  The solid lines are polynomial fits modified to represent probability density functions over the given intervals. Panels (c) and (d) show the associated cumulative distribution functions.      Turning to the subject of grading criteria it is noted that a low value of the modulus of the encounter velocity, 𝑢, is desirable not only because it would mean a longer duration of measurements. A high flyby velocity means also a greater risk for severe instrument- or spacecraft damage upon strike by cometary dust particles (e.g., Fink et al., 2021; Marschall et al., 2022). Also, a high spacecraft velocity can make impact of cometary coma molecules a strong driver of electron emission from the spacecraft and thereby complicate the analysis/interpretation of in situ plasma measurements (e.g., Grard et al., 1988; Johansson et al., 2022). Additionally, a lower velocity is valuable as it makes it easier for rotating mirrors to track the nucleus. While designed for the limiting 70 km/s case, a lower speed means an easier tracking and the possibility of a closer flyby for the same angular velocity of tracking mirrors. For the flyby solar aspect angle, 𝜉, a value near 90° brings optimal illumination conditions for imaging and a requirement of 45°≤𝜉≤135° is set as driven by sun avoidance for remote sensing, and also spacecraft power needs, as solar panels will be kept edge on to ram direction to minimize dust impacts during flyby. With these requirements in mind, let us, just as an example, assume that we are to grade comets via (𝑢\\]^+𝑢\\_`−𝑢)/𝑢\\_`2sin9𝜉−1 where we adapt 𝑢\\]^=10  km s-1 and 𝑢\\_`=70  km s-1 and where we restrict 10 km s-1 ≤𝑢≤70 km s-1 and 45°≤𝜉≤135°. Assuming independence, sampling 𝑢d and 𝜉d, while respecting the distribution functions of Figs. 2a and 2b, respectively, can be done by drawing two random numbers, 𝑟d< and 𝑟df, with uniform probability distribution from 0,1, and then interpolate with respect to the cumulative distribution functions of Figs. 2c and 2d, respectively. In Fig. 3a is plotted scaled down bin heights from a histogram based on 2×10N sample points of the form 𝑓d=(𝑢\\]^+𝑢\\_`−𝑢d)/𝑢\\_`2sin9𝜉d−1. A fit, 𝑓(𝑥), in form of a polynomial with properties of a probability density function is shown by the gray line while Fig. 3b shows the corresponding 𝑐 value to settle for as a function of 𝜈𝑡 as determined from the recursive approach outlined in Section 3. We refrain from presenting explicitly the involved polynomial fit functions as to reduce the risk of overselling the results from this exercise, as it is intended mainly as a demonstration of that the formalism can be applied also for non-trivial probability density functions. We remark also that  uncertainties in the distribution of cometary properties naturally will propagate into uncertainties in the value of 𝑓(𝑥), but deem a proper error analysis to be out of the scope of the present work.    Figure 3: The dots in panel (a) relate to a histogram of (𝑢\\]^+𝑢\\_`−𝑢d)/𝑢\\_`2sin9𝜉d−1 when sampled over  2×10N random points respecting the distributions in Fig. 2a and Fig. 2b. The gray solid line is a polynomial fit adjusted as to also represent a probability density function. In panel (b) is shown the associated 𝑐 value to settle for as a function 𝜈𝑡.  5: CONCLUDING REMARKS     We have presented formalism of potential use in the ultimate selection of target for the Comet Interceptor mission should the question emerge of whether it is better to go for an available target or risk waiting (and thereby “loosing it”) in hope of that a better target is observed within a reasonable time frame later on. We have outlined what is required for implementation of the formalism and illustrated its potential use through an example. While the presented formalism applies for a scenario where a deadline is set for the latest target discovery date it can in principle be modified to instead encompass a latest flyby date. Such a modification requires “only” the utilization of a time varying frequency parameter 𝜈 and is important to make when wanting to pay strict respect to the nominal maximum mission duration of 6 years (referring to the time between the launch and the point of interception). It is our hope that the delicate question treated in this work never actually will be faced during the course of the Comet Interceptor mission. Given that the observational surveys are expected to offer warning times (time from discovery to the interception opportunity) of typically more than three years (Sánchez et al., 2021), it is in fact improbable that it will. Still, the question may appear, and in that event it is good to have strategies based on probability theory at hand. To this end it may be further noted that the probability density functions considered in this work all yield a scenario where at low value of 𝜈𝑡 the 𝑐 value to settle for changes quickly. This highlights that the reliability in using our formalism is tightly connected to the reliability/accuracy in the estimate of the frequency of discovery of suitable comets, 𝜈. Work is already ongoing on constraining 𝜈 and such efforts will benefit greatly from the LSST once it is running. As a final remark is to be noted that the following of the strategy outlined in this work, even in the scenario of a well-constrained parameter set, is correct only in the average sense. In the isolated case it is not necessarily rewarding to turn down a semi-decent offer.   ACKNOWLEDGEMENT  The work of NJTE was supported by SNSA grant 2021-00047. We are thankful to Andreas Dieckmann for helping out in verifying the correctness of the presented exact condition for 𝑐 value to settle for in the case of a uniform probability density function. We are also thankful to reviewers of the original manuscript who raised important remarks, thereby contributing to improving our work.      REFERENCES  Fink, U., Harris, W., Doose, L., et al. 2021. Dust Outburst Dynamics and Hazard Assessment for Close Spacecraft–Comet Encounters. PSJ. 2, 154.  Fulle, M., Bockelée-Morvan, D., & La Forgia, F. 2023. A possible selection criterion of the target of Comet Interceptor mission.  Adv. Sp. R. 71, 4424.  Grard, R. J. L., McDonnell, J. A. M., Grün, E., & Gringauz, K. I. 1988. Secondary Electron Emission by Gas and Dust Impacts on Giotto, VEGA-1 and VEGA-2 in the Environment of Comet P/Halley. In: Exploration of Halley’s Comet. Springer Berlin Heidelberg (pp.785-788).     Hill, T. P. 2009. Knowing when to stop: how to gamble if you must – the mathematics of optimal stopping. American Scientist. 97, 126.  Hoover, D. J., Seligman, D. Z., & Payne, M. J.  2022. The Population of Interstellar Objects Detectable with the LSST and Accessible for In Situ Rendezvous with Various Mission Designs. PSJ. 3, 71.  Ivezić, Ž., et al. 2019. LSST: From Science Drivers to Reference Design and Anticipated Data Products. ApJ, 873, 111.  Johansson, F. L., Vigren, E., Waite, J. H., et al. 2022. Implications from secondary emission from neutral impact on Cassini plasma and dust measurements. MNRAS. 515, 2340.   Jones, G., et al. 2023. The Comet Interceptor Mission. SSR. Submitted.  Marschall, R., Zakharov, V., Tubiana, C., et al. 2022. Determining the dust environment of an unknown comet for a spacecraft flyby: The case of ESA’s Comet Interceptor mission. A&A, 666, A151.  Meech, K. J., Weryk, R., Micheli, M., et al. 2017. A brief visit from a red and extremely elongated interstellar asteroid. Nat. 552, 378.  Meech, K. J., et al., 2005. Deep Impact: Observations from a worldwide Earth-based campaign. Sci. 310, 265.  Sánchez, J. P., Morante, D., Hermosin, P., et al. 2021. ESA F-Class Comet Interceptor: Trajectory design to intercept a yet-to-be-discovered comet. Acta Astronautica, 188, 265.  Snodgrass, C., & Jones, G. H. 2019. The European Space Agency’s comet interceptor lies in wait. Nat. Comm. 10, 5418.  Snodgrass, C., et al. 2017. The 67P/Churyumov–Gerasimenko observation campaign in support of the Rosetta mission. Phil. Tran. Royal Soc. A., 375, 20160249.  Schwamb, M. E., Knight, M. M., Jones, G. H., et al. 2020. Potential Backup Targets for Comet Interceptor. Res. Notes AAS. 4, 21.",
      "references": [
        "Dust Outburst Dynamics and Hazard Assessment for Close Spacecraft–Comet Encounters.",
        "A possible selection criterion of the target of Comet Interceptor mission.",
        "Secondary Electron Emission by Gas and Dust Impacts on Giotto, VEGA-1 and VEGA-2 in the Environment of Comet P/Halley.",
        "Knowing when to stop: how to gamble if you must – the mathematics of optimal stopping.",
        "The Population of Interstellar Objects Detectable with the LSST and Accessible for In Situ Rendezvous with Various Mission Designs.",
        "LSST: From Science Drivers to Reference Design and Anticipated Data Products.",
        "Implications from secondary emission from neutral impact on Cassini plasma and dust measurements.",
        "The Comet Interceptor Mission.",
        "Determining the dust environment of an unknown comet for a spacecraft flyby: The case of ESA’s Comet Interceptor mission.",
        "A brief visit from a red and extremely elongated interstellar asteroid.",
        "Deep Impact: Observations from a worldwide Earth-based campaign.",
        "ESA F-Class Comet Interceptor: Trajectory design to intercept a yet-to-be-discovered comet.",
        "The European Space Agency’s comet interceptor lies in wait.",
        "The 67P/Churyumov–Gerasimenko observation campaign in support of the Rosetta mission.",
        "Potential Backup Targets for Comet Interceptor."
      ],
      "meta_data": {
        "arxiv_id": "2308.13098v1",
        "authors": [
          "Erik Vigren",
          "Anders I. Eriksson",
          "Niklas J. T. Edberg",
          "Colin Snodgrass"
        ],
        "published_date": "2023-08-24T21:57:00Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes an expectation-value/optimal-stopping decision framework to support ESA’s Comet Interceptor (CI) target-selection during the L2 parking phase when a feasible long-period comet (LPC) is available but a potentially better target may appear later. Formalizes a threshold rule: accept current comet X with grade c only if c exceeds the expected value of the best target obtainable by waiting until a specified deadline, given an estimated discovery rate ν and a grade distribution f(x). Demonstrates the framework on simple analytic f(x) cases (including a closed-form threshold for uniform f) and on a worked example using digitized CI/LPC encounter-geometry statistics (relative velocity u and solar aspect angle ξ) mapped to a continuous (0,1) grading scale.",
        "methodology": "Frames target choice as an optimal stopping problem under uncertainty. Assumes candidate comets arrive as a Poisson-like process with rate ν and have i.i.d. grades drawn from a known density f(x) on (0,1). Discretizes remaining time t to deadline into N small bins (probability of >1 arrival per bin negligible) and uses backward induction to compute recursively the acceptance threshold e_k (expected value when k bins remain). In the N→∞ limit, compares current grade c to the limiting threshold. Provides recursion equations (Eqs. 1–3); derives an analytic solution for uniform f(x): accept if c ≥ (νt)/(νt+2). For a realistic example, constructs f(x) by sampling u and ξ from fitted PDFs (digitized from published histograms), assumes independence, maps (u,ξ) to a scalar grade, and numerically computes threshold c*(νt) via the recursion.",
        "experimental_setup": "No new mission data; uses literature-based and simulated-statistics inputs. (1) Synthetic experiments: evaluates thresholds for four toy grade densities f(x) (uniform; increasing linear 2x; decreasing linear 2−2x; a peaked polynomial-like 6(x−x^2) per the paper) and computes c*(νt) numerically with large N (reported as N=10^4) to approximate the continuous-time limit, producing guidance curves c vs νt. (2) Realism-motivated example: digitizes published histograms of encounter relative velocity u and flyby solar aspect angle ξ from Jones et al. (2023) / CI Definition Study Report; restricts to feasible ranges (u≤70 km/s, ξ in [45°,135°], and further u in [10,70] km/s for grading), fits high-order polynomials to obtain PDFs and CDFs; Monte Carlo samples 2×10^4 (u,ξ) pairs assuming independence, computes scalar grades, fits a polynomial PDF f(x) over grades, then runs the recursive threshold computation to obtain c*(νt). Validation is illustrative/sanity-check (curve behavior vs νt), not predictive benchmarking against real selection outcomes.",
        "limitations": "Relies on strong assumptions: (i) existence of an agreed continuous grading system mapping heterogeneous science/engineering priorities to a single scalar; (ii) accurate estimates of discovery rate ν and stable stationarity over time; (iii) i.i.d. grade draws with known f(x) and (in the worked example) independence between u and ξ; (iv) simplified ‘deadline’ defined by latest discovery date rather than latest flyby date, avoiding time-dependent accessibility and transfer-time constraints—strict adherence to the 6-year mission duration would require ν(t) and more complex modeling. The method is optimal only in expectation (average sense); for a single realized mission it may recommend rejecting an adequate target and end up worse. Uncertainty propagation and error bars on f(x) and ν are not quantified; polynomial fits to digitized histograms may introduce artifacts. Practical constraints (operational readiness, political/ground-support windows, evolving orbit solutions, non-Poisson discovery process, target observability and characterization uncertainties) are not modeled.",
        "future_research_directions": "Develop and validate a mission-endorsed multi-criteria grading function incorporating science return, dust hazard, activity level, observability/communications geometry, and instrument constraints, possibly via multi-attribute utility or Bayesian decision analysis. Replace constant ν with a time-dependent accessibility model ν(t) tied to transfer time, discovery-to-intercept warning time distributions, and the hard 6-year launch-to-flyby constraint (deadline on flyby date). Use empirically derived LPC/interstellar-object discovery statistics from Rubin/LSST to infer ν and f(x) with uncertainties; propagate uncertainty to acceptance thresholds and provide risk-sensitive policies (e.g., maximize expected utility under constraints, CVaR/robust optimization). Model correlations among encounter parameters (u, ξ, perihelion distance, brightness/activity, dust environment) and include information gain/learning: update f(x) as surveys and orbit determinations improve. Extend to settings with multiple simultaneously reachable candidates, partial commitment/option value (keeping X as an option until departure), and operational costs of waiting or re-planning; explore continuous-time analytic solutions beyond the uniform case.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "TEMPERA: Test-Time Prompt Editing via Reinforcement Learning",
      "full_text": "arXiv:2402.13883v2 [hep-th] Isospin precession in non-Abelian Aharonov-Bohm scattering ∗ P.-M. Zhang1†, and P. A. Horv´ athy2‡ 1 School of Physics and Astronomy, Sun Yat-sen University, Zhuhai 519082, (China) 2 Institut Denis-Poisson CNRS/UMR 7013 - Universit´ e de Tours - Universit´ e d’Orl´ eans Parc de Grammont, 37200, Tours, (France) (Dated: May 16, 2025) Abstract The concept of pseudoclassical isospin is illustrated by the non-Abelian Aharonov-Bohm effect proposed by Wu and Yang in 1975. The spatial motion is free however the isospin precesses when the enclosed magnetic flux and the incoming particle’s isosopin are not parallel. The non-Abelian phase factor F of Wu and Yang acts on the isospin as an S-matrix. The scattering becomes side- independent when the enclosed flux is quantized, Φ N = NΦ0 with N an integer. The gauge group SU (2) is an internal symmetry and generates conserved charges only when the flux is quantized, which then splits into two series: for N = 2 k SU(2) acts trivially but for N = 1 + 2 k the implementation is twisted. The orbital and the internal angular momenta are separately conserved. The double rotational symmetry is broken to SO(2) × SO(2) when N odd. For unquantized flux there are no internal symmetries, the charge is not conserved and protons can be turned into neutrons. ∗ Dedicated to the memory of Professor Tai-Tsun Wu (1933-2024). † mailto:zhangpm5@mail.sysu.edu.cn ‡ mailto:horvathy@lmpt.univ-tours.fr 1 arXiv:2402.13883v3  [hep-th]  15 May 2025Contents I. Introduction: the vector potential in gauge theory 3 II. The Wu-Yang phase factor 6 III. (Pseudo)classical isospin 8 A. Internal charges and symmetries 9 B. Conserved quantities 10 IV. Isospin precession 13 V. Isospin scattering 16 VI. Internal charges 21 A. Implementability and internal symmetry 21 B. Conserved charges in the non-Abelian Aharonov-Bohm effect 24 C. Spin from isospin 28 VII. Conserved quantities by van Holten’s algorithm 29 VIII. Conclusion and outlook 32 Acknowledgments 35 References 36 2I. INTRODUCTION: THE VECTOR POTENTIAL IN GAUGE THEORY The vector potential has long been thought of as an auxiliary quantity until Aharonov and Bohm pointed out that it can produce an observable effect [1], shown schematically on FIG. 1 : electrically charged classical particles move freely outside a cylinder carrying a FIG. 1: Schema of the Abelian Aharonov-Bohm experiment [9]. magnetic flux, however the curl-free vector potential A = Aµdxµ allows for a quantum effect. The experimental confirmation of the Aharonov-Bohm prediction [2–8] led Wu and Yang to argue in their seminal paper [9] that : (a) The field strength Fµν under-describes (b) the phase H Aµdxµ over-describes electromagnetism, What provides a complete description that is neither too much nor too little is, FAB = exp \u0014 −i I Aµdxµ \u0015 . (I.1) [. . . ] Electromagnetism is the gauge-invariant manifestation of a nonintegrable phase factor. Here we took unit electric charge; FAB = e−i Φ where Φ = H Aµdxµ is the magnetic flux enclosed by a loop which goes around the cylinder once counter-clockwise 1. Dropping the z direction we choose the gauge where A = Aµdxµ = α dϑ .Shifting α by an integer, 1 In geometric language, such a loop generates the first homotopy group π1 ≃ Z of the space from which the cylinder has been removed [10–17]. All values of the F span the holonomy group [18, 19]. 3α → α + N, shifts the magnetic flux, Φ → Φ + 2Nπ, but yields identical Wu-Yang factors, ΦAB = −2πα − 2Nπ ⇒ FAB α = ei2πα . (I.2) Two vector potentials are therefore physically equivalent whenever their fluxes differ by an even multiple of π [1–3], Φ(1) − Φ(2) = 2N π, N= 0, ±1, . . .[5–8]. The non-Abelian generalisation of electromagnetism was proposed by Yang and Mills (YM) in 1954 [20]. Twenty years later, Wu and Yang [9] underlined the rˆ ole of the non- Abelian generalisation of the phase factor (I.1), FNA = exp[−Φ] = exp \u0002 − I Aµdxµ\u0003 . (I.3) This expression, which will hence be referred to as the Wu-Yang factor, looks as the Abelian expression (I.2) except for that Aµ = \u0000 Aa µ \u0001 is the Lie algebra valued Yang-Mills potential, and path-ordering is understood 2. To verify their proposal Wu and Yang suggested, by analogy with the electromagnetic case, an experimentum crucis. Quoting again from [9], sec. VII, p. 3855 : . . . many theorists . . . believe thatSU(2), gauge fields do exist. However, so far there is no experimental proof, since conservation of isotopic spin only suggests, and does not require, the existence of an isotopic spin gauge field. What kind of experiment would be a definitive test of the existence of an isotopic spin gauge field? A generalized Bohm-Aharonov experiment would be. One constructs the cylinder of material for which the total Iz spin is not zero, e.g., a cylinder made of heavy elements with a neutron excess. One spins the cylinder around its axis, . . . setting up a “magnetic“ flux inside the cylinder along the Iz “direction”(see Fig.1). . . . one imagines a rotating a cylinder which has an average ⟨⃗I⟩ which is [ . . . ] not in the Iz direction [with] a magnetic flux would then be set up which is in a “direction“ other than Iz. Scattering a beam of protons would then produce some neutrons as well as protons . This implies, of course, that there is electric charge transfer between the beam and the cylinder together with the gauge field around it. 2 Greek indices µ, νetc refer to space-time with i, jetc space indices. Latin indices a, betc refer to the gauge group. 4In this paper we argue that while there is no classical electromagnetic Aharonov- Bohm effect, however in the non-Abelian case a “shadow” of the quantum effect, namely isospin precession [9, 22, 35–37] can be recovered at the (pseudo-) classical level. The differ- ence with the Abelian case comes from that a particle with non-Abelian structure carries an additional, internal degree of freedom represented by an isospin vector ⃗Q = (Qa) [or equiv- alently, by a matrix in the Lie algebra, su(2), of the gauge group] which couples the particle to the YM field. The clue is that, unlike the electric charge, the isospin has non-trivial dynamics rather than being a mere constant [55–64, 66]. Then our first result says: Theorem I.1. The isospin of a (pseudo) classical particle with internal stucture in the non-Abelian Aharonov-Bohm field precesses around the non-Abelian flux in isospace. The original Wu-Yang proposal does not appear practical 3. Therefore the Wu-Yang proposal has long be view as a Gedankenexperiment and the study was focused at particles coupled to a pure SU(2) Yang-Mills field with no electromagnetic interaction 4. However, as emphasised by Yang and Mills already [20], “. . . when electro-magnetic interactions can be neglected, [. . . ] the orientation of the isotopic spin is of no physical significance. The differentiation between a neutron and a proton is then a purely arbitrary pro- cess. However, [. . . ] this arbitrariness is subject to the following limitation: [. . . ] once one chooses what to call a proton, what a neutron at one space-time point, one is then not free to make any choices at other space-time points.” However the Wu-Yang suggestion that incoming protons could be turned into neutrons raises deep questions, as it leads to conflict with electric charge conservation [9, 22, 27–29] 5. When should we call a particle a proton or a neutron ? Following Yang and Mills, we posit : Definition I.1. The definition of protons and neutrons should be gauge covariant. A proton must have charge 1 and a neutron must have charge 0. 3 Zeilinger et al. [21] did actually scatter a neutron beam on a rotating rod of 138U, setting up an upper limit 10−15 on the strength of the non-Abelian effect when compared to its electromagnetic counterpart. 4 Ignoring the electric charge is reminiscent of von Neumann’s “dry water”, though. 5 *The question of the referee of [22] about charge conservation which turned out to be related to the so-called “color problem” [23–28] led subsequently to [29]. 5Putting the Yang-Mills argument above into practice, we attach areference frame to each point of spacetime such that satisfies the critera above and then allows us to distinguish protons and neutrons. Our paper is organized as follows. After outlining the general theory, we focus our attention at the pseudoclassical description of the Non-Abelian Aharonov-Bohm Effect. Then in sec.VI we derive conserved charges, associated with internal symmetries. Re- markably, non-Abelian Aharonov-Bohm field admits two types of the latter: a trivial one and a “twisted” one, which arise when the enclosed flux is quantized in even resp. in odd integers, see eqn. (VI.9) [27–29]. Important conceptual and experimental progress has been made since those times, though [30–41]. The developments [42–51] which concern effective non-Abelian gauge fields change the assessment dramatically, opening the way to laboratory testing. II. THE WU-YANG PHASE FACTOR Hence we limit our investigations to G = SU(2) Yang-Mills fields. The Lie algebra G = su(2) is generated by the Pauli matrices, σ1 = \u0010 0 1 1 0 \u0011 , σ2 = \u0010 0 −i i 0 \u0011 , σ3 = \u0010 1 0 0 −1 \u0011 , (II.1) whose commutation relations are [ σa, σb] = 2iϵabcσc . The correspondance between 3-vectors ⃗Q and su(2) matrices Q is, ⃗Q ≡ \u0012 Q1 Q2 Q3 \u0013 ⇔ Q1 σ1 2i + Q2 σ2 2i + Q3 σ3 2i = 1 2i   Q3 Q1 − iQ2 Q1 + iQ2 −Q3   ≡ Q . (II.2) The vector and matrix forms will be used alternatively. The gauge group G is implemented according to, g · ⃗Q i.e., (g1/2)−1Q g1/2 , (II.3) where g1/2 is square-root of the SU(2) matrix g = ( gab) and g · ⃗Q means the action of SO(3) = SU(2) /Z2 matrix on the 3-vector ⃗Q. The Yang-Mills vector potential and field strength [20] are given by,A ≡ Aµdxµ and F = 1 2 Fµνdxµ∧dxν , respectively, whereAµ = (Aa µ) and Fµν = (Fa µν) are G = su(2) matrices. Aµ → g−1Aµ g + g−1∂µg , Fµν → g−1Fµν g under SU(2)-valued gauge transformations [20]. 6We focus henceforth our attention at the Non-Abelian Aharonov-Bohm Effect. A non- Abelian flux is confined to a thin cylinder outside of which Fµν = 0. Dropping the irrelevant z coordinate, we consider a flat SU(2) Yang-Mills potential A = Aµdxµ, DA = 0, on the punctured plane R2 \\ {0}. Such Yang-Mills fields are characterized by the non-Abelian generalization of the non-integrable phase factor (I.1), defined, for an arbitrary path γ, as FNA (γ) = exp \u0002 − Z γ Aµdxµ\u0003 ∈ SU(2) , (II.4) with path-ordering understood [9]. Was the gauge field strength non-zero, Fµν ̸= 0, the result would depend on the path. However in the non-Abelian Aharonov-Bohm case the non-Abelian version of Stokes’ theorem [52] implies that, owing to DA = 0, (II.4) is a homotopy invariant [29], i.e., it takes the same values when γ is continuously deformed while keeping its end points fixed. For a generating loop of π1 ≈ Z of the punctured plane which starts and ends at some point x0 (II.4) depends on the choice of x0 : choosing a different x0 results by conjugation by a fixed element of the gauge group. Keeping this fact in mind, we shall write simply F = exp[−Φ] = exp[− H Aµdxµ] as in (I.3). The suffix “NA” will be dropped henceforth and the non-Abelian character tacitly assumed. In the gauge we shall call diagonal the non-Abelian vector potential is in polar coordinates (r, ϑ), Ar = 0, A ϑ = a σ3 2i . (II.5) The generated non-Abelian flux 6 and Wu-Yang factor are, Φ = I Aidxi = −iπa σ3 , (II.6a) F = exp[ −Φ] =   eiπa 0 0 e−iπa   , (II.6b) respectively. It follows that gauge potentials which differ by an even integer, a and a + 2k, k = 0, ±1, . . . (II.7) i.e., whose non-Abelian fluxes are related as Φ and Φ − 2kπi σ3 , (II.8) 6 Our non-Abelian flux Φ is Wu-Yang’s I [9]. 7respectively, define identical Wu-Yang factors and are therefore physically equivalent. For an arc between ϑ0 and ϑ0 + ∆ϑ F(φ) = exp \u0002 − ∆ϑa σ3 2i \u0003 , (II.9) is a rotation by angle ∆ϑ a around the 3rd axis in isospace. For a full circle we recover the Wu-Yang factor, F(2π) = F in (II.6). The non-Abelian Wu-Yang factor (II.6b) can be presented in terms of its Abelian counterparts (I.2), FNA ≡ F =   (FAB (α/2) 0 0 ( FAB (−α/2))   . (II.10) Intuitively : “the number of components doubled but the exponents got halved, 2 α → a” when compared to (I.2). The Non-Abelian Phase Factor F transforms covariantly under a gauge transformation, F(x0) → g−1(x0) F(x0) g(x0) , and conversely : two curl-free vector potentials A(1) and A(2) can be shown to be gauge-equivalent if and only if their Wu-Yang factors are conjugate [29? ], F(1) = h−1F(2) h (II.11) for some fixed h ∈ SU(2). In conclusion, we confirm : Theorem II.1. The gauge-equivalent flat YM potentials are in 1-1 correspondance with the Wu-Yang factors up to conjugation by some fixed element of the gauge group. III. (PSEUDO)CLASSICAL ISOSPIN The additional concept we needed is that of classical isospin, represented by a 3-vector ⃗Q or alternatively, by an su(2) matrix, eqn. (II.2). Limiting ourselves at a non-relativistic study in flat space, the equations of motion of a classical isospin-carying particle put forward by Kerner [55], and by Wong [56], and further discussed in [57–68] are, ¨xµ = − Qa Fa µν ˙xν , (III.1a) DtQ ≡ ˙Q + [Aν ˙xν, Q] = 0 , (III.1b) where the “dot” means d/dt with t the non-relativistic time. The first of these is a gener- alized Lorentz-force equation, and the second says that the isospin is parallel transported 8along the space-time trajectory. The latter is gauge invariant whereas that of the isospin transforms covariantly when gauge transformations are implemented in the adjoint repre- sentation, Q → g−1Q g ⇒ DtQ → g−1DtQ g . (III.2) Moreover, deriving |Q|2 = QaQa and using the isospin equation (III.1b) implies that the length of the isospin is a constant of the motion , |Q|2 = |Q0|2 = const , (III.3) where Q0 is an arbitrary point in an adjoint orbit O = \b g−1Q0g |g ∈ G \t of the gauge group [62–69]. For G = SU(2) the orbit is a 2-sphere S2 of radius |Q0| in isospace. A. Internal charges and symmetries As emphasised by Yang and Mills and recalled in the Introduction, the distinction between protons and neutrons is based on their charges. The requirements are satisfied by choosing, in each point, a “direction field” in the Lie algebra G = su(2), represented by vector bΨ of unit length, |bΨ| = 1, which should behave under a gauge transformation as a (normalized) adjoint Higgs field , bΨ → g−1 bΨg . (III.4) Once a choice has been made, the electric charge of our particle will be defined as the projection of the isospin onto bΨ, q = −2 tr(Q · bΨ) = Qa bΨa . (III.5) Its gauge invariance follows from combing (III.4) with that of the isospin vector, (III.2). But is it dynamically conserved ? Before studying the Non-Abelian Aharonov-Bohm Effect, let’s first consider a different but similar problem, namely that of motion in the field of a Wu-Yang monopole [70], Aa i = ϵiak xk r2 . (III.6) This “hedgehog” Ansatz identifies the internal isospace, su(2), with the external 3-space. Then projecting the isospin onto the radial direction, bΨa = xa r . (III.7) 9The auxiliary direction field ˆr = xa r behaves as an adjoint Higgs field in the particular “hedgehog” gauge and then (III.5) allows us to recover the charge, q = Qa xa r , (III.8) whose conservation follows from Dµ bΨ = 0 . The charge (III.5) generalized to non-Abelian monopoles is conserved outside the monopole core, where bΨ is the direction of the Higgs field, which for large r becomes covari- antly constant [52, 71, 72]. More generally, when a covariantly constant “direction field” bΨ does exist then we posit, consistently with what we said in the introduction : Definition III.1. The particle is a proton if its internal charge (III.5) is 1, and it is a neutron if its charge is 0. The charge-based proton/neutron distinction makes us wonder: when do we get an ordi- narily, and not only covariantly, Dt( ·) = 0, conserved charge, ˙q = 0 ? (III.9) The parallel-transport equation (III.1b) implies at once that this is so whenbΨ is convariantly constant, Dµ bΨ = ∂µ bΨ + [Aµ, bΨ] = 0 . (III.10) However diatomic molecules [35–37] provide us with an example where no covariantly constant direction fields and thus no conserved charges. Our investigations in sec.VI then show that this is also what happens in the Non-Abelian Aharonov-Bohm Effect — with the remarkable exception when the non-Abelian flux is quantized as in (VI.9). B. Conserved quantities Conserved quantities are usually derived from space-time symmetries by Noether’s theo- rem [73, 74]. Below we argue that the charge (III.5) is generated by an internal symmetry [27, 28]. We first recall some general facts [66–68, 74–78]. A vectorfield X is a symmetry for a gauge field if the variation of the vector potential can be compensated by a gauge transformation, LXA = DW or equivalently F(X ,·) = Dψ where ψ = W − A(X) (III.11) 10for some Lie-algebra valued fields W or ψ, respectively [75–78]. Then the adapted form of the Noether theorem allows one to derive conservation laws [66, 74, 78]. As a first illustration, we consider the angular momentum in the field of a Wu-Yang monopole, (III.6), for which the general formulæ yield the conserved total angular momen- tum [37, 79, 80], ⃗J = ⃗ x× ⃗ π− qˆr (III.12) where q is the conserved charge (III.8). This expression is analogous to the one in the field of a Dirac monopole [36, 66–68, 74, 78–82]. Eliminating ⃗ πin favor of m ˙⃗ x= ⃗ p= ⃗ π+ ⃗A, (III.12) ⃗ x× ⃗A and qˆr combine into the isospin vector, and the angular momentum is separated into the sum of orbital and internal terms, ⃗J = ⃗L − ⃗Q , where ⃗L = r × ⃗ p , (III.13) highlighting the “spin from isospin” phenomenon [74, 78, 81–83]. Similar formulae will show up again in sec.VI C. Now we spell out the general theory for internal symmetries and charges in the non- Abelian Aharonov-Bohm context. We first consider a general compact and connected gauge group G before returning to G = SU(2). The symmetry condition (III.11) assumes that the (infinitesimal) transformation is im- plemented on the fields by the Lie derivative by the vector field X on the spacetime. But a gauge transformation acts on space-time trivially, X = 0; Then how does it act on fields ? And if it has been implemented, is it a symmetry ? The answer, which generalizes what we said in sec.III A for the electric charge, proceeds in three steps [27, 28] : 1. Question : Which elements of the gauge group can be implemented on the gauge field, i.e., for which the action of a g ∈ G on the gauge potential can be defined ? 2. Question : Let K be an implementable subgroup of the gauge group G and k ∈ K. Does it act as a symmetry ? 3. Question : What is the conserved quantity associated with a vector κ of the Lie algebra K of the internal symmetry group K ? Coming to details, 11• A subgroup K of the gauge group G is said implementable if there exists, in each point x of spacetime, an automorphism of K into of the gauge group G, Ω{·}(x) : K → G such that Ωk1k2 (x) = Ωk1 (x)Ωk2 (x) . (III.14) Such a mapping can be made to act on the gauge potential according to 7, k · Aµ(x) = Ωk(x)Aµ(Ωk(x))−1 − ∂µΩk(x)(Ωk(x))−1 . (III.15) If the underlying space has non-trivial topology, a topological obstruction against im- plementing certain elements of the gauge group can arise, and K will be a proper subgroup of G. This is what happens for non-Abelian monopoles [23–28]. For the Non-Abelian Aharonov-Bohm Effect no such obstruction does arise, though [27, 28], therefore the general theory will be skipped; the interested reader is advised to check the literature. However, a (sub)group K can have different inequivalent implementa- tions, – and it is what happens for the Non-Abelian Aharonov-Bohm Effect as it will be shown in sec.VI. Returning to the general theory, let us assume that K ⊂ G is an implementable subgroup. The infinitesimal action of a vector κ ∈ k, the Lie algebra of K, is then a G-valued Higgs-type field bΨκ(x) normalized as |bΨκ| = 1 which acts on the gauge potential by covariant derivative [27, 28, 74–78], bΨκ · Aµ = Dµ bΨκ . (III.16) Then the answers to the questions above are: • κ is an infinitesimal symmetry when ωκ is covariantly constant Dµ bΨκ = 0 . (III.17) • The Noether theorem applied to an internal symmetry then says that qκ = −2 tr(Q · bΨκ) = Qa bΨa κ (III.18) is ordinarily conserved, ˙qκ = 0 generalizing (III.5). 7 For comparison : a gauge transformation by k = k(x) would act instead as Aµ → k−1Aµ k + k−1∂µk . In bundle language, the gauge group acts from the right, while the implementation (III.15) acts from the left [28]. 12For aficionados of fiber bundles and differential geometry [18, 19, 27, 28], we first recall two definitions: (i) the centralizer of a subgroup K ⊂ G is defined as all g elements of G which commute with all k ∈ K, i.e., whose adjoint action leaves all k ∈ K invariant, ZG(K) = n g−1kg = k | for all k ∈ K o . (III.19) The centralizer of K = G itself is thus the center, Z(G), of G. (ii) Using fiber bundle language [18, 19], the horizontal lift, obtained by calculating the integral (I.3) of the YM potential along a loop, ends in the same fiber it started from, and can therefore be obtained by (right) action of a group element. The holonomy group consists of all possible values of the Wu-Yang factor (I.3). In the non-Abelian Aharonov-Bohm context, the holonomy group consists of integer powers of the Wu-Yang factor (I.3), calculated for an arbitrary loop which generates π1 ≈ Z of the punctured plane. Then we quote PROP. 4.3 of [28] : Theorem III.1. A subgroup K of the gauge group G is an internal symmetry group for the YM field A = Aµdxµ if and only if the centralizer of K in G, H = ZG(K), contains the holonomy group of A. The bundle then reduces to one with structure group H = ZG(K) obtained by solving the internal symmetry equation (III.17). The entire G is itself an internal symmetry if and only if the holonomy group belongs to the center, Z(G). In sec.VI we show that, in the non-Abelian Aharonov-Bohm context, the symmetry condition (III.17) may have several inequivalent solutions. IV. ISOSPIN PRECESSION As emphasised in the introduction, there is no classical Aharonov-Bohm effect in the Abelian theory : charged particles move freely in the curl-free magnetic potential. The effect is purely quantum mechanical and is related to the phase freedom [1–3, 5–7, 69]. What about the non-Abelian case ? This is the question to which this paper is devoted. The isospin couples to the non-Abelian field [55, 56] and the results obtained in an analo- gous but different context, namely for diatomic molecules [35, 36] hint at isospin precession, recovered also at the (pseudo-) classical level [37]. 13In this paper we find (pseudo-) classical isospin precession also in the non-Abelian Aharonov-Bohm context. Focusing our attention at the gauge group G = SU(2) we consider a non-Abelian flux confined to a thin cylinder upon which particles with isospin are scattered, as illustrated schematically in FIG.1 and, in more detail, in FIG.2. We shall need a first but important result. Unlike as for monopoles [74, 78–82], theorbital angular momentum alone, ⃗L = ⃗ x× ˙⃗ x , (IV.1) (for unit mass) is conserved, for all values of the non-Abelian flux , in the non-Abelian Aharonov-Bohm context. This is seen directly by using (III.1a) with F ≡ 0: ˙⃗L = ˙⃗ x× ˙⃗ x+ ⃗ x× ¨⃗ x= 0. The sign of ⃗L will be used to distinguishes the half-planes in the two sides of the flux, as illustrated schematically in FIG.2. Further aspects of the angular momentum will be studied in sec. VI C. FIG. 2: Scattering of a nucleon on a non-Abelian flux ⃗Φ, whose direction is perpendicular to the proton/neutron axis. The isopin is rotated by an angle ±∆ϑ a with the sign depending on the side the particles passes, i.e., the sign, L > 0 or L < 0, of the (3rd component) of the orbital angular momentum. The rotation of the isospin is generated by the Non-Abelian Phase Factor F(∆ϑ) in (II.9). The classical dynamics is given by the Kerner-Wong equations (III.1). Outside the cylin- 14der the Yang-Mills field is flat, F = DA = 0, therefore the generalized Lorentz force in (III.1a) vanishes and the space-time motion is free , just like in the Abelian case. As to the isospin, we first remark that, in the free case, a = 0 in the diagonal gauge (II.5), therefore the flux ⃗Φ vanishes and the isospin eqn. (III.1b) implies that, Q(t) = Q0 = const . (IV.2) However switching to another gauge (III.2) by an arbitrary gauge transformation g(xµ), Q → ˜Q(t) = g−1\u0000 xµ(t) \u0001 ˜Q0 g \u0000 xµ(t) \u0001 , (IV.3) the trajectory will look substantially different — but this is a mere gauge artifact. Do we find genuine, non-gauge-artifact classical motions ? When the non-Abelian flux Φ and the incoming isospin ⃗Qin are parallel in internal isospace (considered by Wu and Yang) the answer is negative by (III.1b). However in the second case proposed in [9], i.e., when ⃗Qin and the non-Abelian flux ⃗Φ = (Φ a) are not parallel in isospace, then genuine internal motion can be found, as we show now. To explain how this comes about, let us assume for simplicity that ⃗Qin is perpendicular to the flux in isospace, Q3 in = 0, as depicted in FIG. 2. First of all, we fix our terminology: consistently with the Yang-Mills argument [20] quoted in the introduction we shall call, in the diagonal gauge, the particle incoming from far on the right,    proton if Qp in = 1 2iσ1 = 1 2i \u0010 0 1 1 0 \u0011 neutron if Qn in = 1 2iσ2 = 1 2 \u0010 0 −1 1 0 \u0011 (IV.4) which both have unit length and are orthogonal to each other w.r.t. the scalar product (III.18), −2 tr(Q2 in) = 1 and tr(Qn in · Qp in) = 0 . (IV.5) This fixes the choice at one point – and, as Yang and Mills say [20], at all other points by gauge covariance. Now we study the motion of the isospin. Recall first that (III.3) implies that |Q| = const, therefore Q moves on a circle. Eliminating t in favor of the polar angle ϑ, ˙ϑ drops out from the parallel transport equation (III.1b), leaving us with dQ dϑ + \u0002 Aϑ, Q \u0003 = 0 . It follows that 15Q3 = const. We put Q3 = 0 for simplicity. Let us assume that the incoming particle is a proton, Qin = Qp in. First consider L >0. Then Qp +(φ) = cos(φa) σ1 2i − sin(φa) σ2 2i i.e., Qp +(φ) = 1 2i   0 e+iφa e−iφa 0   , (IV.6) where we shifted to φ = ϑ − π, 0 ≤ φ ≤ π. The initial position, far on the left, has φin ≈ 0. A similar formula works for L <0 with φ = π − ϑ which changes just a sign, Qp −(φ) = cos(φa) σ1 2i + sin(φa) σ2 2i , i.e., Qp −(φ) = 1 2i   0 e−iφa e+iφa 0   . (IV.7) The motion of the isospin can be viewed as a rotation of the isospin around the 3rd axis, ⃗Qp out =    R+φa ⃗Qp in for L ≥ 0 R−φa ⃗Qp in for L ≤ 0 , (IV.8) as suggested by Kerner [55], – but the rotation is in opposite directions as shown in FIGs.2-3. The intuitive picture of the opposite shifts suffered on the two sides is : sailing downwind resp. upwind shifts the phase by the relative phase difference. Notice however that for φ >0 the upper and lower values of the isospin in (IV.6) and (IV.7), respectively, do not match unless a is an integer , a = 2k or a = 1 + 2k . (IV.9) Similarly for an incoming neutron, Qn in = σ2 2i , we get, Qn ±(φ) = cos(φa) σ2 2i ∓sin(φa) σ1 2i i.e., Qn ±(φ) = 1 2   0 −e±iφa e∓iφa 0   . (IV.10) The formulae (IV.6)-(IV.7) and (IV.10) will be recovered in sec.IV, # (VII.11) using a different framework. V. ISOSPIN SCATTERING We are be particularly interested by the value of the isospin far on the right, at φout = π, shown in FIGs.2-3-4. In other words, we want to study the Non-Abelian Aharonov-Bohm 16FIG. 3: The “isospin clockwork” for non-Abelian flux Φa = −iπaσ3, ( a ̸= 0 ). The incoming proton (at L±) is aligned with ⃗ p= σ1/2i (at “6 o’clock”), Qp in= ⃗ p. For L > 0 the isospin rotates clockwise, but for L < 0 it rotates counter-clockwise. For odd integer flux ( a = 1, for example), drawn in red and blue, ⃗Qp out = −⃗Qp in (R± at “12 o’clock”) on both sides. However when a is not an integer e.g. for a = 1 2 (mod even integers), drawn in magenta and cyan, the isospin is lagged behind. ⃗Qp out and (“9 o’clock”) and ⃗Qp out (“3 o’clock”) are both orthogonal to ⃗Qp in and are oppositely oriented in the two L-sectors. Similar behavior could be observed for any a. Effect from the viewpoint of scattering theory [22]. In terms of the polar angle, ∆ ϑ = ±π , Qp out = 1 2i   0 e±iπa e∓iπa 0   , Q n out = 1 2   0 −e±iπa e∓iπa 0   , (V.1) with the sign depending on which side of the flux line had the particle passed, correlated in turn with the sign of the vertical component, L ≡ Lz, of the conserved orbital the angular momentum ⃗L = x × ˙x in (IV.1). The rotations in (V.1) [or more generally in (IV.6)-(IV.7)] on the two sides do not match for L → 0 in general, though. However in the non-trivial exceptional case a = 1 + 2k, for example, the outgoing result becomes side-independent, Qp out = − 1 2iσ1 = −Qp in (V.2) 17FIG. 4: The “isospin clockwork” for protons and neutrons for a = 1 + 2k where k is an integer. i.e., for flux Φ = −i(1 + 2k)πσ3. The incoming proton (in red and blue is aligned with ⃗ p, Qp in = σ1/2i (“6 o’clock”) as in FIG.3. The incoming neutron (in purple and green) is along ˆz (at “3 o’clock”), Qn in = σ2/2i on both sides of the flux. For L > 0 the isospin rotates clockwise for both Qp and Qn and for L < 0 they rotate counter-clockwise. Qp and Qn turn rigidly, remaining orthogonal to each other. ⃗Qp,n out = −⃗Qp,n in (“12 o’clock” and “9 o’clock”, respectively) on both sides, consistently with S = −I in (V.4). The centers of the “shifting clocks” are at φ = 0, π/2, π. consistently also with R±π = −I in (IV.8) and shown in FIG.4. More generally, the isospin is reversed in both sectors. The rˆ ole of theminus sign here will be explained in sec.III A. Eqns. (IV.6)-(IV.7) with ∆ θ = π allow us the express the result in terms of the Wu-Yang factor (II.6) in matrix resp. in vector form [22], Qp out = \u0000 F \u0001−1/2 Qp in \u0000 F \u00011/2 i.e., ⃗Qp out = F ⃗Qp in . (V.3) These formulae, consistent with (II.3), confirm that the isospin is rotated by F(φ) cf. (II.9), as shown in FIGs.3 and 4. They are gauge-covariant when the incoming direction also transforms covariantly, Qp out = F Qp in → (g−1Fg)(g−1Qin g) = g−1(F Qin)g = g−1Qout g . For both exceptional values a = 2k and a = 1 + 2k the phase difference accumulated on the two sides is ±2π, implying that the non-Abelian fluxes differ on by an even integer and 18thus yield identical Wu-Yang factors, F =    I when a = 2k −I when a = 1 + 2k . (V.4) Therefore ⃗Qout = ±⃗Qin on both half-planes, (V.2), as shown in FIGs. 3 and 5. The non-integrable phase factor for the half-loop (II.9) is the square root of the Wu-Yang factor (II.6), F(±π) = \u0000 F \u0001±1/2 =   e±iπ( 1 2 a+k) 0 0 e∓iπ( 1 2 a+k)   , (V.5) where k is an integer 8. This expression is explained as follows. The Wu-Yang factor is obtained by integrating along a generating loop of π1 put together from two half-loops, starting from ϑ = −π, going first in the L >0 half-plane to the turning point at ϑ = 0 and then completing the loop by returning to ϑ = π in the L < 0 half-plane. Each half-loop contributes equally. Thus F = F(+π)F−1(−π) = diag \u0000 eiπa, e−iπa\u0001 . Hence we focus our attention at the odd case. Identifying the external and inter- nal coordinates by mimicking monopole theory [52, 70–72], we get a sort of “Aharonov- Bohm hedgehog”. Then the trajectory of the isospin is that of a point on a rolling circle whose center moves on the spatial trajectory, — i.e., a cycloid segment, depicted in FIG.5. In detail, • For a proton Qp 1 = |⃗Q0| \u0000 φ − sin φ \u0001 , Q p 2 = −|⃗Q0|cos φ, 0 ≤ φ = ϑ − π ≤ π L > 0 (V.6a) Qp 1 = |⃗Q0| \u0000 φ + sinφ \u0001 , Q p 2 = −|⃗Q0|cos φ, 0 ≤ φ = π − ϑ ≤ π L < 0 (V.6b) • For a neutron Qn 1 = |⃗Q0| \u0000 φ + cosφ \u0001 , Q n 2 = −|⃗Q0|sin φ, 0 ≤ φ = ϑ − π ≤ π L > 0 (V.7a) Qn 1 = |⃗Q0| \u0000 φ + cosφ \u0001 , Q n 1 = |⃗Q0|sin φ, 0 ≤ φ = π − ϑ ≤ π L < 0 (V.7b) 8 F in (V.5) acts on the su(2) matrix Q in the adjoint representation. In vector terms, Q ⇝ ⃗Q, the arguments of the exponential are doubled, cf. (V.3). 19pL- pL+ nL- nL+ FIG. 5: Isospin motion for a = 1 for both protons (V.6) and neutrons (V.7), in both respective half-planes. The trajectories follow cycloid segments, which all satisfy ⃗Qout = −⃗Qin. S-matrix [22]. Eqn (V.3) allows us to consider a pseudo-classical S-matrix defined, separately, on the two half-planes distinguished by the sign of the orbital angular momentum L S± = F(±π) =   e±iπ( 1 2 a+k) 0 0 e∓iπ( 1 2 a+k)   (V.8) cf. (V.5) and (IV.8). The ± expressions match for a = 0 or a = 1 mod. even integers. In conclusion, Theorem V.1. When the incoming isospin ⃗Qin and the non-Abelian flux ⃗Φ are not parallel, the isospin vector ⃗Q precesses in opposite directions, depending on which side the particle goes and distinguished by the sign of the orbital angular momentum L. The Wu-Yang factor 20F (I.3) acts as “classical S-matrix” by rotating the isospin, Qout = S Qin = (F)−1/2Qin(F)1/2 (V.9) i.e., ⃗Qout = S ⃗Qin where S = R±πa, (V.10) illustrated in FIGs. 3 and 4. In the odd case i.e., for non-Abelian flux Φodd = (1 + 2k) Φ0, Φ0 = πσ3/i , (V.11) the S-matrix is minus the identity on both sides, cf. (V.4) and FIG.4. For non-integer values of a the S-matrix will be side-dependent. For a = 1/2 (shown in FIG.3 in magenta and in cyan), for example, Qp out(a = 1 2 ) =    −σ2 2i for L >0 +σ2 2i for L <0 (V.12) so that the incoming the isospin is rotated by ∓π/2, depending on the side. These properties are related to charge conservation, studied in the next section. VI. INTERNAL CHARGES Does the minus signs in (V.2) imply that a proton would have become an “antiproton” ?! We argue that no ! The reason is gauge covarance. Our clue is that following Yang-Mills [20] the proton/neutron identity of a particle is defined by its charge derived by the Noether theorem, and not by the apparent value of its isospin, which is gauge-dependent. The charge should be defined by setting up a direction field, as we now explain by applying the general theory to the Non-Abelian Aharonov-Bohm Effect outlined in sec. III A. A. Implementability and internal symmetry Implementability. Unlike as for monopoles [24–28], no topological obstruction arises against implementing gauge transformations in the non-Abelian Aharonov-Bohm case. Leaving the mathematical subtleties to the literature, we merely cite the statement proved in [27–29] : 21Theorem VI.1. In the Non-Abelian Aharonov-Bohm Effect the entire gauge group SU(2) is implementable, namely as in (III.15). However we have two inequivalent implementations , namely a trivial one and a twisted one, defined, in the diagonal gauge, by Ω(1) k (x) = k = \u0000 kij \u0001 and Ω(2) k (x) =   k11 eiφk12 e−iφ¯k12 ¯k11   , (VI.1) where det k = |k11|2 + |k12|2 = 1. Infinitesimally, bΨ(1) κ (φ) = 1 2iκ = 1 2i \u0000 κab \u0001 and bΨ(2) κ (φ) = 1 2i   κ11 eiφ κ12 e−iφ κ12 −κ11   , (VI.2) respectively, where we normalized to |bΨ| = 1. Recalling that φ = ±(ϑ − π), the second implementation can also be written in terms of the polar coordinate 0 ≤ ϑ ≤ 2π as, Ω(2) k (ϑ) =   k11 −e±iϑ k12 −e∓iϑ k12 −k11  , bΨ(2) κ (ϑ) = 1 2i   κ11 −e±iϑ κ12 −e∓iϑ κ12 −κ11  , (VI.3) respectively, with the sign depending that of the orbital angular momentum, L. Reassur- ingly, for κ = σ1 we recover an incoming proton, bΨ(1,2)(φ = 0) = bΨ(1,2)(ϑ = π) = Qp in . (VI.4) The 2nd implementation, by Ω (2) h , could actually be brought also to position-independent form by gauge transformations, defined separately in the two contractible domains,    g− = diag(eiϑ/2, e−iϑ/2) in V− = 0 ≤ ϑ ≤ π g+ = diag(e−iϑ/2, eiϑ/2) in V+ = π ≤ ϑ ≤ 2π (VI.5) Ω(2) k (ϑ) → g±Ω(2) k (ϑ)g−1 ± =   k11 −k12 −¯k12 ¯k11   . (VI.6) However the gauge transformations do not match on the negative x axes, g+(ϑ = π) = −iσ3, g−(ϑ = π) = iσ3 . “Dressing” Ω(2) k to a constant introduces a non-trivial transition function, −I 9. In what follows, we consider the infinitesimal version. 9 The inequivalent implementations correspond to the center of the gauge group [28]. The trivial one corresponds to I and the twisted one in (VI.1) corresponds to −I. For G = SO(3) one has only I. 22Internal symmetries. Which of the implementations (VI.2) are symmetries ? We recall that (as said in sec.III A) : κ = ( κab) ∈ G generates an internal symmetry when bΨκ is covariantly con- stant, Dµ bΨκ = 0, cf. (III.17). Then putting the covariant derivates to zero and taking into account that a and a − 2k with k an arbitrary integer are equivalent, (II.7)-(II.8), Dµ bΨ(1) κ = a − 2k i   0 κ12 −κ12 0   , (VI.7a) Dµ bΨ(2) κ = a − (1 + 2k) i   0 eiφκ12 −e−iφκ12 0   . (VI.7b) Thus the covariant derivatives vanish and thus yield an internal symmetry only for either a = 2k or a = 1 + 2k (k an integer) , (VI.8) respectively. In both cases, the enclosed magnetic flux is quantized for both series, Φeven k = (2 k) Φ0, Φ0 = (−iπ σ3), k = 0, ±1, ±2, . . . , (VI.9a) Φodd k = (1 + 2k) Φ0, Φ0 = (−iπ σ3), k = 0, ±1, ±2, . . . , (VI.9b) Comparison with (I.2) shows that the Abelian series splits into two series, one even and one odd. The Wu-Yang factor takes its values in the center Z(G) = {I, −I} of the gauge group SU(2) 10 , as seen in (V.4). We have, of course also bΨσ3 = σ3 2i . (VI.10) Theorem VI.2. For a = 0 or for a = 1 (mod even numbers) i.e., for Wu-Yang factor F = ±I, each κ ∈ su(2) generates, via the first resp. the second implementation in (VI.2) an internal symmetry and thus generate 3 conserved internal charges, (III.18), q(1) b = Qb for a = 2k q(2) b = −2tr \u0010 Q · bΨ(2) σb \u0011 for a = 1 + 2k (VI.11) 10 For SO(3) = SU(2) /Z2 the center would be just {I} and the twisted implementation would not be symmetry, leaving us only with the trivial one. 23where b = 1, 2, 3 are internal indices. For non-integer values of a in the vector potential i.e., for F ̸= ±I, there are no internal symmetries and thus no conserved internal charges. Thus the conserved charges are q(1) b = Qb for the trivial implementation a = 2k and q(2) b in the twisted case with vector potential a = 1 + 2 k. Then for the initial choices (IV.4) we recover the proton, for b = 1 and the neutron for b = 2 with charges qproton = 1 and qneutron = 0, respectively, consistently with (IV.5). The conservation of the charges (VI.11) follows also from covariant-constancy, ˙q = −2tr \u0000 DtQ · bΨ + Q · Dt bΨ \u0001 = 0. The full gauge group G = SU(2) is a symmetry in both cases [27, 28]. However in the even case a = 2k the field (II.5) is a pure gauge Aµ = g−1∂µg for g(x) = eikϑσ3 = diag \u0000 eikϑ, e−ikϑ\u0001 . (VI.12) In the non-trivial odd case, a = 1 + 2 k, when the flux is quantized in odd units as in (VI.9b). For this value of a the field is not a gauge transform of the vacuum : although the it could seemingly be removed by gauge transforming by g(ϑ) = eiϑσ3/2 However this function is not well-defined on the plane, because g(ϑ = 0) = I but g(ϑ = 2π) = eiπσ3 = −I ̸= I 11. B. Conserved charges in the non-Abelian Aharonov-Bohm effect Henceforth we focus our attention at a = 1 + 2k an odd integer, we fix the implemention to be that of the initial proton direction, (IV.4), Qp in = 1 2iσ1 = 1 2i \u0010 0 1 1 0 \u0011 . Then parallel transport implies that 12, bΨσ1 (φ) = Qp ±(φ) = cos φσ1 2i ∓ sin φσ2 2i = 1 2i   0 e±iφ e∓iφ 0   . (VI.13) The charge is the projection onto the direction field, (III.18). Spelling out for the proton trajectories (IV.6)-(IV.7) we get indeed, for all φ qproton = −2tr (Qp ±(φ))2 = 1 , (VI.14) as expected: σ1 ∈ su(2) generates an internal symmetry and thus the charge is conserved: the incoming proton remains a proton during the scattering. For the S-matrix for example, 11 Remember that for a = 1 + 2k the S-matrix is minus the identity, S = −I, as noted in (V.4). 12 The two signs correspond to the two half-planes: the sign should change when we switch from one to the other. Remember that φ = ±(ϑ − π), depending on the sign of the orbital angular momentum, L. 24the minus sign in (V.4) combines with the minus sign of the direction field and thus yield conserved charge equal to 1, interpreted as the electric charge of the proton. For σ2 and σ3 we have instead, by (IV.10), bΨσ2 (φ) = 1 2 \u0012 0 −e±iφ e∓iφ 0 \u0013 ⇒ qneutron = 0 , (VI.15a) bΨσ3 (φ) = 1 2i   1 0 0 −1   ⇒ qvert = Q3(0) . (VI.15b) The 3rd charge here corresponds to σ3 perpendicular to the plane of the motion. For our choice it vanishes. These formulæ are consistent with (IV.5) and confirm that when the flux is quantized in odd units, Φ = −(1+2 k)π(σ3/i) with k an integer, cf. (VI.9b), then we have three conserved charges. However if a is not an integer i.e., for flux ⃗Φ ̸= Nπσ3 the charge is not conserved. For a = 1/2, for example, \u0010 Qp 1 2 \u0011 ± (φ) = 1 2i   0 e±iφ/2 e∓iφ/2 0   (VI.16) and the charge is not conserved, ⇒ q1 2 (φ) = −2tr \u0010 bΨσ1 · Q1 2 \u0011 = cos φ 2 ̸= const . (VI.17) Thus the incoming proton looses its initial charge qin = 1 and goes out with zero charge, qout = 0, i.e., as a neutron, Qin = Qp → Qout = Qn , (VI.18) as argued by Wu and Yang [9]. Intuitively, the “electromagnetic direction field”bΨ is in fact that of a proton fora = 1+2k (drawn in red and in blue in FIG.3), and the isospin for a = 1 2 (drawn in magenta and cyan) turn at different angular velocity, therefore the projection of the latter onto the former one is not a constant, (VI.17). Similarly, an incoming neutron would come out as a proton, as shown in FIG.3. Generalizing our previous investigations, we now consider motion with the same external initial conditions as before but allowing a non-zero vertical component for the isospin, Qin = \u0010 sin ασ1 2i + cosασ3 2i \u0011 , (VI.19) 25where 0 ≤ α ≤ π is the angle of the initial isospin with the σ3-oriented flux, shown in FIG.6. The spatial trajectory remains unchanged, whereas the isospin equation is solved by Q±(φ) = 1 2i   sin α   0 e±iφa e∓iφa 0   + cosα σ3    . (VI.20) Then the projection onto the flux’ direction is ⃗Q · ⃗Φ = Q3(0) = cos α = const , (VI.21) which implies that the isospin precesses on a circular section at constant height of a cones with opening angle α w.r.t. the σ3-oriented flux ⃗Φ, whereas theirs apexes travel along the free spatial trajectories as for α = π/2, see FIG.6. Maintaining our charge-based proton/neutron definition with direction field bΨσ1 , ˜q = sin α (VI.22) which is conserved, is however neither a proton nor a neutron except for α = 0 (when it is a neutron) or α = π/2 (when it is a proton) 13. Similarly, the charge generated by bΨσ3 = σ3, −2tr \u0010 ˜Q±(φ) · bΨσ1 \u0011 = cos α , (VI.23) although a constant of the motion, is 1 only for α = 0 but is zero α = π/2 (VI.15b). However as emphasised by Yang and Mills, we are allowed to change our proton/neutron definition and call instead proton an incoming particle with ˜Ψ(φ = 0) = Qin in (VI.19) and promote (VI.20) with a = 1 to a “moving reference frame” ˜Ψ(φ) = 1 2i   sin α   0 e±iφ e∓iφ 0   + cosα σ3    , 0 ≤ φ ≤ π, (VI.24) Then projecting onto the latter, ˜q = −2tr (˜Ψ · ˜Q) = 1 2tr   sin2 α   eiφ(a−1) 0 0 e−iφ(a−1)   + cos2 α   1 0 0 1      , (VI.25) would again be a satisfactory charge definition in that it would be (i) gauge-covariant and (ii) for a = 1 equal to 1 for all φ by Pythagoras’ theorem. The redefined charge (VI.25) would again be conserved and remain a “proton” with respect the new direction field ˜Ψ. 13 The situation is analogous to that observed for classical spin, which becomes a half-integer only under quantization [69, 84]. 26FIG. 6: The isospin vector ⃗Q whose initial direction has an angle α with that of the non-Abelian flux ⃗Φ rotates along a horizontal circle on the cone of opening angle α around the flux’ direction. The sense whose the rotation depends on the side the. The apex of the cone is carried along the free motion in space. The charge defined as the projection of ⃗Q onto the comoving “electric direction” bΨ is conserved when the flux is quantized in odd units (VI.9b). The electric-type internal charges defined by projecting onto a covariantly constant mov- ing reference frame, (III.18), are conserved. Another curious feature of the inherent freedom advocated by Yang and Mills [20] to choose what we call a proton [9] is the following. Let us assume that a = 1; then we have 3 internal symmetries. Selecting again the covariantly constant direction field bΨσ1 (VI.13) above provides us, in the twisted case, with 3 conserved charges as seen above. However, the components of a constant isospin trajectory, ⃗Q(φ) = ⃗Q0 = const , (VI.26) which would provide us with trivially conserved charges in the even case a = 2k will not be conserved in the twisted case a = 1 in general. Using (VI.11) we find indeed,    Q1(φ) = σ1 2i q1 = cos φ ̸= const Q2(φ) = σ2 2i q2 = sin φ ̸= const Q3(φ) = σ3 2i q3 = 0 = const (VI.27) 27C. Spin from isospin To get an insight, we start with the trivial case a = 2k i.e., F = I. The particle is free and the total angular momentum, ⃗J = ⃗L − ⃗Q where ⃗L = r × ⃗ p , (VI.28) is conserved. The two terms here are in fact separately conserved : ⃗L, the orbital one is conserved, because the spatial motion is free; the internal one, ⃗Q, is conserved because in the diagonal gauge the commutator drops out from the isospin equation (III.1b) leaving us with d ⃗Q/dt = 0. For a = 2k the gauge potential can be gauged away (as noted above) and the equation is gauge-covariant. This scenario realizes Souriau’s “d´ ecomposition barycentrique” [84] chap. III. p.163 14, which says that when the “space of motions” (his replacement for the phase space) has at least 8 dimensions which is our case here because of the isospin, then, for an isolated system with pairwise internal interactions, the rotations of the center of mass and the rotations around the latter act separately and both contribute to the angular momentum. Thus the SO(3) symmetry is in fact enlarged to SO(3) ext × SO(3)int . In the non-Abelian Aharonov-Bohm setup the flux is enclosed into a thin cylinder that arguably reduces the external symmetry to an axial SO(2) ext. However this mechanical constraint is unrelated to the internal structure (isospin) of our particles, leaving us with the enlarged rotational symmetry SO(2)ext × SO(3)int , (VI.29) generated by the orbital momentum ⃗L and the isospin 3-vector ⃗Q, respectively, as in (VI.28). Souriau illustrates his “th´ eor` eme de d´ ecomposition barycentrique” by a non-relativistic particle with spin [84]. However, expressed in Souriau’s language, the internal symplectic form for the isospin is in fact identical to that for his classical non-relativistic spin [84], namely the surface form of the two-sphere of radius |Q0|, O ∼ S2 ⊂ su(2). The (separately) conserved quantities are the orbital angular momentum ⃗L and the 3 internal charges which are in fact the 3 conserved components Qi of the isospin in eqn. (VI.11). 14 Anticipated by Jacobi in his K¨ onigsberg lectures [85]. 28What happens in the non-trivial case ? Our general theory implies that when a is not an integer then no covariantly constant “direction fields” and thus no internal symmetries do exist, implying that the total angular momentum (VI.28) can not be conserved, leaving us with the mere orbital expression. We are still left with the other exceptional case, a = 1 + 2 k. However recall that the orbital angular momentum, ⃗L, is conserved for all a ̸= 2 k as we have seen in Sec.III B. But this implies that the total angular momentum, (VI.28) can not be conserved whenever the isospin is not a constant of the motion (except for a particular initial condition). We conclude that the internal structure breaks the full rotational symmetry (VI.28) to mere orbital one: for a = 1 + 2 k, σ3 generates, as before, internal rotations around the flux’ direction which yields the internal angular momentum, Lint = Q3 σ3 2i (VI.30) which is conserved since bΨσ3 = σ3 and Q3 = Q3(0)). Thus the non-trivial flux breaks the symmetry (VI.29) down to a double axial one, SO(2)ext × SO(2)int , (VI.31) generated by independent external and internal rotations around the (respective) 3rd direc- tions. For α = 0 the cone degenerates to a vertical vector of unit length which is conserved; for α = π/2 we recover our previous result in FIGs.3-4-5. The total angular momentum (VI.28) is conserved in the vacuum case a = 2k (F = I) but broken to (VI.31) for a = 2k + 1 (F = −I), leaving us with the 3rd component only, J3 = L3 − Q3 . (VI.32) The spin from isospin term (although only along the 3rd direction) but the two other com- ponents in (VI.28) are not more conserved — the isospin precesses. VII. CONSERVED QUANTITIES BY VAN HOLTEN’S ALGORITHM To deepen our insight, we revisit the internal symmetry question from a rather different point of view, namely by using van Holten’s approach [79, 80]. His algorithm amounts to developping the conserved quantity we are looking for into powers Πi = ˙xi (we took m = 1), I = C + CiΠi + 1 2CijΠiΠj + ... (VII.1) 29whose conservation, {I, H} = 0 requires order 1 : DiC = QaFa ijCj order 2 : DiCj + DjCi = Qa \u0000 Fa ikCkj + Fa jkCki \u0001 , ... (VII.2) which stops when we have a Killing tensor. Applied to the Non-Abelian Aharonov-Bohm Effect, Fa ij = 0, this simplifies to order 1 : DiC = 0, order 2 : DiCj + DjCi = 0 , ... (VII.3) For simplicity we restrict our attention at planar real-space motion, i.e., assume that the initial conditions imply that the real-space motion is in the x −y plane with the z-direction identified with σ3 in isospace, ˆz ∼ σ3. Planar rotations, for example, are generated by the Killing vector ˆz × ⃗ x .Then the algorithm yields C = 0, Ci = −εijxj [79, 80] and we recover the 3rd component of the conserved orbital angular momentum (IV.1), L ≡ L3 = εijxi ˙xj , as expected. Now we rederive our previous results about internal charges by the van Holten method. Searching for the first-order in ⃗Q coefficient in the form C (⃗ x, Q) = Ca (⃗ x) Qa (VII.4) and using the parallel transport eqn. DiQa = 0, we get DiCa = ∂iCa − εabcAb i Cc = 0 . (VII.5) Inserting here the non-Abelian Aharonov-Bohm gauge potential Aa i = −Ba r2 εijxj with (Ba) = (0, 0, a), these equations become    ∂xC1 − aC2 y r2 = 0, ∂ xC2 + aC1 y r2 = 0, ∂ xC3 = 0, ∂yC1 + aC2 x r2 = 0, ∂ yC2 − aC1 x r2 = 0, ∂ yC3 = 0 , (VII.6) which admits the nontrivial solution C1(x, y) = α sin µ, C2(x, y) = α cos µ, where µ = µ(x, y) = a arctan x y + ϕ0 , (VII.7) where ϕ0 is an arbitrary constant. This should be completed with C3 = β = const which follows from having directed the vector potential into the 3rd direction. 30For α = 0, the isospin has only a σ3 component and (VI.30) is recovered. We put hence α = 1, β= 0 and focus our attention at the two remaining components. Then the algorithm yields, I = sin µ Q1 + cosµ Q2 , (VII.8) This eqn. is plainly valid for y ̸= 0, i.e., in the upper and lower half-planes separated by the x-axis because of y in the denominator for µ. However letting y → 0 from the two sides yields, for fixed x >0, x/y → ±∞, depending on the sign of y. Then the asymptotic behavior of the arctan implies, µ →    µ+ = aπ/2 + ϕ0 when y >0 µ− = −aπ/2 + ϕ0 when y <0 ⇒ µ− = µ+ − aπ . (VII.9) Thus the trigonometric functions do not match in general, I+ ̸= I− : there is no well- defined conserved quantity on the entire plane , consistently with the breaking of the internal symmetries found earlier. However remember that there might be a transition function (in fact a constant element) between the two half-planes which should belong to the center of the gauge group [27, 28], I+ = h I− , where h ∈ ZG = {I, −I}. (VII.10) For h = I this requires sin aπ 2 = 0 so that a is an even integer, a = 2k. For h = −I we get instead cos aπ 2 = 0 implying that a is an odd integer, a = 1 + 2k .In both cases we get from by using (VII.4) with (VII.7) (VII.8), which is a well-defined conserved quantity for both of exceptional values of a. Conversely, remembering that the length of ⃗Q is a constant, |⃗Q|2 = I2, allows us to recover the trajectory using the conserved quantity I as 15 Q1(x, y) = I sin µ(x, y) and Q2(x, y) = I cos µ(x, y) (VII.11) consistently with (IV.6)-(IV.7) and (IV.10). Inserting (VII.11) into (VII.8) identity as it should. The conserved quantity (VII.8) is indeed the conserved charge q = QaΨa in (III.5) or more generally, in (VI.11) with (VII.11) for bΨσ1 chosen as in (VI.13). 15 More generally, |⃗Q|2 = I2 + |Q3|2. 31In particular for a = 2k (even) µ → ±kπ 2 + ϕ0 = const, implying that ⃗Q is a constant, Q1 = (−1)k I sin ϕ0 = const and Q2 = (−1)k I cos ϕ0 = const . (VII.12) For a = 1 + 2k (odd) we have µ = µ(x, y) ̸= const, (VII.8), however the limits from the two sides, µ± → ±π 2 (1 + 2k) + ϕ0, do match when y → 0, (VII.9). These results confirm those found in sec.V. as we show for a = 1 + 2k with y >0. For the “in” (resp. “out”) states, choosing ϕ0 = (1 + k)π (VII.11) implies, Q1 in = 1, Q2 in = 0 resp. Q1 out = −1, Q2 out = 0 , (VII.13) consistently with (V.2). The other values shown in FIG.4 are recovered also by appropriate choices of ϕ0. Returning to the angular momentum, we get, consistently with our previous results, Theorem VII.1. When the Wu-Yang factor does not belong to the center, F ̸= ±I, then only the 3rd component of the orbital angular momentum, L = L3, is conserved. However when the flux is quantized in either of the two series in (VI.9), then L3 and Q3 in (IV.1) and (VI.30), are both separately conserved and the symmetry is doubled,(VI.31), consistently with Souriau’s d´ ecomposition barycentrique[84]. VIII. CONCLUSION AND OUTLOOK This project, suggested by T. T. Wu to PAH over 40 years ago [22, 29, 86–89] 16, studies the Non-Abelian Aharonov-Bohm Effect proposed by Wu and Yang in their cele- brated paper on the non-integrable phase factor [9]. While the original proposal is field- theoretical/quantum mechanical, here we argue that a “shadow” of it, namely isospin pre- cession17, can be recovered also at the the pseudo-classical level, with the isospin represented by a vector ⃗Q of the Lie algebra of the gauge group 18. This is in sharp contrast with the Abelian (electromagnetic) case : Aharonov-Bohm effect is purely quantum mechanical [1]. 16 H. Bacry, musing over a piece of paper in Marseille in the eighties, more than once whispered : “On ´ ecrit le mˆ eme article durant toute sa vie” (One writes the same paper during all his life). 17 Isospin precession was found also for diatomic molecules [35, 36]; see the recent study [37]. 18 Subtleties of the concept of classical isospin were discussed in [90–92]. 32Comparing the Abelian and the non-Abelian case, we argue that “the electric charge is a constant scalar (in fact the same constant for all motions), but the non-Abelian isospin vector has a proper dynamics, – even if it is rather poor and, being parallel transported, is determined by the space-time motion. Thus the difference comes from the (partial) observability of the phase. Physical interpretation in terms of protons and neutrons can be obtained by following the charge-based arguments of Yang and Mills [20]. The charge is the projection of the isospin vector onto a chosen direction field bΨ(xµ), (III.18). Then a proton is a particle with charge 1 and a neutron has zero charge. However, the charge is conserved only when bΨ(xµ) is covariantly constant (III.17), Dµ bΨ = 0, i.e., when bΨ corresponds to an internal symmetry [27, 28]. For non-Abelian monopoles this happens outside their core [52, 71, 72], allowing to reduce the dynamics of a test particle to an Abelian problem [93, 94]. In the non-Abelian Aharonov-Bohm case, though, a covarantly constant bΨ exists only when the non-Abelian flux is quantized in either of the two distinct series (VI.9), then we get 3 conserved internal charges [27, 28]; for other flux values we do not get any. The flux quantization we find here is reminiscent of the one in superconducting rings [95–97]. This surpising behaviour hints at the global structure of the gauge group: it comes from the center of SU(2) being composed of I and −I; for SO(3) = SU(2) /Z2 we have only I. Thus answering the question : — “Is the non-Abelian flux quantized ? And if it is, then in which units ?” would depend on what the “true” gauge group is. The most dramatic prediction of Wu and Yang [9] is that an incoming proton might be turned into an outgoing neutron under specific circumstances. We confirm their statament when the flux is not quantized : for half-integer flux a = 1 2 , for example, then Dµ bΨ ̸= 0 and an incoming proton could indeed turn into an outgoing neutron . However the initial choice can be changed yielding different casts, illustrating the arbi- trariness of identifying protons and neutrons, as emphasised by Yang and Mills [20]. Such an eventuality raises the fundamental question : Is such a transition indeed possible or not ? What would happen with the electric charge ? What could be the physical mechanism for it. We confess not to be able to answer. It might well happen that the test particle framework we follow in this paper (or its quantum mechanical antecedents [22, 86– 88]) are just not sufficient to answer. Another idea could be to enlarge the commutative 33part of the gauge group from SU(2) to Weinberg-Salam U(2) (locally (U(2) × U(1)). The direction field bΨ(xµ) behaves, mathematically, as a non-dynamical Higgs field in the adjoint representation – which however plays only a passive role, as a sort of “measuring apparatus”. For a Wu-Yang monopole [70], the archi-type of non-Abelian monopoles, for example, it corresponds to their radial “hedgehog” direction (III.7). For a ’t Hooft-Polyakov monopole instead, the direction field is promoted to a dynamical field which allows for finite- energy soliton solutions of the coupled Yang-Mills-Higgs equations [52, 71, 72]. The direction field chosen tacitly is the normalized physical Higgs field, bΨ = Ψ/|Ψ|. Outside the monopole core the SU(2) gauge symmetry is spontaneously broken to a residual U(1) implying that the Higgs field is covariantly constant there. The non-Abelian flux line we studied in this paper corresponds, intuitively, to a pure YM vortex whose “outside region” is the entire plane with the origin removed. Form the point of view of a field-theoretical YMH Lagrangian, the kinetic term DµΨDµΨ is switched off, reducing Ψ to a non-dynamical quantity. The contrast between pure YM and YMH becomes even more salient in Chern-Simons theory [98, 99], whose non-Abelian version [100, 101] is strongly reminiscent of non-Abelian Aharonov-Bohm — except for that the Higgs field is, once again, dynamical. However the analytic solutions found in [100, 101] are as remote as possible from the non-Abelian Aharonov-Bohm in that they are self-dual, 1 2 ϵijkFjk = DiΨ . instead of our cast with Fjk ∝ δ(x). The angular momentum in the Non-Abelian Aharonov-Bohm Effect exhibits the “spin from isospin” phenomenon [66, 74, 81–83]. The orbital angular momentum is conserved for all fluxes. However for the two particular values of the flux in (VI.9), we have additional terms generated by internal symmetry which is independent of space-time rotations. The total angular momentum, J in (VI.28) splits into two separately conserved pieces, one ex- ternal (orbital) and one internal (isospin), realizing Souriau’s doubled rotational symmetry idea [84]. When the flux quantized in odd units, eqn (VI.9b), the remnant of the internal symmetry is detected by the precessions of the isospin on horizontal circles, shown in FIG.6. Further insight can be gained by looking at the problem from a different point of view, advocated by van Holten [79, 80], see sec.VII. The flux quantization we found above has some analogy with the Dirac quantization of a monopole: choosing the Dirac string along (say) the negative z axis, in its neighborhood the 34vector potential will behave approximately as an Aharonov-Bohm solenoid, which remains unobservable when the flux is quantized [52, 102, 103]. Thus the non-Abelian Aharonov-Bohm setup is indeed the planar analog of the non- Abelian monopole [52, 104, 105]. Our investigations confirm the credo of Wu and Yang [9] : . . . what constitutes an intrinsic and complete description of electromagnetism? [. . . ] An examination of the Bohm-Aharonov experiment indicates that in fact only the phase factor (I.1) and not the phase is physically meaningful. Quantum aspects were studied in [22, 27–29, 86–89]. See [37–41] for a (very incomplete) selection. It is remarkable that part of the Non-Abelian Aharonov-Bohm Effect can be captured by the (pseudo-)classical isospin of Kerner and Wong [55, 56], confirming that the isospin is neither a truly classical nor a truly quantum concept: it is at half ways between the two, justifying our terminology. Yang et al [47, 106] attribute the non-Abelian Aharonov-Bohm effect to the breaking of time-reversal symmetry. However parallel transport implies that real time is replaced, in our framework, by the angle φ. Thus the effect comes from “navigating” on either sides of the flux. However we found that when the flux is quantized, (VI.9) then the time reversal symmetry disappears: side-dependent phase shifts yield identical contributions on both sides, as highlighted by doubling the rotational symmetry, as in (VI.28)-(VI.31). Intensive ongoing work on effective/artifical gauge fields [35–51] provide promising testing ground of the conceptual suggestions of Wu and Yang, allowing us to hope that the Non- Abelian Aharonov-Bohm Effect may, on one day, not be considered anymore as . . . “dry water” (using a celebrated expression put forward by J. von Neumann). Acknowledgments This paper is dedicated to the memory of “Tai” (Tai-Tsun Wu : 1933-2024)). Corre- spondance with Janos Balog, Gary Gibbons, Csaba S¨ uk¨ osd and A. Zeilinger is gratefully acknowledged. PMZ was partially supported by the National Natural Science Foundation of China (Grant No. 11975320). 35[1] Y. Aharonov and D. Bohm, “Significance of electromagnetic potentials in the quantum the- ory,” Phys. Rev. 115 (1959), 485-491 doi:10.1103/PhysRev.115.485 [2] R. G. Chambers, “Shift of an Electron Interference Pattern by Enclosed Magnetic Flux,” Phys. Rev. Lett. 5 (1960) no.1, 3-5 doi:10.1103/physrevlett.5.3; [3] G. M¨ ollenstedt and W. Bayh, “Messung der kontinuierlichen Phasenschiebung von Elektro- nenwellen im kraftfeldfreien Raum durch das magnetische vektorpotential einer Luftspule,” Naturwiss. 49, 81 (1962) https://doi.org/10.1007/BF00622023. [4] N. Osakabe, T. Matsuda, T. Kawasaki, J. Endo, A. Tonomura, S. Yano and H. Yamada, “Experimental confirmation of Aharonov-Bohm effect using a toroidal magnetic field confined by a superconductor,” Phys. Rev. A 34 (1986), 815-822 doi:10.1103/PhysRevA.34.815 [5] M. Peshkin, “The Aharonov-Bohm Effect. Part 1: Theory,” Lect. Notes Phys. 340 (1989), 1-34 doi:10.1007/BFb0032077 [6] A. Tonomura, “Applications of electron holography” Rev. Mod. Phys. 59, 639-669 (1987). DOI:https://doi.org/10.1103/RevModPhys.59.639 [7] A. Tonomura, “The Aharonov-Bohm Effect. Part 2: Experiment,” Lect. Notes Phys. 340 (1989), 35-152 doi:10.1007/BFb0032078 [8] H. Batelaan and A. Tonomura, “The Aharonov-Bohm effects: Variations on a subtle theme,” Phys. Today 62N9 (2009), 38-43 doi:10.1063/1.3226854 [9] T. T. Wu and C. N. Yang, “Concept of Nonintegrable Phase Factors and Global Formulation of Gauge Fields,” Phys. Rev. D 12 (1975), 3845-3857 doi:10.1103/PhysRevD.12.3845 [10] L. S. Schulman, “Approximate topologies” J. Math. Phys. 12, 304 (1971) https://doi.org/10.1063/1.1665592 [11] L. S. Schulman, Techniques and applications of path integration, J. Wiley, N.Y. (1981); [12] M.G.G. Laidlaw and C. Morette-DeWitt, Phys. Rev. D3, 1375 (1971); [13] J.S. Dowker, J.Phys. A5, 936 (1972); [14] J.S. Dowker, “Selected topics in topology and quantum field theory,”, Austin Lectures (1979) [15] P. A. Horvathy, G. Morandi and E. C. G. Sudarshan, “Inequivalent quantizations in multiply connected spaces,” Nuovo Cim. D 11 (1989), 201-228 doi:10.1007/BF02450240 [16] P. A. Horvathy, “Quantization in Multiply Connected Spaces,” Phys. Lett. A 76 (1980), 11 36doi:10.1016/0375-9601(80)90133-4 [17] P. A. Horvathy, “Prequantization From Path Integral Viewpoint,” Lect. Notes Math. 905 (1982), 197-206 CPT-80-P-1230. Available as [arXiv:2402.17629 [math-ph]]. [18] S. Kobayashi and K. Nomizu, Foundations of Differential Geometry Vols. I and II, (Inter- science, New York, 1963 and 1969) [19] D. Husemoller, Fibre Bundles . 3rd edition Graduate Texts in Mathematics. https://doi.org/10.1007/978-1-4757-2261-1 Springer NY (2013). [20] C. N. Yang and R. L. Mills, “Conservation of Isotopic Spin and Isotopic Gauge Invariance,” Phys. Rev. 96 (1954), 191-195 doi:10.1103/PhysRev.96.191 [21] A. Zeilinger, M.A Horne and C.G. Shull, “Search for unorthodox phenomena by neutron interference experiments,” in Proc. Int. Symp. on the foundations of quantum mechanics , Tokyo 1983, ed. S. Kamefuchi, (Physical Society of Japan), Tokyo 1984, pp.289-293. [22] P. A. Horvathy, “The non-Abelian Aharonov-Bohm effect,” BI-TP-82/14. Inspire code Hor- vathy:1982fx. Available as arXiv: 2312.16133 [hep-th]. [23] A. S. Schwarz, “FIELD THEORIES WITH NO LOCAL CONSERVATION OF THE ELEC- TRIC CHARGE,” Nucl. Phys. B 208 (1982), 141-158 doi:10.1016/0550-3213(82)90190-0 [24] P. C. Nelson and A. Manohar, “Global Color Is Not Always Defined,” Phys. Rev. Lett. 50 (1983), 943 doi:10.1103/PhysRevLett.50.943 [25] A. P. Balachandran, G. Marmo, N. Mukunda, J. S. Nilsson, E. C. G. Sudarshan and F. Za- ccaria, “Monopole Topology and the Problem of Color,” Phys. Rev. Lett. 50 (1983), 1553 doi:10.1103/PhysRevLett.50.1553 [26] P. C. Nelson and S. R. Coleman, “What Becomes of Global Color,” Nucl. Phys. B 237 (1984), 1-31 doi:10.1016/0550-3213(84)90013-0 [27] P. A. Horvathy and J. H. Rawnsley, “Internal Symmetries of Nonabelian Gauge Field Con- figurations,” Phys. Rev. D 32 (1985), 968 doi:10.1103/PhysRevD.32.968 ; [28] P. A. Horvathy and J. H. Rawnsley, “The Problem of ‘Global Color’ in Gauge Theories,” J. Math. Phys. 27 (1986), 982 doi:10.1063/1.527119 [29] P. A. Horvathy, “The Nonabelian Aharonov-Bohm Effect,” Phys. Rev. D 33 (1986), 407-414 doi:10.1103/PhysRevD.33.407 [30] M. G. Alford, K. Benson, S. R. Coleman, J. March-Russell and F. Wilczek, “The Interactions and Excitations of Nonabelian Vortices,” Phys. Rev. Lett. 64 (1990), 1632 [erratum: Phys. 37Rev. Lett. 65 (1990), 668] doi:10.1103/PhysRevLett.64.1632 [31] F. Wilczek and Y. S. Wu, “SPACE-TIME APPROACH TO HOLONOMY SCATTERING,” Phys. Rev. Lett. 65 (1990), 13-16 doi:10.1103/PhysRevLett.65.13 [32] M. G. Alford, J. March-Russell and F. Wilczek, “Discrete Quantum Hair on Black Holes and the Nonabelian Aharonov-Bohm Effect,” Nucl. Phys. B 337 (1990), 695-708 doi:10.1016/0550-3213(90)90512-C [33] J. Preskill and L. M. Krauss, “Local Discrete Symmetry and Quantum Mechanical Hair,” Nucl. Phys. B 341 (1990), 50-100 doi:10.1016/0550-3213(90)90262-C [34] R. H. Brandenberger, “Topological defects and structure formation,” Int. J. Mod. Phys. A 9 (1994), 2117-2190 doi:10.1142/S0217751X9400090X [arXiv:astro-ph/9310041 [astro-ph]]. [35] J. Moody, A. Shapere, and F. Wilczek, “Realization of magnetic monopole gauge fields : diatoms and spin precession,” Phys. Rev. Lett. 56, 893 (1986) [36] R. Jackiw, “Angular momentum for diatoms described by gauge fields” Phys. Rev. Lett. 56, 2779 (1986). [37] P. A. Horvathy and P. M. Zhang, “Kerner equation for motion in a non-Abelian gauge field,” The Languages of Physics. A Themed Issue in Honor of Professor Richard Kerner on the Occasion of His 80th Birthday. Universe 9 (2023) no.12, 519 doi:10.3390/universe9120519 [arXiv:2310.19715 [math-ph]]. [38] M. Bright and D. Singleton, “Time-dependent non-Abelian Aharonov-Bohm effect,” Phys. Rev. D 91 (2015) no.8, 085010 doi:10.1103/PhysRevD.91.085010 [arXiv:1501.03858 [hep-th]], [39] S. A. Hosseinin Mansoori and B. Mirza, “Non-Abelian Aharonov–Bohm effect with the time- dependent gauge fields,” Phys. Lett. B 755 (2016), 88-91 doi:10.1016/j.physletb.2016.02.004 [arXiv:1601.08164 [quant-ph]]. [40] C. Chatterjee and M. Nitta, “Aharonov-Bohm Phase in High Density Quark Matter,” Phys. Rev. D 93 (2016) no.6, 065050 doi:10.1103/PhysRevD.93.065050 [arXiv:1512.06603 [hep-ph]]. [41] R. N´ emeth and J. Cserti, “Differential scattering cross section of the non-Abelian Aharonov-Bohm effect in multiband systems,” Phys. Rev. B 108 (2023) no.15, 155402 doi:10.1103/PhysRevB.108.155402 [arXiv:2306.13448 [quant-ph]]. [42] K. Osterloh, M. Baig, L. Santos, P. Zoller, and M. Lewenstein, “Cold Atoms in Non-Abelian Gauge Potentials: From the Hofstadter ‘Moth’ to Lattice Gauge Theory,” Phys. Rev. Lett. 95, 010403 (2005). 38[43] N. Goldman, G. Juzeli¯ unas, P. ¨Ohberg and I. B. Spielman, “Light-induced gauge fields for ultracold atoms,” Rept. Prog. Phys. 77 (2014) no.12, 126401 doi:10.1088/0034- 4885/77/12/126401 [arXiv:1308.6533 [cond-mat.quant-gas]]. [44] A. Jacob, P. ¨Ohberg, G. Juzeli¯ unas, and L. Santos, “Cold atom dynamics in non-Abelian gauge fields,” Appl. Phys. B 89, 439 (2007). [45] J. Dalibard, F. Gerbier, G. Juzeli¯ unas, P. ¨Ohberg, “Colloquium: Artificial gauge potentials for neutral atoms,” Rev. Mod. Phys. 83, 1523-1543 (2011). [46] Y. Chen, R. Y. Zhang, Z. Xiong, Z. H. Hang, J. Li, J. Q. Shen and C. T. Chan, “Non-Abelian gauge field optics,” Nature Commun. 10 (2019) no.1, 3125 doi:10.1038/s41467-019-10974-8 [arXiv:1802.09866 [physics.optics]]. [47] Y. Yang, C. Peng, D. Zhu, H. Buljan, J. D. Joannopoulos, B. Zhen and M. Soljaˇ ci´ c, “Synthesis and Observation of Non-Abelian Gauge Fields in Real Space,” Science 365 (2019), 1021 doi:10.1126/science.aay3183 [arXiv:1906.03369 [physics.optics]]. [48] Michel Fruchart, Yujie Zhou and Vincenzo Vitelli, “Dualities and non-Abelian mechanics,” Nature 577, 636?640 (2020) [49] J. Wu, Z. Wang, Y. Biao, F. Fei, S. Zhang, Z. Yin, Y. Hu, Z. Song, T. Wu, F. Song, et al., “Non-Abelian gauge fields in circuit systems,” Nature Electronics 5, 635 (2022). [50] Y. Yang, B. Yang, G. Ma, J. Li, S. Zhang and C. T. Chan, “Non-Abelian physics in light and sound,” Science, 383, 844-858 (2024). DOI: 10.1126/science.adf9621 [arXiv:2305.12206 [physics.optics]]. [51] Huanhuan Yang, Lingling Song, Yunshan Cao, Peng Yan, “Circuit realization of topological physics,” Physics Reports, 1093, (2024) 1-54 https://doi.org/10.1016/j.physrep.2024.09.007. [52] P. Goddard and D. I. Olive, “New Developments in the Theory of Magnetic Monopoles,” Rept. Prog. Phys. 41 (1978), 1357 doi:10.1088/0034-4885/41/9/001 [53] M. Asorey, “Some Remarks on the Classical Vacuum Structure of Gauge Field Theories,” J. Math. Phys. 22 (1981), 179 [erratum: J. Math. Phys. 25 (1984), 187] doi:10.1063/1.524732 [54] M. Asorey, “REGULARITY OF GAUGE EQUIVALENCE IN QUANTUM MECHAN- ICS AND THE AHARONOV-BOHM EFFECT,” Lett. Math. Phys. 6 (1982), 429-435 doi:10.1007/BF00405862 [55] R. Kerner, “Generalization of the Kaluza-Klein Theory for an Arbitrary Nonabelian Gauge Group,” Ann. Inst. H. Poincare Phys. Theor. 9 (1968), 143-152 39[56] S. K. Wong, “Field and particle equations for the classical Yang-Mills field and particles with isotopic spin,” Nuovo Cimento 65A, 689 (1970) [57] A. Trautman, “Fiber bundles associated with space-time,” Rept. Math. Phys. 1 (1970), 29-62 doi:10.1016/0034-4877(70)90003-0 [58] Y. M. Cho, “Higher - Dimensional Unifications of Gravitation and Gauge Theories,” J. Math. Phys. 16 (1975), 2029 doi:10.1063/1.522434 [59] E. Witten, “Search for a Realistic Kaluza-Klein Theory,” Nucl. Phys. B 186 (1981), 412 doi:10.1016/0550-3213(81)90021-3 [60] A. P. Balachandran, P. Salomonson, B. S. Skagerstam and J. O. Winnberg, “Classical De- scription of Particle Interacting with Nonabelian Gauge Field,” Phys. Rev. D 15 (1977), 2308-2317 doi:10.1103/PhysRevD.15.2308 [61] A. P. Balachandran, S. Borchardt and A. Stern, “Lagrangian and Hamiltonian Descriptions of Yang-Mills Particles,” Phys. Rev. D 17 (1978), 3247 doi:10.1103/PhysRevD.17.3247 [62] S. Sternberg, “Minimal Coupling and the Symplectic Mechanics of a Classical Parti- cle in the Presence of a Yang-Mills Field,” Proc. Nat. Acad. Sci. 74 (1977), 5253-5254 doi:10.1073/pnas.74.12.5253 [63] V. Guillemin and S. Sternberg, “On the Equations of Motion of a Classical Particle in a Yang-Mills Field and the Principle of General Covariance,” Hadronic J. 1 (1978), 1 TAUP- 655-78. [64] C. Duval, “On the prequantum description of spinning particles in an external gauge field,” Proc. Aix Conference on Diff. Geom. Meths. in Math. Phys. Ed. Souriau. Springer LNM 836, 49-67 (1980). This paper is an extended version of C. Duval’s unpublished paper [65]. [65] C. Duval, “Sur les mouvements classiques dans un champs de Yang-Mills,” Marseille preprint CPT-78-P-1056. [66] C. Duval and P. Horvathy, “Particles With Internal Structure: The Geometry of Clas- sical Motions and Conservation Laws,” Annals Phys. 142 (1982), 10 doi:10.1016/0003- 4916(82)90226 - 3 [67] A. W. Wipf, “NONRELATIVISTIC YANG-MILLS PARTICLES IN A SPHERICALLY SYMMETRIC MONOPOLE FIELD,” J. Phys. A 18 (1985), 2379-2384 doi:10.1088/0305- 4470/18/12/034 40[68] L. G. Feher, “Classical motion of coloured test particles along geodesics of a Kaluza-Klein spacetime,” Acta Phys. Hung. 59 (1986), 437-444 [69] P. A. Horvathy, “Classical Action, the Wu-Yang Phase Factor and Prequantization,” Lect. Notes Math. 836 (1980), 67-90 doi:10.1007/BFb0089727 [70] T. T. Wu and C. N. Yang, “Some solutions of the classical isotopic gauge field equations,” in Properties of Matter under Unusual Conditions. Festschrift for the 60th birthday of E. Teller. p. 349. Ed. H. Mark and S. Fernbach. Interscience: (1969). [71] G. ’t Hooft, “Magnetic monopoles in unified gauge theories,” Nucl. Phys. B79 (1974) 276 https://doi.org/10.1016/0550-3213(74)90486-6 [72] A. M. Polyakov, “Particle Spectrum in Quantum Field Theory,” JETP Lett. 20 (1974), 194-195 PRINT-74-1566 (LANDAU-INST). [73] L. F. Abbott and S. Deser, “Charge Definition in Nonabelian Gauge Theories,” Phys. Lett. B 116 (1982), 259-263 doi:10.1016/0370-2693(82)90338-0 [74] R. Jackiw and N. S. Manton, “Symmetries And Conservation Laws In Gauge Theories,” Annals Phys. 127 (1980) 257. [75] A. S. Schwarz, “On Symmetric Gauge Fields,” Commun. Math. Phys. 56 (1977), 79-86 doi:10.1007/BF01611118 [76] P. Forg´ acs and N. S. Manton, “Space-Time Symmetries in Gauge Theories,” Commun. Math. Phys. 72 (1980), 15 doi:10.1007/BF01200108 [77] J. P. Harnad, L. Vinet and S. Shnider, “Group Actions on Principal Bundles and Invariance Conditions for Gauge Fields,” J. Math. Phys. 21 (1980), 2719 doi:10.1063/1.524389 [78] R. Jackiw, “Invariance, Symmetry and Periodicity in Gauge Theories,” Acta Phys. Austriaca Suppl. 22 (1980), 383-438 PRINT-80-0254 (SANTA-BARBARA,ITP). [79] J. W. van Holten, “Covariant Hamiltonian dynamics,” Phys. Rev. D 75 (2007), 025027 doi:10.1103/PhysRevD.75.025027 [arXiv:hep-th/0612216 [hep-th]]. [80] P. A. Horvathy and J. P. Ngome, “Conserved quantities in non-abelian monopole fields,” Phys. Rev. D 79 (2009), 127701 doi:10.1103/PhysRevD.79.127701 [arXiv:0902.0273 [hep- th]]. [81] R. Jackiw and C. Rebbi, “Spin from Isospin in a Gauge Theory,” Phys. Rev. Lett. 36 (1976), 1116 doi:10.1103/PhysRevLett.36.1116 [82] P. Hasenfratz and G. ’t Hooft, “A Fermion-Boson Puzzle in a Gauge Theory,” Phys. Rev. 41Lett. 36 (1976), 1119 doi:10.1103/PhysRevLett.36.1119 [83] X. S. Chen, X. F. Lu, W. M. Sun, F. Wang and T. Goldman, “Spin and orbital angular momentum in gauge theories: Nucleon spin structure and multipole radiation revisited,” Phys. Rev. Lett. 100 (2008), 232002 doi:10.1103/PhysRevLett.100.232002 [arXiv:0806.3166 [hep-ph]]. [84] J.-M. Souriau, Structure des syst` emes dynamiques, Dunod (1970). Structure of Dynamical Systems. A Symplectic View of Physics , Birkh¨ auser, Boston (1997). [85] C. G. J. Jacobi, “Vorlesungen ¨ uber Dynamik.” Univ. K¨ onigsberg 1842-43. Herausg. A. Cleb- sch. Zweite ausg. C. G. J. Jacobi’s Gesammelte Werke. Supplementband. Herausg. E. Lottner. Berlin Reimer (1884). An english translation is available as Jacobi’s Lectures on Dynamics , 2nd edition, Texts and Readings in Mathematics , https://doi.org/10.1007/978-93-86279-62- 0, Hindustan Book Agency Gurgaon. [86] P. A. Horvathy, “THE WU-YANG FACTOR AND THE NONABELIAN AHARONOV- BOHM EXPERIMENT,” Eur. Phys. Lett. 2 (1986), 195 doi:10.1209/0295-5075/2/3/005 [87] P. A. Horvathy and J. Kollar, “The Nonabelian Aharonov-Bohm Effect in Geometric Quan- tization,” Class. Quant. Grav. 1 (1984), L61 doi:10.1088/0264-9381/1/6/002. [88] P. A. Horvathy and J. Kollar, “AHARONOV-BOHM EFFECT IN SU(N) GAUGE THE- ORY,” BI-TP-82-1A. (inspire code Horvathy:1982sj). In Proc. Int. Conf.Monopoles in Quan- tum Field Theory , Trieste, dec. 1981 ed. Craigie, P. Goddard, W. Nahm (eds) (1982), pp. 277-278. [89] R. Sundrum and L. J. Tassie, “Nonabelian Aharonov-Bohm Effects, Feynman Paths, and Topology,” J. Math. Phys. 27 (1986), 1566-1570 doi:10.1063/1.527067 [90] H. Arodz, “Colored, Spinning Classical Particle in an External Nonabelian Gauge Field,” Phys. Lett. B 116 (1982), 251-254 doi:10.1016/0370-2693(82)90336-7; [91] H. Arodz, “A Remark on the Classical Mechanics of Colored Particles,” Phys. Lett. B 116 (1982), 255-258 [92] : H. Arodz, “LIMITATION OF THE CONCEPT OF THE CLASSICAL COLORED PAR- TICLE,” Acta Phys. Polon. B 14 (1983), 13-21 [93] L. G. Feh´ er, “Bounded Orbits for Classical Motion of Colored Test Particles in the Prasad- Sommerfield Monopole Field,” Acta Phys. Polon. B 15 (1984), 919 Print-84-0247 (BOLYAI). [94] L. G. Feh´ er, “Quantum Mechanical Treatment of an Isospinor Scalar in Yang-Mills Higgs 42Monopole Background,” Acta Phys. Polon. B 16 (1985), 217 PRINT-84-0552 (BOLYAI- INST). [95] B. S. Deaver, Jr. , and W. M. Fairbank, “Experimental evidence for quantized flux in super- conducting cylinders,” Phys. Rev. Letters 7, 43 (1961) [96] N. Byers and C. N. Yang, “Theoretical Considerations Concerning Quantized Mag- netic Flux in Superconducting Cylinders,” Phys. Rev. Lett. 7 (1961), 46-49 doi:10.1103/PhysRevLett.7.46 [97] L. Onsager, “Magnetic Flux Through a Superconducting Ring,” Phys. Rev. Lett. 7, 50 (1961) DOI:https://doi.org/10.1103/PhysRevLett.7.50 [98] R. Jackiw and E. J. Weinberg, “SELFDUAL CHERN-SIMONS VORTICES,” Phys. Rev. Lett. 64 (1990), 2234 doi:10.1103/PhysRevLett.64.2234 [99] R. Jackiw and S. Y. Pi, “Classical and quantal nonrelativistic Chern-Simons the- ory,” Phys. Rev. D 42 (1990), 3500 [erratum: Phys. Rev. D 48 (1993), 3929] doi:10.1103/PhysRevD.42.3500 [100] G. V. Dunne, R. Jackiw, S. Y. Pi and C. A. Trugenberger, “Selfdual Chern-Simons solitons and two-dimensional nonlinear equations,” Phys. Rev. D 43 (1991), 1332 [erratum: Phys. Rev. D 45 (1992), 3012] doi:10.1103/PhysRevD.43.1332 [101] G. V. Dunne, “Selfdual Chern-Simons theories,” Lect. Notes Phys. M 36 (1995), 1-217 1995, doi:10.1007/978-3-540-44777-1 [102] A. Frenkel and P. Hrasko, “Invariance Properties of the Dirac Monopole,” Annals Phys. 105 (1977), 288 doi:10.1016/0003-4916(77)90242-1 [103] P. Hrasko and J. Balog, “Rotation Symmetry in the Hamiltonian Dynamics,” Nuovo Cim. B 45 (1978), 239-254 doi:10.1007/BF02894683 [104] P. Goddard, J. Nuyts and D. I. Olive, “Gauge Theories and Magnetic Charge,” Nucl. Phys. B 125 (1977), 1-28 doi:10.1016/0550-3213(77)90221-8 [105] P. A. Horvathy and J. H. Rawnsley, “Topological Charges in Monopole Theories,” Commun. Math. Phys. 96 (1984), 497 doi:10.1007/BF01212532 [106] David L. Chandler, “Exotic physics phenomenon is observed for first time. Observation of the predicted non-Abelian Aharonov-Bohm Effect may offer step toward fault-tolerant quantum computers,” https://news.mit.edu/2019/aharonov-bohm-effect-physics-observed-0905 Sept. 5 (2019). 43",
      "references": [
        "Significance of electromagnetic potentials in the quantum the- ory,",
        "Shift of an Electron Interference Pattern by Enclosed Magnetic Flux,",
        "Messung der kontinuierlichen Phasenschiebung von Elektro- nenwellen im kraftfeldfreien Raum durch das magnetische vektorpotential einer Luftspule,",
        "Experimental confirmation of Aharonov-Bohm effect using a toroidal magnetic field confined by a superconductor,",
        "The Aharonov-Bohm Effect. Part 1: Theory,",
        "Applications of electron holography",
        "The Aharonov-Bohm Effect. Part 2: Experiment,",
        "The Aharonov-Bohm effects: Variations on a subtle theme,",
        "Concept of Nonintegrable Phase Factors and Global Formulation of Gauge Fields,",
        "Approximate topologies",
        "Techniques and applications of path integration",
        "Selected topics in topology and quantum field theory,",
        "Inequivalent quantizations in multiply connected spaces,",
        "Quantization in Multiply Connected Spaces,",
        "Prequantization From Path Integral Viewpoint,",
        "Foundations of Differential Geometry Vols. I and II,",
        "Fibre Bundles . 3rd edition Graduate Texts in Mathematics.",
        "Conservation of Isotopic Spin and Isotopic Gauge Invariance,",
        "Search for unorthodox phenomena by neutron interference experiments,",
        "The non-Abelian Aharonov-Bohm effect,",
        "FIELD THEORIES WITH NO LOCAL CONSERVATION OF THE ELEC- TRIC CHARGE,",
        "Global Color Is Not Always Defined,",
        "Monopole Topology and the Problem of Color,",
        "What Becomes of Global Color,",
        "Internal Symmetries of Nonabelian Gauge Field Con- figurations,",
        "The Problem of ‘Global Color’ in Gauge Theories,",
        "The Interactions and Excitations of Nonabelian Vortices,",
        "SPACE-TIME APPROACH TO HOLONOMY SCATTERING,",
        "Discrete Quantum Hair on Black Holes and the Nonabelian Aharonov-Bohm Effect,",
        "Local Discrete Symmetry and Quantum Mechanical Hair,",
        "Topological defects and structure formation,",
        "Realization of magnetic monopole gauge fields : diatoms and spin precession,",
        "Angular momentum for diatoms described by gauge fields",
        "Kerner equation for motion in a non-Abelian gauge field,",
        "Time-dependent non-Abelian Aharonov-Bohm effect,",
        "Non-Abelian Aharonov–Bohm effect with the time- dependent gauge fields,",
        "Aharonov-Bohm Phase in High Density Quark Matter,",
        "Differential scattering cross section of the non-Abelian Aharonov-Bohm effect in multiband systems,",
        "Cold Atoms in Non-Abelian Gauge Potentials: From the Hofstadter ‘Moth’ to Lattice Gauge Theory,",
        "Light-induced gauge fields for ultracold atoms,",
        "Cold atom dynamics in non-Abelian gauge fields,",
        "Colloquium: Artificial gauge potentials for neutral atoms,",
        "Non-Abelian gauge field optics,",
        "Synthesis and Observation of Non-Abelian Gauge Fields in Real Space,",
        "Dualities and non-Abelian mechanics,",
        "Non-Abelian gauge fields in circuit systems,",
        "Non-Abelian physics in light and sound,",
        "Circuit realization of topological physics,",
        "New Developments in the Theory of Magnetic Monopoles,",
        "Some Remarks on the Classical Vacuum Structure of Gauge Field Theories,",
        "REGULARITY OF GAUGE EQUIVALENCE IN QUANTUM MECHAN- ICS AND THE AHARONOV-BOHM EFFECT,",
        "Generalization of the Kaluza-Klein Theory for an Arbitrary Nonabelian Gauge Group,",
        "Field and particle equations for the classical Yang-Mills field and particles with isotopic spin,",
        "Fiber bundles associated with space-time,",
        "Higher - Dimensional Unifications of Gravitation and Gauge Theories,",
        "Search for a Realistic Kaluza-Klein Theory,",
        "Classical De- scription of Particle Interacting with Nonabelian Gauge Field,",
        "Lagrangian and Hamiltonian Descriptions of Yang-Mills Particles,",
        "Minimal Coupling and the Symplectic Mechanics of a Classical Parti- cle in the Presence of a Yang-Mills Field,",
        "On the Equations of Motion of a Classical Particle in a Yang-Mills Field and the Principle of General Covariance,",
        "On the prequantum description of spinning particles in an external gauge field,",
        "Sur les mouvements classiques dans un champs de Yang-Mills,",
        "Particles With Internal Structure: The Geometry of Clas- sical Motions and Conservation Laws,",
        "NONRELATIVISTIC YANG-MILLS PARTICLES IN A SPHERICALLY SYMMETRIC MONOPOLE FIELD,",
        "Classical motion of coloured test particles along geodesics of a Kaluza-Klein spacetime,",
        "Classical Action, the Wu-Yang Phase Factor and Prequantization,",
        "Some solutions of the classical isotopic gauge field equations,",
        "Magnetic monopoles in unified gauge theories,",
        "Particle Spectrum in Quantum Field Theory,",
        "Charge Definition in Nonabelian Gauge Theories,",
        "Symmetries And Conservation Laws In Gauge Theories,",
        "On Symmetric Gauge Fields,",
        "Space-Time Symmetries in Gauge Theories,",
        "Group Actions on Principal Bundles and Invariance Conditions for Gauge Fields,",
        "Invariance, Symmetry and Periodicity in Gauge Theories,",
        "Covariant Hamiltonian dynamics,",
        "Conserved quantities in non-abelian monopole fields,",
        "Spin from Isospin in a Gauge Theory,",
        "A Fermion-Boson Puzzle in a Gauge Theory,",
        "Spin and orbital angular momentum in gauge theories: Nucleon spin structure and multipole radiation revisited,",
        "Structure des syst` emes dynamiques,",
        "Structure of Dynamical Systems. A Symplectic View of Physics ,",
        "Vorlesungen ¨ uber Dynamik.",
        "Jacobi’s Lectures on Dynamics ,",
        "THE WU-YANG FACTOR AND THE NONABELIAN AHARONOV- BOHM EXPERIMENT,",
        "The Nonabelian Aharonov-Bohm Effect in Geometric Quan- tization,",
        "AHARONOV-BOHM EFFECT IN SU(N) GAUGE THE- ORY,",
        "Nonabelian Aharonov-Bohm Effects, Feynman Paths, and Topology,",
        "Colored, Spinning Classical Particle in an External Nonabelian Gauge Field,",
        "A Remark on the Classical Mechanics of Colored Particles,",
        "LIMITATION OF THE CONCEPT OF THE CLASSICAL COLORED PAR- TICLE,",
        "Bounded Orbits for Classical Motion of Colored Test Particles in the Prasad- Sommerfield Monopole Field,",
        "Quantum Mechanical Treatment of an Isospinor Scalar in Yang-Mills Higgs Monopole Background,",
        "Experimental evidence for quantized flux in super- conducting cylinders,",
        "Theoretical Considerations Concerning Quantized Mag- netic Flux in Superconducting Cylinders,",
        "Magnetic Flux Through a Superconducting Ring,",
        "SELFDUAL CHERN-SIMONS VORTICES,",
        "Classical and quantal nonrelativistic Chern-Simons the- ory,",
        "Selfdual Chern-Simons solitons and two-dimensional nonlinear equations,",
        "Selfdual Chern-Simons theories,",
        "Invariance Properties of the Dirac Monopole,",
        "Rotation Symmetry in the Hamiltonian Dynamics,",
        "Gauge Theories and Magnetic Charge,",
        "Topological Charges in Monopole Theories,",
        "Exotic physics phenomenon is observed for first time. Observation of the predicted non-Abelian Aharonov-Bohm Effect may offer step toward fault-tolerant quantum computers,"
      ],
      "meta_data": {
        "arxiv_id": "2402.13883v3",
        "authors": [
          "Peng-Ming Zhang",
          "Peter Horvathy"
        ],
        "published_date": "2024-02-21T15:50:56Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Studies the non-Abelian Aharonov–Bohm (AB) setup of Wu–Yang for flat SU(2) Yang–Mills connections on the punctured plane and shows that, unlike the Abelian case, there is a genuine (pseudo)classical effect: the internal isospin of a Wong/Kerner particle precesses even though spatial motion is free. Interprets the Wu–Yang holonomy factor as a (pseudo)classical S-matrix acting on isospin, yielding side-dependent “scattering” unless the flux is quantized so that the holonomy lies in the center (F=±I). Establishes that internal SU(2) charges/symmetries exist only for quantized flux and split into two inequivalent series: even N (trivial implementation) and odd N (twisted implementation), with distinct symmetry realization and conservation laws. Clarifies proton/neutron identification via gauge-covariant charge projections onto covariantly constant direction fields, explaining when proton↔neutron conversion can occur (unquantized flux) and when it cannot (quantized flux). Analyzes angular momentum: orbital and internal parts are separately conserved only in quantized cases; for odd flux the doubled rotational symmetry reduces to SO(2)_ext×SO(2)_int. Re-derives conserved quantities using van Holten’s covariant Hamiltonian algorithm as an independent check.",
        "methodology": "Uses classical Yang–Mills test-particle dynamics with internal degrees of freedom via Kerner–Wong equations: free spatial motion outside the flux tube (F_{ij}=0) plus parallel transport of the isospin Q along trajectories. Models the non-Abelian AB field as a flat connection A on R^2\\{0} with holonomy (Wu–Yang phase factor) computed via path-ordered exponentials; works largely in a diagonal gauge A_\\vartheta∝σ_3. Connects gauge-equivalence classes of flat potentials to holonomy up to conjugation. Develops a gauge-covariant charge notion by introducing adjoint “direction fields” \\hat\\Psi and applying an internal-symmetry/Noether framework (implementability, covariantly constant fields D_\\mu\\hat\\Psi=0, centralizer/holonomy criterion) to determine when ordinary conserved charges exist. Computes isospin evolution explicitly as rotations in isospace generated by the holonomy for paths in the two half-planes (sign of orbital L_z). Classifies trivial vs twisted implementations of SU(2) actions on the gauge potential and uses them to derive charge conservation and symmetry breaking patterns. Independently applies van Holten’s algorithm (expansion in covariant momenta Π_i) to solve for first integrals and recover the quantization conditions a∈Z and the two series via matching across half-planes with possible center-valued transition functions.",
        "experimental_setup": "No numerical experiments or datasets; the work is analytical/theoretical. The physical setup is an idealized 2D scattering of an isospin-carrying nonrelativistic particle by a thin flux tube confining non-Abelian magnetic flux, with field strength vanishing outside (flat connection) and nontrivial holonomy around the puncture. Validation is by internal consistency checks across methods: (i) explicit solution of Wong parallel-transport equation for Q(\\vartheta) in the diagonal gauge, (ii) holonomy-based interpretation of Q_out=F·Q_in, (iii) Noether/internal-symmetry analysis using implementability and covariantly constant direction fields, and (iv) re-derivation of the same conserved quantities and flux quantization conditions using van Holten’s covariant Hamiltonian algorithm. The paper situates results relative to prior theoretical literature and notes an existing neutron-interference bound (Zeilinger et al.) and modern platforms for synthetic non-Abelian gauge fields, but does not perform new experimental comparisons.",
        "limitations": "Restricted to a test-particle (probe) approximation with prescribed external Yang–Mills potential; backreaction, radiation, and full field-theoretic consistency (especially regarding electric charge exchange with the source) are not modeled. Primarily nonrelativistic and planar (punctured plane) with an idealized infinitely thin flux line and strictly flat connection outside; real vortices/finite-width cores and time-dependent fields are not treated. Focuses on SU(2) gauge group; conclusions about flux quantization and twisted implementations depend on the global structure/center (differences would arise for SO(3) or larger groups). The “direction field” used to define proton/neutron is nondynamical (Higgs-like measuring frame) and its physical realization is not derived from a dynamical Higgs sector. For unquantized flux, internal symmetries/charges fail globally; the framework indicates proton↔neutron conversion but cannot resolve the physical mechanism for apparent electric-charge nonconservation, suggesting the model may be insufficient without embedding into a larger gauge theory (e.g., electroweak U(2)) or including source dynamics. Scattering is treated in a classical/isospin-precession sense; no differential cross-sections or quantum interference patterns are computed in this work.",
        "future_research_directions": "Embed the analysis in a fully dynamical field theory including the source of the flux and electromagnetic/electroweak interactions to address charge-transfer/charge-conservation issues (e.g., extend to U(2) or Weinberg–Salam and model how ‘proton/neutron’ labels emerge). Study finite-core non-Abelian vortices (Yang–Mills–Higgs or Chern–Simons–Higgs) whose outside region approximates the flat connection but with controlled core physics; connect holonomy quantization to topological vortex data. Develop the quantum counterpart of the pseudoclassical S-matrix/holonomy action on internal states, including interference and cross sections, and compare to existing non-Abelian AB scattering literature in condensed-matter multiband systems. Generalize to other gauge groups and representations (SU(N), non-simply connected groups) to map how center structure controls twisted implementations, charge conservation, and symmetry breaking. Explore time-dependent/nonadiabatic generalizations and relation to time-reversal symmetry breaking claims. Propose concrete laboratory realizations using synthetic non-Abelian gauge fields (cold atoms, photonics, circuits, acoustics) where internal-state precession and holonomy can be measured directly, including protocols to detect side-dependence and the two flux-quantization series.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
      "full_text": "Delegated Classification Eden Saig, Inbal Talgam-Cohen, Nir Rosenfeld Technion – Israel Institute of Technology Haifa, Israel {edens,italgam,nirr}@cs.technion.ac.il Abstract When machine learning is outsourced to a rational agent, conflicts of interest might arise and severely impact predictive performance. In this work, we propose a theoretical framework for incentive-aware delegation of machine learning tasks. We model delegation as a principal-agent game, in which accurate learning can be incentivized by the principal using performance-based contracts. Adapting the eco- nomic theory of contract design to this setting, we define budget-optimal contracts and prove they take a simple threshold form under reasonable assumptions. In the binary-action case, the optimality of such contracts is shown to be equivalent to the classic Neyman-Pearson lemma, establishing a formal connection between contract design and statistical hypothesis testing. Empirically, we demonstrate that budget- optimal contracts can be constructed using small-scale data, leveraging recent advances in the study of learning curves and scaling laws. Performance and eco- nomic outcomes are evaluated using synthetic and real-world classification tasks. 1 Introduction The acclaimed success of machine learning at effectively solving difficult prediction tasks across diverse problem domains has made it highly appealing for firms, institutions, and individual practi- tioners. But machine learning has also become increasingly complex, cumbersome, and difficult to operate—and not all those who seek to learn have access to the necessary expertise, infrastructure, and designated resources required for learning effectively. This gap has created a new market for outsourced machine learning, in which a client interested in obtaining an accurate predictive model can hire the services of a specialized provider which, for a price, trains the model on their behalf. Consider for example a hospital purchasing a classifier for deciding between hospitalization and outpatient treatment when triaging patients. The provider invests in curating, cleaning and annotating training data, and delivers a trained model in return to payment from the hospital. Having a budget to expend on outsourced learning [51], we model the client as aiming to obtain the best possible predictive model. At first glance, it is tempting to assume that the optimal strategy is simply to pay the provider the maximal feasible amount—and hope to get a high-end model in return. After all, if the client were to spend the budget directly on learning, investing the maximal available sum would yield the best possible results. But this neglects to account for the incentives of the provider, who is interested in maximizing profit. Since the actions of the provider remain private, it is in his best interest to (secretly) minimize efforts, which in turn can result in his delivering a suboptimally-trained model. In our example, the provider can cut costs by annotating only a subset of the data, obtaining cheaper low-quality annotations, or neglecting to meticulously remove all outliers. Outsourced learning is hence susceptible to moral hazard, an economic situation which might occur under information asymmetry, and to the detriment of the client. Motivated by this observation, in this paper we initiate the study of delegated learning, and aim to explore the economic, algorithmic, and statistical implications that occur when learning is delegated to a specialized provider. Our key novelty is in instantiating delegated learning as a problem ofoptimal contract design [10, 42, 47, 48]. Broadly, contracts are an important monetary device that allows the client to establish a payment scheme 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.11475v2  [cs.LG]  5 Dec 2023Observe initial data; Design contract Strategically choose training set size !Sample data at cost \"!, and learn ℎSample validation set $∼&\"Count correct predictions '=)⋅acc#ℎ Private PrivateInteraction Classifier ℎ- Principal Agent Receive .'monetary units Contract .:0,…,)→ℝ$% Payment .' Figure 1: Delegated classification interaction sequence. The principal examines initial information, and designs a contract t : {0, . . . , m} →R≥0. Having observed t, the agent strategically selects a dataset size n that will maximize his expected utility. He samples a training set S ∼ Dn, incurs cost cn, then trains the classifier h ∈ Hand sends it to the principal. Upon receiving h, the principal evalu- ates its accuracy on a random validation set V ∼ Dm, and pays the agent according to the contract t. which, if properly set, serves to align incentives and guarantee that both parties are well-off. Our main challenge is to design effective contracts specialized to the task of delegated learning on a budget. Towards this, we begin with a conventional supervised classification setup, and impose economic structure by assuming that acquiring training examples is costly. We then conceptually “split” the conventional self-sufficient learner into two rational entities: a principal, who controls the budget and is interested in maximizing predictive accuracy; and an agent, who controls learning (in particular the training set) and is interested in maximizing profit. This allows us to model principal-agent relations as a Stackelberg game, in which the principal commits to acontract t, determining a priori the amount to be paid for every possible (stochastic) level of obtained accuracy. The agent best-responds to the contract by choosing the profit-maximizing number of samples n, and training the predictive model. Under this setting, we study the algorithmic problem of designing an optimal contract. As is standard in economic analysis, we begin with the assumption that the principal has full information on the distribution of possible outcomes for each of the agent’s possible actions. In our setting, actions correspond to the number of training samples, and outcomes to the empirical classifier accuracy; thus, the main object of interest for contract design in delegated learning settings is the learning curve, which describes the (stochastic) performance of learning per sample size. Under certain plausible conditions on the learning curve, namely MLRP and a certain notion of concavity, our main result here is that optimal contracts are simple, and in particular, take on the form of simple threshold functions. Simple contracts are appealing because they are straightforward to understand and communicate; in our setting, they are also easy to compute, and we give a closed-form solution for the optimal threshold contract. Providing an interpretation of the closed-form solution, our findings establish a new connection between contracts and the renowned Neymon-Pearson lemma [ 45], which we consider as one of our central theoretical contributions. We then switch gears and turn to empirically studying the construction of contracts from partial information. In particular, we consider a setting where the principal can only estimate the learning curve from small available data (e.g., by bootstrapping on small n and extrapolating). Using the recent LCDB dataset of learning curves [43], we show that threshold contracts generally perform well on estimated curves despite the inherent uncertainty. We also explore the role of different parameters of our setup, consider various tradeoffs in curve-fitting and contract design, and discuss limitations by pointing out certain failure modes to which contracts may be susceptible. Taken together, our results shed light on why and how simple contracts for delegated learning work to correctly balance between the incentives of both delegator and delegatee in outsourced learning. 1.1 Related work Previous works have considered delegation of ML-related tasks that differ from our task of training a classifier: labeling of data points in [ 14], gathering information in [16], and computing a costly high-dimensional function in [5]. In the delegated task of [24], the agent provides a classifier and the principal verifies its near-optimality withinH. A fundamental difference is that their agent is assumed to be adversarial rather than rational, and so interactive proofs are used instead of economic incentives. The Neyman-Pearson lemma has recently been connected to economic design by [8] in the context of adverse selection rather than moral hazard. The agent has a hidden type (e.g., whether a new 2drug is effective), and the optimal menu to offer this agent is designed based on the theory of e-values. When the hidden type has binary support, the method is equivalent to Neyman-Pearson. A “moral” link between the design of contracts (for a non-budgeted principal) and statistical inference (in particular likelihood ratios) was observed already in [ 25], but no connection was made to the power of hypothesis tests. Other intersections of ML and contracts that do not involve delegation of learning-related tasks include strategic classification [e.g., 36, 37, 3] and online learning of optimal contracts [29, 17, 52] Contract settings with a binary action and/or outcome space have been studied in [e.g., 20, 7, 22]. Not to be confused with our notion of delegation, there is a growing computational literature on delegation without monetary payments [e.g., 35]. To extrapolate from partial data, our work builds upon recent advancements in the study of learning curves, which characterize the expected generalization of learning as a function of dataset size and other exogenous factors [49]. There is growing empirical evidence that performance of modern neural networks can be predicted using simple scaling laws [e.g., 34, 41, 50, 23, 2, 46, 30], and theoretical results that back these findings in simplified settings [11, 12, 33, 9]. Perhaps closest to ours is the concurrent work [4], which analyzes a similar setting under different assumptions: Rather than assuming budget constraints, they assume that the principal’s utility is linear in accuracy and payout, and the learning curve takes a specific functional form. Based on these assumptions, they derive approximately optimal linear contracts which are robust to adverse selection. In contrast, we obtain globally optimal simple contracts based on hypothesis testing, and present data-driven methods to handle partial information. Together, the two studies demonstrate the important role of simple contracts in the growing ecosystem of machine learning delegation. 2 Problem Setup The core of our setting is based on a standard supervised classification task. Let x ∈ Xbe features and y ∈ Ybe labels, and assume there is some unknown joint distribution D over (x, y) pairs. Given a sample set S = {(xi, yi)}n i=1 ∼ Dn, the goal in learning is to use S to find a classifier h : X → Y from a class H that maximizes expected accuracy, accD(h) = P(x,y)∼D[h(x) = y]. Because the underlying distribution D is unknown, expected performance is estimated by the empirical average on an additional held-out validation set V ∼ Dm of size m, as accV (h) = 1 m Pm i=1 1 [h(xi) = yi], which is a consistent and unbiased estimator of accD(h). We will assume throughout that both the learning algorithm and the validation set size m are known and fixed. Learning on a budget. We will be interested in studying learning when certain resources are limited or costly. Our main focus will be on the setting where the main cost of learning is the number of labeled examples n, but we note that our approach can in principle extend to other forms of learning ‘effort’.1 We assume the learner has a monetary budget B to spend on samples, and is interested in maximizing accuracy under budget constraints. Let cn ≥ 0 be the cost of n samples (assumed to be increasing in n), then the learner aims to solve: n∗ = argmaxn Ehn[accD(hn)] s.t. cn ≤ B (1) where hn is a classifier learned from a random dataset of size |S| = n. Note that hn is a ran- dom variable with distribution depending on n. We denote the out-of-sample accuracy of hn by αn = accD(hn). When the learner is a self-sufficient entity, and when the expected αn improves monotonically in n, then n∗ in Eq. (1) in naturally the largest affordable n (see Sec. 3). However, as we will see, when learning is delegated—this seemingly straightforward observation can break. Delegation. We model the delegation of learning as a conceptual partition of the learner into two distinct entities: an agent, who controls learning; and a principal, who controls the validation process. The principal outsources the learning task to the agent, who in turn uses the training set S to train the classifier h; once delivered, the principal validates the performance of h using the validation set V . Whereas the classifier’s accuracy benefits the principal alone, the cost of learning (i.e., the cost of acquiring S) is born exclusively by the agent. Importantly, the amount of invested effort remains private to the agent; in our example, the principal cannot know how many examples received quality labeling. Because the agent seeks to maximize profit, the principal can use her budget as a source of monetary payment to incentivize the agent to invest in larger|S| = n. Intuitively, one could expect 1For example, [34] argue that not only training-set size, but also computation time and model size are related to accuracy through scaling laws, which have tight connections to the assumptions we discuss in Sec. 3. 30.0 0.5 1.0 Outcome (validation-set accuracy) Action n (training set size) 16 32 64 128 256 512 1024 2048 4096 8192 Outcome distributions fn Delegated classiﬁcation setting 0.00 0.25 0.50 0.75 1.00 Outcome (validation-set accuracy) 0 200 400 600 800 1000Payment t(j) Diﬀerent contracts with identical budget constant linear threshold 100 1000 Action n (training set size) 0.5 1.0 Utility un(t) Agent’s perspective: Maximize utility constant linear threshold Contract 0.0 0.5 E[accD(hn(t))] Principal’s perspective: Accuracy 101 102 103 Costcn(data acquisition cost) Figure 2: A delegated classification setting (data from Sec. 4). (Left) Each costly action taken by the agent (training set size n) induces a distribution fn of possible outcomes (classifier accuracy). The principal seeks to construct a contract t that incentivizes a profit-maximizing agent to take actions entailing favorable outcomes. Note thefn exhibit increasing expectation, but decreasing variance, inn. (Center) Three contracts for a given budget B, mapping outcomes to payments. (Top-right) Agent’s utilities un(t) and best responses n(t) (stars) for each contract t. (Bottom-right) Expected accuracies for principal resulting from each contract; here the threshold contract is optimal (see Sec. 3). larger payments to entail larger n, and therefore higher-accuracy h. However, as we will see, this is not always the case, and careful planning is required in order to fully utilize a given budget. 2.1 Delegation as contract design As the training set remains private to the agent, there is an information gap between the two parties. This creates a conflict of interest for the agent known asmoral hazard [10], in which the agent may be tempted to invest sub-par effort, while claiming that efforts were in fact his honest best. In economics, the celebrated solution to moral hazard are contracts [31]: pay-per-performance rules that a-priori determine future payments for every possible outcome, which we formally describe next. Contract design. A contract setting is defined by a set of actions A = {a1, . . . , aN } that can be taken by the agent, and a set of possible outcomes j ∈ {0, . . . , m}. Each action ai is associated with a cost ci, and w.l.o.g. we assume c1 ≤ ··· ≤cN so that actions correspond to increasing effort levels. The agent’s choice to perform action a ∈ Ayields a random outcome j ∼ fa for the principal, where fa describes a distribution over the possible outcomes associated with action a. The principal, who does not observe the agent’s chosen action, can incentivize the agent through acontract, t : {0, . . . , m} →R≥0, according to which she pays the agent t(j) ≥ 0 when the materialized outcome is j. Given contract t, let ua(t) be the agent’s expected utility from taking action a ∈ Aat cost ca (via stochastic outcomes j ∼ fa), and let a(t) be the agent’s best response—an action that maximizes his expected utility (and following standard tie-breaking assumptions as in [18]). Then: ua(t) = Ej∼fa[t(j)] − ca, a (t) ∈ argmaxa∈Aua(t). (2) Every action a∗ that is the best response a∗ = a(t) to some contract t is called implementable. In economic terms, the principal and agent are playing a Stackelberg game, in which the principal commits to a contract t and the agent best-responds by choosing action a(t) that maximizes his expected utility ua(t). The goal of the principal is to design a contract t which incentivizes the agent to take best-response actions yielding favorable outcomes for the principal. Contracts for delegated learning. We propose to formulate delegated learning as a problem of optimal contract design, instantiated as follows. First, we relate agent actions a with the number of samples n, and denote A = {n1, . . . , nN } as the possible sizes of S that the learning agent can work with. The cost of acquiring samples naturally maps as ca = cn, and agent’s best response is a(t) = n(t). Next, we associate outcomes j with accuracy for the principal by defining j as the number of validation samples (out of the possible m) on which h is correct; note this implies accV (h) = j/m, and we will therefore use j and accV (h) as ‘outcomes’ interchangeably. Finally, for an action n, we set fn to be the distribution over possible accuracies obtained when learning with n samples, namely fn(j) = Phn,V [accV (hn) = j/m] ∀j. We will also use the matrix form Fnj = fn(j), where F ∈ [0, 1]N×(m+1). Note that F admits two sources of variation: (i) a-priori variation in hn due to stochasticity in S ∼ Dn; and (ii) a-posteriori variation in j for any fixed hn 4due to stochasticity in V ∼ Dm. When hn is fixed, the outcome distribution admits a simple binomial form, namely j ∼ Binomial(m, αn). Empirically, we observe this to be the dominant component. 2.2 Delegation as an optimization problem Budget-optimal contracts. Recall that the principal seeks to maximize accuracy under budget constraints (Eq. (1)). Once learning is delegated to an agent and framed as a contract design problem, the principal’s objective becomes: t∗ = argmaxt∈[0,B]m Ehn(t) [accD(hn(t))] (3) Contract t∗ is chosen to incentivize the agent to invest effort n(t) (via eq. (2)) such that the training of hn(t) yields high dividends for the principal in terms of expected accuracy. We will refer to t∗ as a budget-optimal contract, and to the general task of finding t∗ as budget-optimal contract design. Information structure. Delegated learning settings have actions n and costs cn known to both sides, and outcome distribution Fnj known to the agent. For the principal, we explore varying levels of knowledge: In Sec. 3, we assume (as in the classic contract design literature) that the principal has full information of F (i.e., knows the learning curve), and focus on characterizing the optimal contract. In Sec. 4 we relax this assumption, and explore a partial-information setting in which the principal relies instead on an empirically-estimated curves ˆF. 3 Budget-Optimal Contract Design 3.1 The problem Why agents cut corners. The conceptual challenge in designing contracts lies in that agents cannot reliably report what they did. For example, consider a principal who, after delegation, received a classifier attaining 0.74 (validation) accuracy. Should she be happy? The crux is that there are two ways that this could have happened: (i) the agent invested high effort in learning (largen), but received an uninformative S by chance, and delivered a low-quality h as a result; and (ii) the agent invested low effort (small n). Since the agent’s actions are private, and because outcomes are stochastic, the principal can never know for certain which is the true underlying cause. In other words, a ‘lazy’ (or rather strategic) agent can hide behind the uncertainty that is inherent in learning outcomes.2 Contract types. To overcome this informational gap, the principal must devise a contract to align in- centives and encourage the agent to prefer certain actions over others. But not all contracts are equally effective. Fig. 2 illustrates for a budget B three contract types and their economic implications: • Constant contract (t(j) = B): The agent is paid B regardless of the outcome. His best-response in this case is to choose the least-costly action—to the detriment of the principal. • Linear contract (t(j) = Bj/m): The agent is paid a fraction of B, linear in the resulting accuracy. Linear contracts are a popular and extensively-studied class of contracts [e.g.,32, 15]. Nonetheless, and though seemingly sensible, linear contracts turn out to be sub-optimal for our setting. • Threshold contract (t(j) = B1 [j ≥ j0] for some j0): The agent is paid B provided the empirical accuracy surpasses a threshold j0. In the example in Fig. 2, the threshold contract is optimal. Rather than committing a-priori to some type of contract, we seek to find the best budget-optimal contract by solving Eq. (3). For this it is useful to have structure. Stochastic learning curves (and where to find them). Our approach uses the observation that there is a tight connection between the set of distributions {fn} encoded in F, and learning curves, which describe the anticipated accuracy of a classifier as a function of the size of its training set. Learning curves typically depict only expected accuracy, but there is also inherent variation in outcomes. We will therefore broadly use the term ‘stochastic learning curve’ to describe both mean trendand variation in accuracy as a function of n; formally, a stochastic learning curve is defined precisely by F. This connection is useful because learning curves have structure: First, expected learning curves are typically monotone [34, 11]; when not [43, 49], they can be monotonized [12]. Second, stochastic learning curves are likely to satisfy the monotone likelihood ratio property (MLRP), which states that the better the performance of a classifier, the more likely it was trained on more data (see Def. 1). 2The agent can also hide in the uncertainty due toV , particularly when m is small; see experiment in Sec. 4.2. 5Table 1: Characterization of simple min-budget contracts in different settings. Simple contract forms include all-or-nothing contracts and their subclass of threshold contracts. The table specifies for each configuration either the simple form that is optimal (in one case through equivalence to the Neyman-Pearson lemma), or that the simple form is non-optimal or intractable. Problem size Structural assumptions Actions Outcomes No assumptions MLRP Concave-MLRP |A| = 2 any size All-or-nothing (T1) Threshold (B.7.1) |A| > 2 m + 1 = 2 All-or-nothing (B.5.1) Threshold (B.7.2) m + 1 > 2 Simple is NP-hard (T3) ∃ non-threshold (B.7.3) Threshold (T4) 3.2 Optimization via min-budget contracts Our main technique for solving Eq. (3) relies on a reduction to what we refer to as min-budget contracts. Given an (implementable) target actionn∗ ∈ A, a min-budget contract forn∗ is a contract t that incentivizes the agent to employ precisely the actionn∗, while minimizing the maximum payment by the principal ∥t∥∞ = maxj∈{0,...,m}{t(j)}; i.e., t implements n∗ at minimum budget. Formally: t∗ = argmint ∥t∥∞ s.t. n(t) = n∗ (4) Our reduction relies on the following claim (Proof in Appendix B.1): Proposition 1. Every budget-optimal contract design problem has an optimal solution which is also min-budget. Using Prop. 1, a solution to Eq. (3) can be obtained by iteratively solving Eq. (4): For allni ∈ A, solve Eq. (4) with target actionn∗ = ni, and return t∗(ni) for the best implementable ni whose budget does not exceed B. The budget-optimal problem thus reduces to solving multiple min-budget problems. To compute each t∗(ni) in Eq. (4), we formulate a novel MIN-BUDGET linear program (LP), 3 detailed in Appx. B.2. One way to solve this LP is with generic solvers—an approach which is valid, but can be costly. One of our contributions is in identifying natural cases where min-budget contracts take on simple forms, which are easier to optimize, and have practical merit. In particular, we show that binary-action contracts have all-or-nothing structure (t(j) ∈ {0, B}), and plausible structural assumptions on the learning curve give rise to threshold contracts (t(j) = B1 [j ≥ j0] for some j0). Our theoretical results are summarized in Table 1, and detailed in the rest of the section. 3.3 All-or-nothing contracts: Binary action space and the statistical connection We begin with a simple delegated learning setting in which the agent can choose one of two actions, A = {n1, n2}, e.g., a ‘small’ vs. ‘large’ training set, and the principal seeks to incentivize training with more data.4 This reduced case will be useful as a building block for the general case (which we re- turn to in Sec. 3.4), and for making a precise connection between contract design and hypothesis tests. Simple min-budget contracts for binary action space. Our first result shows that optimal binary-action contracts are all-or-nothing contracts whose budget is determined by the total variation distance between outcome distributions f2 and f1, namely ∥f2 − f1∥TV = 1 2 Pm j=0 |f2,j − f1,j|. Theorem 1 (Optimal binary-action contract) . In a binary-action contract setting with outcome distributions f1, f2 and costs c1, c2, the min-budget contract is an all-or-nothing contract, given by: t∗(j) = B1 [f2(j) ≥ f1(j)] ∀j ∈ {0, . . . , m}, where B = (c2−c1)/∥f2−f1∥TV. (5) The proof (in Appendix B.4.2) is by LP duality. Intuitively, the optimal contract pays the agent for outcomes that are more likely to come from f2 than from f1. Moreover, it requires a higher budget the smaller the distance is between the two distributions f1, f2. At the extremes, if their distance is 1 (i.e. no overlap among their supports), the required budget for incentivizing n2 is c2 − c1, whereas if their distance is 0 (i.e. f1 = f2) it becomes impossible to incentivize n2. 3The MIN-BUDGET LP is closely related to the well-known MIN-PAY LP from non-budgeted contract design [e.g., 19], but with a different objective, and hence very different optimal solutions (see Appx. B.3). 4When n2 is not the target or when it is not implementable, then the solution is immediate: always pay c1. 6Formal connection to optimal hypothesis testing. Theorem 1 also uncovers a direct corre- spondence between optimal contracts and optimal hypothesis tests. Intuitively, given the outcome distributions {f1, f2}, and in order to incentivize n2, the principal wishes to pay the agent if the observed outcome j ∈ {0, . . . , m} is more likely to have originated from f2. The principal can attempt to identify whether the outcome j is drawn from distribution f1 or f2 through hypothesis testing, where a hypothesis test ψ : {0, . . . , m} → {0, 1} maps a sample j to either f2 (indicated by 1) or to the null hypothesis f1 (indicated by 0). For our purpose it is convenient to allow tests to be non-integral, in which case ψ : {0, . . . , m} →[0, 1] maps j to a probability with which it originates from f2. The quality of a hypothesis test is measured by summing its type-1 and type-2 errors: Pm j=0 f1,jψj + Pm j=0 f2,j(1 − ψj). The test that minimizes this sum is known as the most powerful hypothesis test, and has been characterized by Neyman and Pearson [45, 4.3]. We now turn to formally establishing the connection. For a fixed B, observe that every contract with budget B can be mapped to a hypothesis test via the bijection ψ(j) = t(j)/B. Then: Theorem 2 (Optimal contract vs. test). Consider binary-action contract design with distributions f1, f2 and costs c1, c2. A contract t with budget B is optimal if and only if its corresponding hypothesis test ψ = t/B is maximum power with type-1 and type-2 errors summing to 1 − c2−c1 B . The proof (Appendix B.4.2) is by a non-linear variable transformation to the MIN-BUDGET LP. Theorem 2 implies that the optimal contract for the binary-action case (Theorem 1) is equivalent to the well-known Neyman-Pearson lemma characterizing the most powerful hypothesis test: Lemma 1 (Neyman-Pearson [e.g., 45]). Let f1, f2 be two discrete probability distributions. Then the most powerful hypothesis test for f1, f2 is the likelihood ratio test ψ(j) = 1 [f2(j) ≥ f1(j)], which attains the optimal bound 1 − ∥p − q∥TV on the sum of type-1 and type-2 errors. Theorem 2 establishes a new formal connection between the two domains of contract design and hypothesis testing. In the context of contract design, it provides a statistical interpretation: A min- budget contract can be interpreted as an optimal hypothesis test, and the ability to distinguish between the two hypotheses determines the required budget. In the converse direction, it enables a new proof for the Neyman-Pearson using the min-budget contract given by Theorem 1 (see Appendix B.4.2). 3.4 All-or-nothing contracts: Beyond binary action For general action spaces, optimal contracts are not guaranteed to be all-or-nothing. In fact, we show that determining whether there exists an all-or-nothing contract that is optimal is NP-hard: Theorem 3 (Hardness). Finding a min-budget all-or-nothing contract is NP-hard. The proof is by reduction from 3SAT, and appears in Appx. B.5.2. Nonetheless, there are special but important cases—notably the binary outcome case5— in which results from Sec. 3.3 hold, suggesting that ideas from Thm. 1 apply more broadly. The following algorithm makes use of these ideas, showing good empirical performance, and provable performance under an MLRP condition (Sec. 3.5). Single binding action algorithm. Revisiting the closed-form all-or-nothing contract in Eq. (5), we observe that the result is based on the fact that binary action spaces have only one alternative action. Building upon this observation, we propose the single binding action (SBA) algorithm, which computes a solution to Eq. (4) in the general (many-actions) case: Given target action n∗ ∈ A, loop over all actions n ̸= n∗, and apply the closed-form formula in Eq. (5) to obtain an (all-or-nothing) contract t∗(n, n∗). If the agent’s best response (Eq. (2)) satisfies n (t∗(n, n∗)) = n∗, return t∗. If the loop ends without returning a contract, return ‘fail’. The following claim shows the algorithm is sound: Proposition 2 (Soundness of SBA). When the single binding action algorithm terminates successfully, it returns an optimal contract which is an all-or-nothing contract. Proof in Appendix B.6. Prop. 2 ensures that if SBA succeeds, then the returned all-or-nothing contract t∗ is optimal. Moreover, failure does not preclude the existence of an optimal all-or-nothing contract. If SBA fails, we solve Eq. (4) with a generic LP solver. Empirically, SBA was successful in more than 85% of cases, and is ∼103 times faster than the LP solver (Appendix C.3). Thus, this optimistic ‘try SBA first’ approach typically succeeds, adds negligible overhead if not, and guarantees correctness. 5In this case there are binary outcomes (m = 1), such as when the agent’s efforts can result in either success or failure [6, 28, 21]. For this case, we show that the optimal contract is also all-or-nothing (Appx. B.5.1). 73.5 Threshold contracts: MLRP assumption In this section we provide sufficient conditions, in the form of natural structural properties of (stochas- tic) learning curves, that guarantee the optimality of even simpler contracts—namely threshold contracts—and the success of SBA. With inspiration from hypothesis testing [40] and contract theory [26, 19], it is natural to consider the monotone likelihood ratio property (MLRP) assumption: Definition 1 (MLRP [e.g., 26]). A contract design setting satisfies MLRP if for every pair of actions a, a′ such that ca < ca′, the likelihood ratio fa′(j)/fa(j) is monotonically increasing in j. In our context, MLRP states that the better the validation-set performance of a classifier, the more likely it was trained on more data. This holds in particular for monotone learning curves with binomial outcome distribution [19, B.1]. For a binary action space, MLRP ensures that n2 is always implementable,6 and that the optimal contract is a threshold contract: t∗(j) = B1 [j ≥ j0]. This is by Theorem 1, and by the fact that ∃j0 such that f2(j)/f1(j) ≥ 1 iff j ≥ j0 (see also Appendix B.7.1).7 Interestingly, this is similar to the relation between the Neyman-Pearson lemma and the Karlin-Rubin theorem, which characterizes the most powerful hypothesis test under monotone likelihood ratio [40]. MLRP for general action space. MLRP does not guarantee threshold contracts in general: In Appendix B.7.3, we give a constructive counterexample satisfying MLRP, but for which the optimal contract is not threshold. However, refining MLRP to also capture ‘diminishing returns’ turns out to be sufficient for recovering guarantees generally. For target actionaN , denote the survival probability of an action ai by si = Pj∼fi[j ≥ j∗], where j∗ is the minimal outcome at which action aN is more likely than aN−1. These will serve as formal means for capturing the concavity of learning curves. Definition 2 (C-MLRP). A contract design setting satisfies Concave-MLRP (C-MLRP) if it satisfies MLRP , and additionally the actions’ survival probability is concave as a function of the actions’ cost. Our final result shows that C-MLRP guarantees optimality of threshold contracts, and success of SBA: Theorem 4 (Sufficiency for threshold). Consider a contract design setting with C-MLRP. Then the optimal contract is a threshold contract, and is recovered by the SBA algorithm. We prove this claim by showing that concavity implies that only one alternative action is binding in the linear program equivalent to Eq. (4), reducing the problem to the two-action case. By applying Theorem 1, we obtain optimality of threshold contracts in this case as well (proof in Appendix B.7.3). This also implies that SBC always terminates successfully on inputs that satisfy C-MLRP. In practice, we believe that C-MLRP is a reasonable assumption for realistic learning curves: in Appendix B.8.1, we prove it is satisfied by a standard theoretical model of learning curves, and in Ap- pendix C.3, we empirically demonstrate that it is (approximately) satisfied in settings where threshold contracts are optimal. Interestingly, in our empirical study, threshold contracts were often optimal even when C-MLRP did not hold—suggesting the condition is sufficient, but not necessary (see Sec. 4.1). 4 Experiments We now turn to our empirical investigation of delegated learning under full and partial information. We base our experiments on the recently curated Learning Curves Database(LCDB) [ 43], which includes a large collection of stochastic learning curves for multiple classification datasets and methods. For each dataset and method, the database includes held-out accuracy measurements obtained for increasing sample sizes n ∈ \b 24, 24.5, . . . ,215\t , with multiple repetitions per n; these provide us with stochastic learning curves. Here we focus primarily on the popular MNIST dataset [39] as our case study, and on MLP and GBDT as representative classifiers, but we refer the reader to Appendix C for further experiments on additional datasets and methods. Code is available at: https://github.com/edensaig/delegated-classification. 4.1 Full information We begin with the full information setting to explore in a clean environment how different parameters of the learning setting and environment affect predictive performance and economic outcomes. 6As a corollary of a similar result for min-pay contracts [19, Lemma 7]. 7This also holds for binary outcomes in an arbitrary action space, see Appendix B.7.2. 8102 103 104 Size of training set n 0.4 0.6 0.8 1.0 Accuracy accD(hn) MNIST learning curves MLP GBDT 0 20 40 Size of validation set m 10000 20000 30000Required budget B∗ Required budget for 85% accuracy MLP GBDT cn∗ MLPGBDT 0.5 0.6 0.7 0.8 0.9 Target accuracy E[accD(hn)] −105 −102 0 102 105 ∆B∗ = B∗ GBDT − B∗ MLP Which algorithm is less costly to delegate? ∆B Figure 3: Delegating with full information. (Left) Typical learning curves for two learning algorithms on MNIST. (Center) Required budget for target accuracy of 0.85 per validation set size m. (Right) Different cost regimes, indicating per accuracy region which of the two methods is cheaper to delegate. Validation set size. Fig. 3 (left) presents typical stochastic learning curves for two learning algo- rithms: Multi-Layered Perceptron (MLP) and Gradient-Boosted Decision Trees (GBDT). We take an arbitrary accuracy point on the curve at acc(n) = 0 .85 (dotted line) to examine the effects of validation set size m on min-budget contracts. Notice that MLP requires larger n to obtain 0.85; Fig. 3 (center) shows how this translates to a larger required budget B∗, which holds for all m. As m increases, required budgets and the difference between them both decrease. Larger validation sets are therefore useful for reducing required budget. Nonetheless, even for reasonable m, obtained budgets still remain higher than their theoretical lower bounds (target action costs cn∗). Budget regimes. Fig. 3 (left) also indicates two points in which the learning curves cross (dashed lines), at ∼0.74 and ∼0.94 accuracy. These correspond to sample sizes n for which both methods obtain matching accuracies (in expectation). For a self-sufficient learner, the implication is that at each of these points, both methods are equally costly, i.e., both cost cn. Interestingly, and in contrast, delegation can entail different required budgets despite equal accuracies. Fig. 3 (right) shows for each target accuracy the gap in required budgets between both methods, ∆B∗ = B∗ GBDT − B∗ MLP . As can be seen, each method is comparatively more (or less) costly in different accuracy regimes (up to 0.6; between 0.6 and 0.92; and above 0.92). Crucially, the budget gap can be large even when accuracies match (dashed lines). For example, even though both MLP and GBDT require n=362≈217/2 samples to obtain ∼0.74 accuracy, GBDT is cheaper (∆B∗= − 102); for ∼ 0.94 which requires n=23170≈229/2 from both, GBDT is significantly more expensive (∆B∗=105). The reason for this is that optimal budgets are determined by the ability to distinguish between distributions (Sec. 3.3). Prevalence of simple contracts. To understand the applicability of our theoretical findings, in Appendix C.3 we conduct an empirical prevalence evaluation on additional learning algorithms, and across target actions. We observed that min-budget contracts assume a threshold form and the SBC algorithm returns correct results in more than 85% of cases overall. Restricting optimization to simple contracts, budget requirements were generally less than 1% higher than that of a min-budget contract, suggesting that simple contracts may provide a good approximation even when min-budget contracts do not assume a simple form. 4.2 Partial information We now turn to consider delegation under partial information, in which the principal must rely on an estimated learning curve. We instantiate this idea by assuming that the principal has access to a small ‘pilot’ dataset of size k, where k is considered small. Using this set, the principal creates an estimated learning curve ˆF by fitting a curve to accuracies obtained for up to some n0 ≤ k, and extrapolating to larger n > n0. In particular, we experiment with fitting parametric power-law curves of the form E[αn] = a − bn−c, which have been shown to provide good fit in various scenarios both empirically and theoretically [49, 34, 11]. Since power-law curves are monotone, composition with binomial distributions increasing in p provably results in MLRP stochastic curves [19, B.1]. Bias-variance tradeoff. Given k pilot examples, there are different ways in which the principal can use them to construct an estimated curve. Here we consider a simple tradeoff: setting n0 to be small but with more samples per n < n0 (low variance), or setting n0 to be large but with few samples per n < n0 (low bias). We definer as the number of samples pern (so low r means larger n0). Then, for a 90.5 1.0 Diﬀerent sampling strategies r = 1 0.5 1.0 Expected accuracy r = 3 102 103 104 Training set size n 0.5 1.0 r = 5 102 103 Exploration set size k 0.6 0.7 0.8 0.9 Attained accuracy accD(hn(ˆt)) Extrapolation convergence 102 103 Pilot dataset size k 0 10 20 30Size multiplier µ(k) = n(ˆt)/k Cost-eﬃciency −0.6 −0.4 −0.2 0.0 0.2 Extrapolation residual at n(t∗) -100% -80% -60% -40% -20% 0% Rel. accuracy loss in delegation Eﬀective loss Observs. Median Figure 4: Delegating with partial information. (Left) Extrapolated learning curves for different r. (Center-left) Accuracy obtained via delegation per pilot set sizek. (Center-right) Multiplicative gain in effective number of samples due to delegation. (Right) Implications of over vs. under-estimation. given r, we set n0 such that P n≤n0 r·n ≤ k (i.e., such that the total number of used samples does not exceed k). Fig. 4 (left) shows different curve fits forr ∈ {1, 3, 5}, and corresponding n0. Then, Fig. 4 (center-left) shows for a certain fixed budget the accuracy level that can be attained for increasing k, and as a function of r. As can be seen, having sufficient points k for constructing ˆF is important, but performance grows quickly with k (note log-scale x-axis). It is also apparent in our example that low bias (via larger n0) is much more important than low variance for constructing useful ˆF. Cost-efficiency tradeoff. Because the pilot set provides the principal a basic means for obtaining minimal accuracy, we can ask: given k examples, and for a fixed budget B, what is the added benefit of delegating learning? For this, we define µ(k) = n(ˆt)/k to be the sample-size multiplier, i.e., the multiplicative gain in the effective number of samples due to delegation. Fig. 4 (center-right) shows µ(k) for increasing k and across r. For r = 1 (which is superior in terms of performance and outcomes), µ begins at ∼10, increases to ∼30 at around k = 190, and slowly decreases back to ∼10 towards k = 1, 000. For r > 1, we observe that µ ≈ 1, i.e., there is effectively no gain from delegation, until around k = 100, only after which some gain is restored. This highlights the importance of obtaining an accurate estimate ˆF in terms of the economic consequences of delegation. Over vs. under-estimation. Typically in curve-fitting, over and under-estimation are treated equally, since both types of error can negatively affect goodness of fit and extrapolation quality. However, for delegation, the implications of over vs. under-estimation on contract outcomes are highly asymmetric. Fig. 4 (right) shows for a target incentivized number of samplesn(t∗) the relation between the (theoretical) signed extrapolation error n(t∗) (i.e., over- or under-estimate, measured in accuracy points) and the eventual loss in accuracy obtained through delegation, relative to perfect estimation. Each point in the plot corresponds to one curve-fitting instance, with points shown for varying k, n0, and r, and with multiple independent repetitions. Results show that in the under- estimation regime (i.e., negative extrapolation error), loss in accuracy degrades gracefully with the estimation error. In stark contrast, even minimal over-estimation (positive extrapolation error) causes accuracy to plummet dramatically, as the agent’s rational response in those cases was to use the smallest dataset possible. We interpret this as a consequence of ‘setting the bar too high’—a rational decision to minimize effort in response to unrealistic expectations. This has important implications for the choice of how to fit and extrapolate learning curves, suggesting that contracts can be tolerant to under-estimation, while over-estimation should be avoided at all costs. 5 Discussion Motivated by the increasingly-common practice of outsourcing learning tasks, this paper sets out to introduce and study the novel problem of delegated classification. Our findings suggest that conflict of interests should not be overlooked, and that contracts hold potential as a means for aligning them. Our analysis relies on a set of assumptions, which should be carefully considered by practitioners and empiricists alike; we also believe that there are likely further fruitful connections to explore between contracts and statistical hypothesis testing. As a problem of contract design, and when the learning task is reasonably well-behaved, delegated learning manifests in the form simple threshold contracts. A natural question for future work is whether simplicity also implies robustness to partial knowledge—as is often the case [19]. 10Acknowledgements. The authors would like to thank Ruth Heller, Shafi Goldwasser, Jonathan Shafer, Ohad Einav, and anonymous reviewers for their insightful remarks and valuable suggestions. Nir Rosenfeld is supported by the Israel Science Foundation grant no. 278/22. Eden Saig is supported by the Israel Council for Higher Education PBC scholarship for Ph.D. students in data science. Funded by the European Union (ERC, ALGOCONTRACT, 101077862, PI: Inbal Talgam-Cohen). References [1] Milton Abramowitz, Irene A Stegun, and Robert H Romer. Handbook of mathematical functions with formulas, graphs, and mathematical tables, 1988. [2] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [3] Tal Alon, Magdalen Dobson, Ariel D. Procaccia, Inbal Talgam-Cohen, and Jamie Tucker-Foltz. Multiagent evaluation mechanisms. In AAAI 2020, pages 1774–1781, 2020. [4] Nivasini Ananthakrishnan, Stephen Bates, Michael I Jordan, and Nika Haghtalab. Delegating data collection in decentralized machine learning. arXiv preprint arXiv:2309.01837, 2023. [5] Pablo D Azar and Silvio Micali. Computational principal–agent problems. Theoretical Eco- nomics, 13(2):553–578, 2018. [6] Moshe Babaioff, Michal Feldman, and Noam Nisan. Combinatorial agency. In Proceedings of the 7th ACM Conference on Electronic Commerce, pages 18–28, 2006. [7] Moshe Babaioff, Michal Feldman, Noam Nisan, and Eyal Winter. Combinatorial agency. Journal of Economic Theory, 147(3):999–1034, 2012. [8] Stephen Bates, Michael I Jordan, Michael Sklar, and Jake A Soloff. Principal-agent hypothesis testing. arXiv preprint arXiv:2205.06812, 2022. [9] Devansh Bisla, Apoorva Nandini Saridena, and Anna Choromanska. A theoretical-empirical approach to estimating sample complexity of dnns. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3270–3280, 2021. [10] Patrick Bolton and Mathias Dewatripont. Contract theory. MIT press, 2004. [11] Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van Handel, and Amir Yehudayoff. A theory of universal learning. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 532–541, 2021. [12] Olivier J Bousquet, Amit Daniely, Haim Kaplan, Yishay Mansour, Shay Moran, and Uri Stemmer. Monotone learning. In Conference on Learning Theory, pages 842–866. PMLR, 2022. [13] Michael L. Bynum, Gabriel A. Hackebeil, William E. Hart, Carl D. Laird, Bethany L. Nicholson, John D. Siirola, Jean-Paul Watson, and David L. Woodruff.Pyomo–optimization modeling in python, volume 67. Springer Science & Business Media, third edition, 2021. [14] Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation with strategic data sources. In COLT 2015, pages 280–296, 2015. [15] Gabriel Carroll. Robustness and linear contracts. American Economic Review, 105(2):536–563, 2015. [16] Junjie Chen, Minming Li, and Haifeng Xu. Selling data to a machine learner: Pricing via costly signaling. In International Conference on Machine Learning, pages 3336–3359. PMLR, 2022. [17] Alon Cohen, Argyrios Deligkas, and Moran Koren. Learning approximately optimal contracts. In SAGT 2022, pages 331–346, 2022. [18] Paul Dütting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In EC 2019, pages 369–387, 2019. 11[19] Paul Dütting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In Proceedings of the 2019 ACM Conference on Economics and Computation, pages 369–387, 2019. [20] Paul Dütting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Combinatorial contracts. In FOCS 2021, pages 815–826, 2021. [21] Paul Dütting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Combinatorial contracts. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 815–826. IEEE, 2022. [22] Paul Dütting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Multi-agent contracts. In STOC 2023, 2023. To appear. [23] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=hR_SMu8cxCV. [24] Shafi Goldwasser, Guy N Rothblum, Jonathan Shafer, and Amir Yehudayoff. Interactive proofs for verifying machine learning. In12th Innovations in Theoretical Computer Science Conference (ITCS 2021). Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2021. [25] Sanford J. Grossman and Oliver D. Hart. An analysis of the principal-agent problem. Econo- metrica, 51(1):7–45, 1983. [26] Sanford J Grossman and Oliver D Hart. An analysis of the principal-agent problem. In Foundations of insurance economics, pages 302–340. Springer, 1992. [27] William E Hart, Jean-Paul Watson, and David L Woodruff. Pyomo: modeling and solving mathematical programs in python. Mathematical Programming Computation, 3(3):219–260, 2011. [28] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In Proceedings of the fifteenth ACM conference on Economics and computation, pages 359–376, 2014. [29] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. Journal of Artificial Intelligence Research, 55:317–359, 2016. [30] Derek Hoiem, Tanmay Gupta, Zhizhong Li, and Michal Shlapentokh-Rothman. Learning curves for analysis of deep networks. In International conference on machine learning, pages 4287–4296. PMLR, 2021. [31] Bengt Holmström. Moral hazard and observability. The Bell Journal of Economics , 10(1): 74–91, 1979. [32] Bengt Holmstrom and Paul Milgrom. Aggregation and linearity in the provision of intertemporal incentives. Econometrica: Journal of the Econometric Society, pages 303–328, 1987. [33] Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021. [34] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [35] Jon Kleinberg and Robert Kleinberg. Delegated search approximates efficient search. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 287–302, 2018. [36] Jon Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategi- cally? In EC 2019, pages 825–844, 2019. [37] Jon M. Kleinberg and Manish Raghavan. Algorithmic classification and strategic effort. SIGe- com Exch., 18(2):45–52, 2020. 12[38] Sébastien Lahaie. How to take the dual of a linear program. Columbia University, New York, 2008. [39] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. [40] Erich Leo Lehmann, Joseph P Romano, and George Casella. Testing statistical hypotheses, volume 3. Springer, 1986. [41] Rafid Mahmood, James Lucas, David Acuna, Daiqing Li, Jonah Philion, Jose M Alvarez, Zhiding Yu, Sanja Fidler, and Marc T Law. How much more data do I need? Estimating requirements for downstream tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 275–284, 2022. [42] David Martimort and Jean-Jacques Laffont. The theory of incentives: the principal-agent model. Princeton University Press, 2009. [43] Felix Mohr, Tom J Viering, Marco Loog, and Jan N van Rijn. Lcdb 1.0: An extensive learning curves database for classification tasks. Machine Learning and Knowledge Discovery in Databases, ECMLPKDD. p. accepted. Lecture Notes in Computer Science, Springer, 2022. [44] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. [45] Phillippe Rigollet and Jan-Christian Hütter. High dimensional statistics. Lecture notes for course 18S997, 813(814):46, 2015. [46] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=ryenvpEKDr. [47] Royal Swedish Academy of Sciences. Scientific background on the 2016 Nobel Prize in Economic Sciences, 2016. [48] Bernard Salanie. The Economics of Contracts: A Primer. MIT press, 2017. [49] Tom Viering and Marco Loog. The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [50] Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. Training trajectories of language models across scales. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 13711–13738. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL -LONG.767. [51] Xinyi Zhao, Weixin Liang, and James Zou. Data budgeting for machine learning.arXiv preprint arXiv:2210.00987, 2022. [52] Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I. Jordan. The sample complexity of online contract design. In Proceedings of the 24th ACM Conference on Economics and Computation, EC 2023, London, United Kingdom, July 9-12, 2023, page 1188. ACM, 2023. doi: 10.1145/3580507.3597673. 13A Broader implications In this paper, we set out to formalize and study the task of delegating classification through the lens of contract design. Given that our work is largely motivated by the increasingly common practice of outsourcing learning tasks to specialized service providers, we believe our algorithm, analysis, and empirical observations carry meaningful implications for practitioners and decision-makers alike. At the same time, it is important to remember that our work—as others considering economic aspects of learning—studies delegation in a simplified setting and under certain assumptions. As such, and since devising and agreeing to legally-binding contracts can have concrete implications on real-world out- comes, care should be taken when applying ideas or conclusions that derive from our work in practice. For example, consider that our formalism relies on the assumption that the learning agent is all- knowing and rational. Yet in reality, agents must act under partial information, face irreducible (and often unquantifiable) uncertainty, and—being human—are subject to common behavioral biases. It is unclear a-priori if and how our statements and conclusions carry over to this setting. As another ex- ample, notice that our formalism considers a one-shot setting where a single contract between a single principal-agent pair is instantiated once. But in reality, competition and long-term reputation may play a significant role in determining the agent’s incentive structure, and consequently, her behavior. In such cases, naïvely applying our framework without careful inspection of the appropriate incentives on both ends can result in sub-optimal contracts, possibly to the detriment of all involved parties. Nonethe- less, given that our work aims to take an initial step towards establishing delegated learning, we view its extension to non-rational agents and temporal and competitive settings as intriguing future work. One message that our work conveys is that in delegation, simplicity, in the form of threshold contracts, has merit. This draws connections to other works that similarly argue for simplicity as an important and useful property of effective delegation mechanisms. Our work shows that simple contracts are, under reasonable conditions, computationally feasible and theoretically optimal. Economically, threshold contracts are practically appealing since they are easy to understand, communicate, and regulate. Given that contracts are in essence social constructs, we believe these properties are key for establishing threshold contracts as effective building blocks for machine learning markets. B Min-budget contract design – deferred proofs Notation. To align with traditional contract design notation, in this section, we denote the action space by A = [n] = {1, . . . , n}, and the outcome space by Ω = {0, . . . , m}. We denote by ∆ (X) the set of distributions over a given set X. For contract design problems, we denote the outcome probabilities by Fi,j, such that Fi ∈ ∆ (Ω)is the outcome distribution associated with action i, and also denote the cost of each action by ci. Given x ∈ R, we denote x+ = ReLU(x) = max {0, x}. We denote the indicator function by 1 [·], the total variation distance between distributions P, Qby ∥P − Q∥TV (see Definition 4), and the survival function of P ∈ ∆ (Ω)by SP (·) (see Definition 14). A note on individual rationality. In the contract design literature, a contract is said to be incentive compatible (IC) with respect to some action a∗ if it satisfies ua∗(t) ≥ ua(t) for all actions a ∈ A. In the delegated classification setting, this corresponds to the constraint n(t) = n∗ in eq. (4). As an additional constraint, contracts in which the agent’s expected utility is always non-negative (un∗(t) ≥ 0) are said to be individually rational (IR). In the case of delegated classification, it is natural to assume that there always exists a valid action that the agent can take at zero cost — for example, returning a dummy classifier that always abstains from prediction (thus having zero accuracy), or a classifier which makes a prediction at random. As such, contracts in the delegated classification setting can be assumed to be individually rational without loss of generality, as any action n that is chosen by the agent has utility which is weakly larger than the utility of the zero-cost action, which is always non-negative. Moreover, even in cases where a zero-loss action does not exist, we show that individual rationality (IR) can be attained in a straightforward manner, by adding the minimal cost c1 to each entry of an incentive compatible (IC) contract (tj 7→ tj + c1): Claim 1. Given an IC contract t with c1 > 0, the contract t + c1 (add c1 to every coordinate of the contract) is both IC and IR. Proof. An IC contract implementing action a∗ satisfies ua∗(t) ≥ ua(t), for all actions a ∈ A. In particular, this inequality holds in relation to the least-costly action, denoted by 1 ∈ A. Plugging the definition of the agent’s expected utility (eq. (2)), we obtain ua∗(t) ≥ Ej∼f1 [t] − c1. Thus, under the 14mapping t′ j = tj + c1, it holds that: ua∗(t′) = ua∗(t + c1) ≥ Ej∼f1 [t + c1] − c1 = Ej∼f1 [t] ≥ 0 and therefore the contract t′ = t + c1 is individually rational and still implements action a∗. B.1 Relation between budget-optimal and min-budget contracts Proof of Proposition 1. Given budget B >0, denote by tBO the budget-optimal contract, and denote the action it implements by n∗. By definition, tBO is a feasible solution to the min-budget contract design problem implementing action n∗. Denote by tMB the corresponding optimal solution to the min-budget problem implementing action n∗ (eq. (4)). tMB implements the same action n∗ by definition, and satisfies ∥tMB∥∞ ≤ ∥tBO∥∞ ≤ B due to the optimization objective. Hence, tMB is a budget-optimal contract for the given budget B, which is also min-budget. Iterative min-budget. To find the budget-optimal contract using iterative applications of eq. (4), we observe that any budget-limited contract t : Ω → [0, B] has bounded expected pay Efn[t] ≤ B for any distribution fn. Hence, actions n′ with cost cn′ > Bcannot be implemented using budget B, as the agent’s utility un′ = Efn′ [t] − cn′ < 0 will be smaller than the utility of the zero-cost action u0 = Efn[t] − 0 ≥ 0. Define the reduced action set: A′ = {n ∈ A |cn ≤ B ∧ n is implementable} A′ is finite when A is finite, or when the data cost is unbounded and the learning curve is monotone. To find n∗ within this space, go over all n ∈ A′, calculate the minimal budget Bn required for implementation, and take argminn∈A′ Bn. This is possible within a finite number of steps. B.2 The min-budget linear program and equivalent forms The min-budget contract (eq. (4)) implementing action i ∈ Ais given by the MIN-BUDGET linear program: min t∈R|Ω| ≥0 ,B∈R≥0 B s.t. ∀j ∈ Ω : tj ≤ B (BUDGET) ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci (IC) (6) The dual of the min-budget LP is given by: Claim 2. The dual linear program of Equation (6) is given by: max λ∈Rn−1 ≥0 ,µ∈R|Ω| ≥0 X i′̸=i (ci − ci′) λi′ s.t. ∀j ∈ Ω : X i′̸=i (Fi,j − Fi′,j) λi′ ≤ µj X j∈Ω µj ≤ 1 (7) Proof. We take the dual by translating the optimization problem into canonical form. The canonical form we target: min x≥0 cT x s.t. Cx ≤ d and its dual, as given by Lahaie [38], is: max y≥0 −dT y s.t. CT y ≥ −c To translate Equation (6), we note that in our case the components c,C,d are given by: 15cT = (0 . . .0 1 ) ∈ R|Ω|+1 C =   F1,0 − Fi,0 . . . F 1,|Ω| − Fi,|Ω| ... ... ... Fi′,j − Fi,j ... Fn,0 − Fi,0 Fn,m − Fi,m 0 ... 0 Im+1 −1 ... −1   ∈ R(n−1+|Ω|)×(|Ω|+1) dT = (c1 − ci . . . cn − ci 0 . . .0) ∈ Rn−1+|Ω| To simplify formulation, we denote the dual optimization variable y as follows: yT = (λ1 . . . λi−1 λi+1 . . . λn µ0 . . . µm) ∈ Rn−1+|Ω| Under this formulation, the dual’s objective is: −dT y = X i̸=i′ (ci − ci′) λi′ (8) The constraints given by the first m rows of CT are: ∀j ∈ Ω : X i′̸=i (Fi′,j − Fi,j) + µj ≥ 0 and equivalently: ∀j ∈ Ω : X i′̸=i (Fi,j − Fi′,j) ≤ µj (9) The constraint corresponding to the last row of CT is given by: X j∈Ω −µm ≥ −1 and equivalently: X j∈Ω µm ≤ 1 (10) Combining equations (8, 9, 10) yields the linear program given by Equation (7). To map between contracts and hypothesis tests, we introduce the following variable transformation: Definition 3 (Statistical representation of contracts) . For a given contract t : Ω → R≥0, denote B = maxj tj. The statistical representation of t is given by: tj = ϕjβ−1 where ϕj ∈ [0, 1] and β = B−1. Note that the transformation (t, B) 7→ (ϕ, β) is non-linear, and well-defined for all B >0. Under this variable transformation, the MIN-BUDGET transforms to an equivalent linear program: 16Lemma 2. For a feasible design problem, the min-budget contract design LP (Equation (6)) is equivalent to: max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. ∀i′ ̸= i : X j∈Ω Fi,j(1 − ϕj) + X j∈Ω Fi′,jϕj ≤ 1 − (ci − ci′)β (11) Proof. Given Equation (6), define ϕ ∈ [0, 1]|Ω|, β ≥ 0 according to definition 3: B = β−1 tj = ϕjB = ϕjβ−1 (12) Under the transformation defined by Equation (12), the (BUDGET) constraint in Equation (6) transforms as follows: tj ≤ B ⇔ ϕjβ−1 ≤ β−1 ⇔ ϕj ≤ 1 (13) and the (IC) constraint in Equation (6) transforms as: X j Fi,jtj − ci ≥ X j Fi′,jtj − ci′ ⇔ X j (Fi,j − Fi′,j) ϕjβ−1 ≥ ci − ci′ ⇔ X j Fi,jϕj − X j Fi′,jϕj ≥ (ci − ci′) β ⇔ 1 − X j Fi,j(1 − ϕj) − X j Fi′,jϕj ≥ (ci − ci′) β ⇔ X j Fi,j(1 − ϕj) + X j Fi′,jϕj ≤ 1 − (ci − ci′) β (14) where the first equivalence is by Equation (12), and the third equivalence is valid as P j Fi′,j = 1 for all i′. Finally, the objective of Equation (6) transforms as: min B ⇔ max β (15) Combining equations (13, 14, 15) yields the linear program in Equation (11). B.3 Relation to min-pay contract design Min-pay contract design aims to design a contract which minimizes the expected pay under the implemented action n∗: t∗ = argmint Ej∼fn∗ [tj] s.t. n(t) = n∗ (16) In contrast to the min-budget contract (eq. (4)), the∥t∥∞ objective measuring maximal pay is replaced with the Ej∼fn∗ [tj] objective measuring expected pay. Equation (16) is equivalent to the MIN-PAY linear program: min t∈R|Ω| ≥0 X j∈Ω Fi,jtj s.t. ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci (IC) (17) B.3.1 Characterization of implementability The implementatbility of min-pay contracts is characterized in Dütting et al. [19]. We cite the main result for completeness: Proposition 3 (Min-pay implementability; [ 19], A.2). An action i ∈ Ais implementable (up to tie-breaking) if and only if there is no convex combination of the other actions that results in the same distribution fi = P i′̸=i αi′fi′, but lower cost ci > P i′̸=i αi′ci′. 170.0 2.5 5.0 7.5 10.0 12.5 15.0 Outcome j 0.00 0.05 0.10 0.15 0.20 0.25 Outcome distributions f1 f2 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Outcome j 0 10 20Payment tj Min-pay and min-budget contracts MIN-PAY MIN-BUDGET Figure 5: Qualitative comparison of min-pay and min-budget contracts under MLRP. (Left) The contract design setting, representing two possible actions with binomial outcome distributions (p1 = 0.5, p2 = 0.8, m = 15). Action 1 has zero cost c1 = 0, and action 2 has unit cost c2 = 1. (Right) The resulting min-pay (red) and min-budget (purple) contracts, obtained by solving eq. (17) and eq. (6), respectively. The min-pay awards payment only for the highest outcome, in alignment with proposition 4; the min-budget is a threshold contract, in alignment with claim 11. We leverage to proposition 3 to characterize the implementability of min-budget contracts. This is possible due to the following connection: Claim 3 (Implementability equivalence). A contract t is feasible solution of MIN-BUDGET if and only if it is a feasible solution of MIN-PAY. Proof. A contract t which satisfies MIN-BUDGET (eq. (6)) satisfies the (IC) constraint, and thus also satisfies MIN-PAY (eq. (17)). Conversely, a contractt which satisfies MIN-PAY satisfies the(IC) constraint. Set B = maxj tj and obtain a feasible solution to MIN-BUDGET. B.3.2 Min-pay contracts under MLRP The optimal min-pay contracts under the MLRP assumption are characterized in Dütting et al. [19]: Proposition 4 (Optimal min-pay contract under MLRP; [19], Lemma 7). Consider a contract design setting for which MLRP holds. If the highest-cost action n is implementable, then there is a min-pay contract has a single nonzero-payment, which is rewarded for the highest outcomem. See fig. 5 for a qualitative comparison of min-pay and min-budget contracts. Min-pay contracts in delegated learning settings with MLRP. While simple, the min-pay contract given by proposition 4 is impractical in realistic delegated classification scenarios. In a delegated classification setting, the highest outcome corresponds to 100% validation set accuracy (i.e all validation set samples classified correctly). As m grows, the highest outcome becomes exponentially less likely, and thus the min-pay contract awards increasingly high payments with increasingly lower probabilities (See fig. 6). In such settings, even a slight degree of risk-averseness is likely to affect decisions: From the agent’s perspective, the probability of any receiving payment from a min-pay contract may become small to a degree where even a slight degree of agent risk-averseness will manifest itself. From the principal’s side, the exponential rate of growth in required budget B.4 Min-budget contracts with two actions In this section, we explore min-budget settings with two actions, and prove Theorem 2. When n = 2, assume without loss of generality that the contract implements action i = 2, and denote c = c2 − c1 > 0. We recall the definition of total variation distance: Definition 4 (Total variation distance). Given two distributions P, Q∈ ∆ (Ω), the total variation distance between P and Q is: ∥P − Q∥TV = 1 2 ∥P − Q∥1 The following equivalent definition is useful: 1810 20 30 40 50 Validation set size m 100 1.2 × 100 1.4 × 100 1.6 × 100 1.8 × 100 2 × 100 2.2 × 100 Ej∼f2 [tj] Expected pay MIN-PAY MIN-BUDGET c2 = 1 10 20 30 40 50 Validation set size m 100 101 102 103 104 B Required budget 10 20 30 40 50 Validation set size m 0.0 0.2 0.4 0.6 0.8 1.0 Pj∼f2 (tj > 0) Probability of any payment Figure 6: Comparison of min-pay and min-budget contracts under MLRP for varying validation set size m. The delegation setting is similar to the one depicted in Figure 5: two binomial-outcome actions (p1 = 0.5, p2 = 0.8) and varying m. Action 1 has zero cost c1 = 0, and action 2 has unit cost c2 = 1. In the left and center plots, the cost of action 2 is represented by an orange line (c2 = 1) representing the lower bound as in fig. 3 (Center). (Left) Expected pay Ej∼f2 [tj]. (Center) Required budget ∥t∥∞. (Right) Probability of getting any payment Pj∼f2 [tj > 0]. Claim 4. Let P, Q∈ ∆ (Ω). It holds: ∥P − Q∥TV = X j∈Ω (Qj − Pj)+ where x+ = max {x, 0}. Proof. By definition: ∥P − Q∥TV = 1 2 ∥P − Q∥1 Decompose the L1 norm: 1 2 ∥P − Q∥1 = 1 2 X j∈Ω \u0010 (Qj − Pj)+ + (Pj − Qj)+ \u0011 As P j∈Ω Pj = P j∈Ω Qj = 1, it holds that: X j∈Ω (Qj − Pj)+ = X j∈Ω (Pj − Qj)+ and therefore ∥P − Q∥TV = P j∈Ω (Qj − Pj)+ as required. B.4.1 Optimal two-action contract Theorem 5 (Two-action min-budget contract; formal statement of Theorem 1). When n = 2, the optimal min-budget contract t∗ is given by: t∗ j = c ∥F2 − F1∥TV 1 [F2,j ≥ F1,j] (18) Proof. We prove this claim using LP duality, by showing that the optimal primal objective corre- sponding to t∗ is identical to a feasible solution of the dual LP. The primal LP (Equation (6)) for two actions is given by: min t∈R|Ω| ≥0 ,B∈R≥0 B (19) s.t. ∀j ∈ Ω : tj ≤ B (BUDGET) X j∈Ω (F2,j − F1,j) tj ≥ c (IC) 19As t∗ is bounded, the optimal objective B∗ corresponds to the maximal possible payout of t∗: B∗ = max j∈Ω t∗ j = c ∥F2 − F1∥TV (20) and therefore the (BUDGET) constraint is satisfied. For the (IC) constraint, denote by Ω≥ the following set: Ω≥ = {j ∈ Ω | F2,j ≥ F1,2} using this notation, we note that t∗ j > 0 if and only if j ∈ Ω≥. Plugging into the constraint, we obtain: X j∈Ω (F2,j − F1,j) t∗ j = X j∈Ω≥ (F2,j − F1,j) B∗ = c P j∈Ω≥ (F2,j − F1,j) ∥F2 − F1∥TV = c Where ∥F2 − F1∥TV = P j∈Ω≥ (F2,j − F1,j) by definition. This shows that t∗ is a feasible solution for the primal LP. To prove optimality, we show that the the dual linear program attains an identical objective. By Claim 2, the dual LP for two actions is given by: max λ∈R≥0,µ∈R|Ω| ≥0 cλ s.t. ∀j ∈ Ω : (F2,j − F1,j) λ ≤ µj X j∈Ω µj ≤ 1 (21) Denote the vector ⃗ vj(λ) ∈ R|Ω| ≥0: ⃗ vj(λ) = λ (F2,j − F1,j)+ We note that outcomes j′ for which F2,j′ ≤ F1,j′ (formally j′ ∈ Ω \\ Ω≥) correspond to constraints which are satisfied for any λ ≥ 0. Otherwise j ∈ Ω≥, and in these cases λ can be increased until ⃗ v(λ) saturates the simplex constraint P j∈Ω µj = 1. The simplex constraint is binding for a value λ∗ > 0 satisfying: X j∈Ω λ∗ (F2,j − F1,j)+ = 1 and therefore the optimal value λ∗ of the dual LP (eq. (21)) is: cλ∗ = cP j∈Ω (F2,j − F1,j)+ = c ∥F2 − F1∥TV (22) Where the second equality is given by claim 4. The dual objective in eq. (22) is identical to the primal objective attained by the contract t∗ in Equation (20), and therefore t∗ is an optimal contract by strong LP duality. B.4.2 Contracts and hypothesis tests We recall the formal definition of the Neyman-Pearson lemma: Lemma 3 (Neyman-Pearson, [e.g., 45, 4.3]). Let P, Q∈ ∆ (X) be two probability measures over an arbitrary set X. Then for any hypothesis test ψ : X → {0, 1}, it holds: P(ψ(x) = 1) + Q(ψ(x) = 0) ≥ 1 − ∥P − Q∥TV . (23) Moreover, equality holds for the Likelihood Ratio testψ∗(x) = 1 [q(x) ≥ p(x)]. 20In our analysis, we will assume that the space X is finite. We also note that Lemma 3 is stated for decision rules with binary output, but its domain can be extended to fractional decision functions ψ : X →[0, 1] without loss of generality: As the sum of errors is linear in ψ, the optimal fractional decision rule is a solution to the linear program minψ∈[0,1]X (P(ψ = 1) + Q(ψ = 0)) where P(ψ = 1) = P x∈X pxψ(x) and Q(ψ = 0) = P x∈X qx(1 − ψ(x)). The feasible region of this linear program is the hypercube [0, 1]X and its vertices are the set of binary decision rules {0, 1}X , which also includes the optimal binary rule ψ∗ given by Lemma 3. As every feasible linear program attains its optimum on a vertex, the binary ψ∗ is also the optimal among fractional rules. Proof of Theorem 2. For a given two-action contract, denote Pj = F1,j, Qj = F2,j, and c = c2 −c1. We recall that the statistical min-budget LP for two actions is given by Lemma 2: max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. X j∈Ω Pjϕj + X j∈Ω Qj(1 − ϕj) ≤ 1 − cβ (24) and the Neyman-Pearson lemma is given by Equation (23). Given a min-budget contract design problem, apply Theorem 1 to obtain its optimal solution, as given by eq. (18): B∗ = c ∥P − Q∥TV t∗ j = c ∥P − Q∥TV 1 [Qj ≥ Pj] Applying the transformation from definition 3 on the optimal contract, we obtain equivalently: β∗ = (B∗)−1 = ∥P − Q∥TV c ϕ∗ j = t∗ j /B∗ = 1 [Qj ≥ Pj] Note that ϕ∗ j is a maximum-likelihood decision rule, similar to the optimal critical function in the Neyman-Pearson lemma. Additionally, by optimality of the contract-design optimization variable β∗, any feasible solution ϕ′ of Equation (24) satisfies: mX j=1 Pjϕ′ j + mX j=1 Qj(1 − ϕ′ j) ≥ 1 − cβ∗ = 1 − ∥P − Q∥TV with equality satisfied by the maximum likelihood rule ϕ∗. Therefore, the min-budget optimality of the contract t∗ implies the power optimality of the hypothesis test ϕ∗. Conversely, let ϕ : Ω → [0, 1] be an maximal-power hypothesis test. This provides a lower bound on the constraint in Equation (24): X j∈Ω Pjϕj + X j∈Ω Qj(1 − ϕj) ≥ 1 − ∥P − Q∥TV By the Neyman-Pearson lemma, the bound is tight for the maximum likelihood rule ϕ⋆ = 1 [Q ≥ P], and therefore the optimal objective β⋆ satisfies: 1 − cβ⋆ = 1 − ∥P − Q∥TV and hence β⋆ = ∥P−Q∥TV c . Applying the transformation in definition 3 yields the optimal contract t⋆ j = c ∥P−Q∥TV 1 [Qj ≥ Pj], showing that the corresponding contract is min-budget optimal if the corresponding hypothesis has optimal statistical power. Remark 1. Since the proof of Theorem 1 is independent of the Neyman-Pearson lemma, the argument proving the optimality of ϕ∗ in the proof above implies the Neyman-Pearson lemma for finiteX. 21B.5 More than two actions Definition 5 (All-or-nothing contract). Let B >0. An all-or-nothing contract t : Ω → R≥0 satisfies tj ∈ {0, B} for all j ∈ Ω. B.5.1 Binary outcomes In this section, we show that every binary-outcome min-budget contract is all-or-nothing. This is a corollary of a more general lemma: Lemma 4. For any feasible min-budget contractt∗, there exists j0 ∈ Ω such that t∗ j0 = 0. Proof. By contradiction, assume that t∗ j > 0 for all j, and denote j0 = argmin j t∗ j . Denote a = minj tj, and note that a >0. Define the contract ˜t as follows: ∀j : ˜tj = tj − a By definition, ˜tj ≥ 0 for all j, and ˜tj0 = 0. Since t∗ is a feasible min-budget contract, it satisfies the min-budget LP in eq. (6), and in particular the (IC) constraint: ∀i ∈ [n − 1] : X j Fi,jt∗ j − ci ≤ X j Fn,jt∗ j − cn Plugging in the definition ˜t = t∗ − a and using the fact that P j Fi,j = 1 for all i ∈ [n], we obtain: ∀i ∈ [n − 1] : X j Fi,j˜tj − ci ≤ X j Fn,j˜tj − cn and therefore ˜t satisfies the (IC) constraint as well. This is a contradiction, since ˜t is a feasible contract with a lower required budget. From this we conclude that the optimal contract must satisfy tj = 0 for some j. Corollary 1. In any min-budget contract design setting with two outcomes, the optimal contract is all-or-nothing. B.5.2 Hardness: Basic definitions and construction In this section, we show that finding optimal all-or-nothing contracts is NP-hard in the general case. Definition 6 (Maximin contract design matrix). For a given contract design problem targeting action n and satisfying ci < cn for all i ∈ [n − 1], the maximin design matrix A is defined as: Aij = Fn,j − Fi,j cn − ci (25) Claim 5. An optimal all-or-nothing contract is a solution of the following optimization problem: max ϕ∈{0,1}|Ω| min i∈[n−1] (Aϕ)i (26) Where A is the corresponding maximin design matrix, as given by Definition 6. Proof. By Lemma 2, Equation (11), the min-budget contract design LP is equivalent to: max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. ∀i < n: X j∈Ω Fn,j − Fi,j cn − ci ϕj ≥ β As Aij = Fn,j−Fi,j cn−ci , it holds that P j∈Ω Fn,j−Fi,j cn−ci ϕj = Aϕ. The LP above is therefore equivalent to: 22max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. ∀i < n: Aiϕ ≥ β as β is a lower bound for every constraint, its optimal value is the maximal minimum attainable through variation of ϕ: max ϕ∈[0,1]|Ω| min i∈[n−1] (Aϕ)i and restricting the optimization space of ϕ to {0, 1}|Ω| yields the desired result. Definition 7 (3-CNF – Conjunctive normal form). A 3-CNF formula over m variables and n clauses is a boolean-valued function ψ : {0, 1}m → {0, 1} of the form: ψ(x1, . . . , xm) = n^ i=1 (zi1 ∨ zi2 ∨ zi3) where zik ∈ {x1, . . . , xm, ¬x1, . . . ,¬xm}. Definition 8 (Number of positives in clause i). Given a 3-CNF ψ and an assignment x ∈ {0, 1}m, denote by σi(ψ, x) the number of variables zik in clause i which evaluate to 1 under assignment x. Claim 6. A formula ψ is satisfiable if and and only if there exists an assignment x ∈ {0, 1}m such that mini∈[n] σi(ψ, x) ≥ 1. Proof. If ψ is satisfiable then there exists x ∈ {0, 1}m such that ψ(x) = 1 . Since ψ is a 3-CNF, every clause i must evaluate to 1, and therefore σi(ψ, x) ≥ 1 for all i ∈ [n]. Conversely, if there exist an assignment x such that mini∈[n] σi(ψ, x) ≥ 1 then by definition x satisfies every clause, and therefore the whole formula ψ. B.5.3 Hardness: Reduction from 3SAT Given a 3-CNF ψ with n clauses and m variables, we define a min-budget contract design problem over n + 1 actions and m + 3 variables: The target action is i = n + 1. The cost associated with action i is: ci = \u001a1 i = n + 1 0 otherwise (27) Contract outcomes are denoted Ω = {1, . . . , m,pos, neg, const}. For simplicity of notations, let ε > 0, which will be assigned a suitable value later in the proof. The distribution of the target outcome is denoted by Q and defined as: ∀j ∈ [m] : Qj = ε m Qpos = 1 − ε \u0012 1 + 3 m \u0013 Qneg = 0 Qconst = 3ε m (28) 23For each i ∈ [n], denote the number of negated variables in clause i by ki ∈ {0, . . . ,3}. The distribution corresponding to the i-th action is denoted by P(i) and defined as: ∀j ∈ [m] : P(i) j =    0 xj exists in clause i and is not negated 2ε m xj exists in clause i and is negated ε m otherwise P(i) pos = 0 P(i) neg = 1 − ε \u0012 1 + 9 − 4ki m \u0013 P(i) const = (3 − ki) ε m (29) These distributions are well-defined whenever ε <1 4 and m ≥ 3. For concreteness, set: ε = 1 10 (30) Using Definition 6 and combining equations (27, 28, 29), the maximin design matrix Aψ ∈ R(n+1)×(m+3) corresponding to the contract design problem above is given by: ∀j ∈ [m] : Aψ i,j =    ε m xj exists in clause i and is not negated − ε m xj exists in clause i and is negated 0 otherwise Aψ i,pos = Qpos Aψ i,neg = −P(i) neg Aψ i,const = ki ε m (31) Definition 9. Given an assignment x ∈ {0, 1}m, the corresponding contract vector ϕx ∈ {0, 1}m+3 is defined as: ∀j ∈ [m] : ϕx j = xj ϕx pos = 1 ϕx neg = 0 ϕx const = 1 (32) Claim 7. Let ψ be a 3-CNF , andx ∈ {0, 1}m an assignment. It holds that: \u0000 Aψϕx\u0001 i = ε mσi(ψ, x) + Qpos Proof. To prove this claim, plug ϕx from Definition 9 into Aψ defined in Equation (31). Here we denote A = Aψ, ϕ = ϕx for simplicity. We obtain: (Aϕ)i = X j∈[m] Ai,jϕj | {z }P xj in clause ϕj−P ¬xj′ in clause ϕj′ + Ai,pos| {z } =Qpos ϕpos|{z} =1 +Ai,neg ϕneg|{z} =0 + Ai,const| {z } =ki ε m ϕconst| {z } =1 = ε m   X xj in clause i ϕj + X ¬xj′ in clause i (1 − ϕj′)   + Qpos = ε mσi(ψ, x) + Qpos Claim 8. A 3-CNF ψ is satisfiable if and only if the corresponding optimization problem in Equa- tion (26) attains a value of at least Qpos + ε m . 24Proof. If ψ is satisfiable by assignment x ∈ {0, 1}m, then σi(ψ, x) ≥ 1 for all i ∈ [n] by Claim 6. Let ϕx denote the vector corresponding to the satisfying assignment, according to Definition 9. Apply Claim 7 to obtain: \u0000 Aψϕx\u0001 i = ε mσi(ψ, x) + Qpos ≥ Qpos + ε m Conversely, assume the optimal solution to Equation (26) is at least Qpos + ε m , and denote the vector attaining the optimal solution by ϕopt. As the corresponding matrix Aψ, defined in Equation (31), satisfies the following for all i ∈ [n]: Ai,pos = Qpos ≥ 0 Ai,neg = −P(i) neg ≤ 0 Ai,const = ki ε m ≥ 0 it can be assumed that the optimal solution ψopt satisfies: ϕopt pos = 1 ϕopt neg = 0 ϕopt const = 1 because otherwise the value of any \u0000 Aψϕ \u0001 i would increase by setting these entries in ϕ to the values above. Hence, we can denote ϕopt = ϕx according to Definition 9, and apply Claim 7 to obtain: \u0000 Aψϕx\u0001 i = ε mσi(ψ, x) + Qpos as the value attained is at least ε m + Qpos, it must hold that σi(ψ, x) ≥ 1 for all i ∈ [n], and therefore x satisfies ψ according to Claim 6. B.5.4 Proof of hardness We are now ready to prove Theorem 3: Proof of Theorem 3. By reduction from 3SAT. Given a 3-CNF ψ with n clauses and m variables, construct in polynomial time the corresponding matrix Aψ as defined by eq. (31), and apply an all-or-nothing min-budget contract solver according to eq. (26) to obtain the optimal solution. The validity of the LP is given by eq. (26). According to Claim 8, the formula ψ is satisfiable if and only if the value attained in the optimization is at least ε m + Qpos, where m is the number of variables in x, and ε, Qpos are constants defined in eq. (30), eq. (28) respectively. B.6 The single binding action algorithm To prove the soundness of SBA, we first make the following definition: Definition 10 ((i′-IC) relaxation). Consider a MIN-BUDGET LP with target action i ∈ A, as given by eq. (6). For any action i′ ̸= i, the ( i′-IC) relaxation of the min-budget problem is given by eliminating all (IC) constraints except for the one corresponding to action i′. Proof of Proposition 2. Denote the target action by i ∈ A. On each iteration of the loop, the algorithm considers some action i′ ̸= i, and applies Theorem 1 on the ( i′-IC) relaxation of the original MIN-BUDGET LP. Since the relaxed LP has only one (IC) constraint, its optimal solution, denoted by t∗(i′, i), is given by eq. (5). By Definition 10, the feasible region of the original LP lies within feasible region corresponding to its (i′-IC) relaxation. Thus, if an optimal solution of the relaxed LP lies within the feasible region of the original LP, then it is also a global optimum of the original LP.t∗(i′, i) lies within the feasible region of the original LP is it satisfies the remaining (IC) constraints—a condition equivalent to the notation a(t∗(i′, i)) = i by eq. (2). The SBA algorithm terminates successfully only in such cases, and therefore it is sound. 25A note on ties in SBA. Denote the target action by i. To simplify presentation, the algorithm presentation in Section 3.4 implicitly assumes that required budgets∥t∗(i′, i)∥∞ are distinct for every pair of actions (i′, i), and therefore the return value of SBA is well-defined. In case of ties, the return value is not well-defined (as the iteration order is not well-defined), but small a modification to the algorithm allows ties to be broken explicitly without affecting other properties of the algorithm: In case of ties in optimal required budget, the soundness of the algorithm and the all-of-nothing property of the returned contracts is not affected. However, the exact functional form of the returned contract may be affected by the iteration order. In case such issue becomes relevant, the SBA algorithm can be modified to first collect a set of optimal contracts (instead of immediately returning when an optimal feasible contract is found), and then select one of the optimal contracts based on the desired criteria. As the proof of Proposition 2 does not depend on iteration order, soundness will not be affected, and worst-case time complexity will remain the same. B.7 MLRP In this section, we explore contract design under the MLRP assumption, and prove Theorem 4. We first recall the statistical notion of monotone likelihood ratio: Definition 11 (Mononote Likelihood Ratio – MLR). The distributions P, Q∈ ∆ (Ω)have Monotone Likelihood Ratio when Qj Pj is monotonically increasing for all j ∈ Ω = {0, . . . , m}. We denote this by P ≺ Q. An introduction to MLR and its relation to statistical hypothesis testing is provided in Lehmann et al. [40, Section 3.4]. Using this notation, we can reformulate Definition 1: Definition 12 (MLRP; reformulation of Def. 1 using MLR notation) . A principal-agent problem satisfies the Monotone Likelihood Ratio Property if Fi′ ≺ Fi for every pair of actions i, i′ such that ci′ < ci. Claim 9. Let P, Q∈ ∆ ([m]) such that P ≺ Q. Then P0 > Q0 and Pm < Qm. Proof. For the first inequality, assume by contradiction that Q0 ≥ P0. Combining with the MLR property, this assumption implies that Qj ≥ Pj for all j ∈ {0, . . . , m}. As the outcome distributions P, Q are distinct, there exists at least onej for which Qj > Pj. Summing over j yields: P j∈Ω Qj >P j∈Ω Pj, which is a contradiction, as the inequality is strict while both sides sum to 1. The proof for the second inequality follows in the same way. Definition 13 (MLR crossing point j∗ P,Q). Let P, Q∈ ∆ (Ω)such that P ≺ Q. The crossing point of Q over P is the outcome j∗ ∈ {0, . . . , m} such that Qj∗−1 < Pj∗−1 and Qj∗ ≥ Pj∗. When distribution are not clear from context, we denote j∗ = j∗ P,Q In words, the crossing point j∗ is the outcome such that for every lower outcome, P is strictly more likely than Q, and starting with this outcome Q is weakly more likely. By claim 9, the MLR crossing point j∗ is uniquely defined for any P ≺ Q, and it holds that j∗ > 0. Definition 14 (Survival function). Given P ∈ ∆ ([m]), the survival function SP (·) : Ω → [0, 1] is defined as: SP (j) = Pj′∼P [j′ > j] = mX j′=j+1 Pi Informally, the survival function is 1 minus the distribution’s CDF. This function is useful in our context, since the total variation distance between two distributions (as appearing in theorem 5) can be represented as the difference between their survival functions when they satisfy MLR. The intuitive reason for this is that MLR means the distributions are “single-crossing”. Claim 10 (Total variation under MLR). Let P, Q∈ ∆ (Ω)such that P ≺ Q. It holds: ∥P − Q∥TV = SQ(j∗ P,Q − 1) − SP (j∗ P,Q − 1) 26Proof. By Claim 4, it holds that: ∥P − Q∥TV = X j∈{0,...,m} (Qj − Pj)+ Using the j∗ notation (see definition 13), we obtain: X j∈{0,...,m} (Qj − Pj)+ = mX j=j∗ P,Q Qj − Pj And the result is obtained by applying the definition of survival function (see definition 14). B.7.1 Two-action contracts with MLRP Combining Theorem 5 with Claim 9 leads to a characterization of threshold contracts in the case of two actions: Claim 11. For any two-action contract design problem satisfying MLRP , the optimal contract is a threshold contract: t∗ j = c ∥F2 − F1∥TV 1 \u0002 j ≥ j∗ F1,F2 \u0003 where j∗ F1,F2 = min {j ∈ {0, . . . , m} |F2,j ≥ F1,j} as in Definition 13. Proof. Combining the result of Claim 9 with the monontonicity assumption, the likelihood ratio F2,j/F1,j crosses 1 exactly once. Denote the crossing point by j∗, and apply Theorem 5 to obtain the optimal contract. B.7.2 Two-outcome contracts with MLRP When there are more than two actions, assume without loss of generality that the contract aims to implement the last action n. The following claim establishes the existence of theshold contracts in the two-outcome setting |Ω| = 2. In contrast to corollary 1, the proof in constructive, and yields a concrete contract: Claim 12. For any contract design problem satisfying MLRP with n > 2 actions and m = 2 outcomes, the optimal min-budget contract is a threshold contract. Proof. By Claim 2, the dual LP for two outcomes is given by: max λ∈Rn−1 ≥0 ,µ∈R2 ≥0 n−1X i=1 (cn − ci)λi s.t. ∀j ∈ {1, 2} : n−1X i=1 (Fn,j − Fi,j) λi ≤ µj X j∈{1,2} µj ≤ 1 (33) From Claim 9, we obtain that Fn,1 − Fi,1 < 0 and Fn,2 − Fi,2 > 0 for all i ∈ [n − 1], and therefore the first constraint in Equation (33) is always satisfied for j = 1. Simplifying Equation (33) we obtain: max λ∈Rn−1 ≥0 ,µ∈R≥0 n−1X i=1 (cn − ci)λi s.t. n−1X i=1 (Fn,2 − Fi,2) λi ≤ 1 (34) 27which is maximized by allocating all budget to the λi maximizing the “bang for buck”. The dual objective is therefore given by: B∗ = max λ∈Rn−1 ≥0 ,µ∈R≥0 X i∈[n−1] (cn − ci)λi = max i∈[n−1] cn − ci Fn,2 − Fi,2 (35) To see that a threshold contract is optimal, let t∗ j = B∗ · 1 [j = 1]. Primal objective is B∗, and the contract is feasible if the primal LP is feasible. The (IC) constraint in Equation (6) can be written as: ∀i ∈ [n − 1] : P2 j=1 (Fn,j − Fi,j) tj cn − ci ≥ 1 and indeed for every action i ∈ [n − 1]: P2 j=1 (Fn,j − Fi,j) tj cn − ci = Fn,2 − Fi,2 cn − ci B∗ = Fn,2 − Fi,2 cn − ci max i∈[n−1] cn − ci Fn,2 − Fi,2 ≥ 1 And therefore t∗ j is feasible in the primal problem, and also optimal by strong LP duality. Also note that the resulting contract coincides exactly with the optimal min-pay contract as attained by Dütting et al. [19, Lemma 7]. B.7.3 General contracts with MLRP In this section, we explore min-budget contracts in MLRP settings with more than two actions and more than two outcomes. We start with a negative example, showing that the MLRP assumption is not sufficient for establishing the optimality of threshold contracts: Claim 13. For|Ω| > 2, there exists a design problem satisfying MLRP for which the optimal contract is not a threshold. Proof. Consider the following contract design problem: F1 ∼ Binomial(10, 0.5) F2 ∼ Binomial(10, 0.65) F3 ∼ Binomial(10, 0.8) c1 = 0 c2 = 0.45 c3 = 1 The distributions are binomial, and therefore the contract satisfies MLRP. Numerically solving Equation (11) yields the following fractional contract: t∗ LP = (0, 0, 0, 0, 0, 0, 0, 1.1, 1.46, 1.46, 1.46) Numerically solving Equation (11) while imposing integer constraints ϕj ∈ {0, 1} yields the follow- ing contract, which has higher max payout: t∗ IP = (0, 0, 0, 0, 0, 0, 0, 1.51, 1.51, 1.51, 1.51) Thus for this case any threshold contract is min-budget suboptimal. A graphical illustration of the proof is provided in Figure 7. Remark 2. The proof of Claim 13 is provided with 10 outcomes (|Ω| = 10) for ease of graphical interpretation, however a minimal counterexample may also be constructed using only3 outcomes. For example, consider the following design problem: F1 = (0.5, 0.3, 0.2) F2 = (0.3, 0.4, 0.3) F3 = (0.1, 0.35, 0.55) c1 = 0 c2 = 0.45 c3 = 1 The proof for this case follows in the same way, where numerical computation yields the contracts: t∗ LP = (0, 1.9, 2.6) t∗ IP = (0, 2.7, 2.7) 280.0 2.5 5.0 7.5 10.0 Outcome j 0.0 0.1 0.2 0.3 Probability mass Outcome distributions Fi F1 F2 F3 j∗ 0.0 2.5 5.0 7.5 10.0 Outcome j 0.0 0.5 1.0 1.5 Payout tj Min-budget - LP vs. integer solutions LP MIP 0.00 0.25 0.50 0.75 1.00 Cost ci 0.2 0.4 0.6 Crossing-point survival si Non-concavity of crossing-point survival Figure 7: Graphical illustration of Claim 13. (Left) Outcome distributions F1, F2, F3 and MLR crossing point j∗ F2,F3 . (Center) The min-budget contract t∗ j , given by numerically solving eq. (11) (purple), and numerically solving eq. (11) while imposing integer constraints ϕj ∈ {0, 1} (cyan). Note that the fractional LP solution achieves lower max payout. (Right) Crossing-point survival si = SFi(j∗ Fi,Fn − 1) as a function of costci (see definition 15). Note that the function is not concave, thus the sufficient condition given in Theorem 4 does not hold in this case. Remark 3. We also note that a counterexample with2 outcomes is not possible due to Claim 12. The negative example shows that even with MLRP, the optimal contract incentivizing the highest implementable action needs to rule out deviations of the agent to all other actions, and not just to the second-highest one. This makes the contract complex. We identify a natural economic condition that is sufficient for considering only a single deviation (to the second-highest action), resulting in a simple contract. Note that considering a single such deviation is equivalent to restricting the action space to actions {n − 1, n}. B.8 Sufficiency condition for more than two actions For the following definition, recall the definition of MLR crossing point j∗ (Def. 13), and survival function S (Def. 14): Definition 15 (Concave-MLRP; Formal statement of Definition 2). For a contract design setting sat- sifying MLRP , denote byj∗ = j∗ Fn,Fn−1 the crossing point of the outcome distribution corresponding to the highest action Fn, and the outcome distribution corresponding to the second-highest action Fn−1. For any action i ∈ [n], the crossing-point survival si is defined as: si = SFi(j∗ − 1) In words, si is the probability to get an outcome at or above crossing-point j∗ according to outcome distribution Fi. From an economic perspective, for every action i ∈ [n], si is the agent’s probability to receive any nonzero payment from choosing action i, if the contract is designed by restricting the action-space to actions {n − 1, n}. Note however that the definition of si only depends on the outcome distribution Fij, and does not require solving the contract design problem. B.8.1 Binomial power-law curves satisfy Concave-MLRP Recent theoretical work on learning curves has focused mainly on power-law expected learning curves of the form an = 1 − βn−γ [11, 33]. In addition, these curves were also found to provide a good fit for certain practical applications [49, POW2]. In this section, we show that a stochastic generalization of these curves satisfies the Concave-MLRP property: Definition 16 (Power-law stochastic learning curve). Let β, γ∈ R≥0, and m ∈ N. A delegated learning setting with actions A has a realizable power-law stochastic learning curve with parameters β, γif Fi = Binomial \u0000 1 − βn−γ i , m \u0001 for all ni ∈ A. For the proof, we also recall the definition of the regularized incomplete beta function: Definition 17 (Regularized incomplete beta function). For k1, k2 ≥ 1, the regularized incomplete beta function is defined as: Ip(k1, k2) = Rp 0 tk1−1(1 − t)k2−1dt R1 0 tk1−1(1 − t)k2−1dt 290 20000 40000 60000 Action cost cn = n 0.4 0.6 0.8 1.0 Expected accuracy - MLP LCDB data Power-law ﬁt 0 20000 40000 60000 Action cost cn = n 0.0 0.2 0.4 Survival sn Survival at likelihood transition ˜n n0 0 20000 40000 60000 Action cost cn = n 0 1 2d2sn/dn2 ×10−8 Second derivative of sn ˜n n0 Figure 8: Graphical intuition for the sufficiency condition in Claim 14. (Left) Expected accuracy curve for MNIST-784 MLP. Blue dots represent empirical data from the LCDB dataset, cyan curve represents power-law fit (an = 1 − βn−γ, with ˆβ = 1.89, ˆγ = 0.33). (Center) Crossing point survival si as a function of cost ci (see definition 15), for m = 30. Cyan dot represents inflection point ˜n ≈ 2022. Red vertical line represents the inflection point bound n0 ≈ 3957 suggested by the proof. It can be observed that the curve is concave for all n >˜n. (Right) Second derivative of the survival function sn, illustrating the position of the bound n0 in relation to the inflection point ˜n. We also recall that Ip is related to the survival function of the binomial distribution [e.g., 1, 6.6.4]: Ip(k + 1, n− k) = Px∼Binomial(p,n)[x > k] Claim 14. Let β, γ∈ R≥0, m ∈ N. A delegated learning setting with a power-law stochastic learning curve Fi = Binomial(1 − βn−γ i , m), action costs ci = ni, and mini ni ≥ \u0000 β(m+γ−1)/1+γ−1 \u00011 γ satisfies the Concave-MLRP assumption. Proof. Denote the expected accuracy of each action by an = 1 − βn−γ. The survival function of the binomial distribution is given by: sn = Ian (j∗, m+ 1 − j∗) Where Ian is the regularized incomplete beta function (definition 17). With slight abuse of notation, we treat an and sn as continuous functions of n. Taking the derivative by n and ignoring the constant denominator, we obtain: dsn dn ∝ aj∗−1 n (1 − an)m−j∗ dan dn Plugging the functional form of an: dsn dn ∝ \u0000 1 − βn−γ\u0001j∗−1 \u0000 βn−γ\u0001m−j∗ βγn−(γ+1) ∝ \u0000 βn−γ\u0001m−j∗+1+γ−1 \u0000 1 − βn−γ\u0001j∗−1 As a function of βn−γ, the function dsn dn attains its extremum at: β˜n−γ = m + 1 − j∗ + γ−1 m + γ−1 ˜n = \u0012 β(m + γ−1) m + 1 − j∗ + γ−1 \u00131 γ And therefore the function sn has an inflection point at ˜n (see fig. 8). From the upper bound j∗ ≤ m we obtain: ˜n ≤ \u0012β(m + γ−1) 1 + γ−1 \u00131 γ = n0 And as mini ni ≥ n0, the function sn is convex as a function of cn for all n ∈ A. The proof is illustrated in Figure 8. 30B.8.2 Proof of sufficiency theorem Theorem 6 (Concave-MLRP implies threshold contracts; formal statement of Theorem 4). For a contract design problem satisfying MLRP , considersi as a function of cost ci. If this function is concave, then the optimal contract is a threshold contract. Furthermore, the contract is successfully recovered by the SBA algorithm. Proof. To prove this claim, we construct a relaxed version of the min-budget LP (eq. (6)), and apply Theorem 5 in order to solve it. We then show that this solution is also feasible for the original LP. Given the min-budget LP, construct a relaxed LP by eliminating (IC) constraints for alli ≤ n − 2: min t∈R|Ω| ≥0 ,B∈R≥0 B s.t. ∀j ∈ Ω : tj ≤ B X j∈Ω Fn−1,jtj − cn−1 ≤ X j∈Ω Fn,jtj − cn (36) Equation (36) only depends on Fn and Fn−1, and therefore claim 11 can be applied to obtain the optimal min-budget contract: t∗ j = cn − cn−1 ∥Fn − Fn−1∥TV 1 [Fn,j ≥ Fn−1,j] (37) Let j∗ = min {j ∈ {0, . . . , m} |Fn,j ≥ Fn−1,j} as in definition 13, and denote si = SFi(j∗ − 1). By claim 10, we obtain: ∥Fn − Fn−1∥TV = sn − sn−1 For all i < n, denote: bi = cn − ci sn − si Using the definition of bi, eq. (37) can be rewritten as: t∗ j = bn−11 [j ≥ j∗] By assumption, si is a concave function of ci, and therefore the function bi is monotonically non- decreasing for all i < n: bn−1 ≥ bi Dividing both sides by bi, we obtain: bn−1 bi ≥ 1 Plugging in the definition of bi, and multiplying both sides by (cn − ci): bn−1 (sn − si) ≥ cn − ci (38) By definition of t∗ j , the expected payout of contract t∗ j under action i′ ∈ [n] is given by: mX j=0 t∗ j Fi′,j = bn−1 mX j=j∗ Fi′,j = bn−1si′ (39) Plugging eq. (38) into eq. (39) yields: mX j=0 tjFn,j − mX j=0 tjFi,j ≥ cn − ci (40) As eq. (40) is identical to the (IC) constraint in eq. (6), we obtain that t∗ j satisfies all the (IC) constraints in the original LP. From this we conclude that t∗ j , which is a threshold contract, is also an optimal contract with the respect to the full min-budget LP. For the second part of the claim, note that the SBA algorithm also attempts to solve eq. (36) on the iteration corresponding to action N − 1. As the solution of eq. (36) is guaranteed to be feasible for the original LP under Concave-MLRP, the SBA successfully recovers it. 310 5 10 15 Outcome j 0.00 0.05 0.10 0.15 0.20 0.25 Probability Fi,j Outcome distributions and crossing point j∗ F1 F2 F3 F4 F5 j∗ F4,F5 0 2500 5000 7500 10000 Cost ci 0.0 0.2 0.4 0.6 si = SFi(j∗ − 1) Crossing-point survival si 0 5 10 15 Outcome j 0 10000 20000 30000 40000Payment tj Optimal min-budget contract Figure 9: Graphical intuition for the sufficiency condition in Theorem 4 (restated in Theorem 6). (Left) Outcome distribution in a contract design setting satisfying MLRP, generated by a series of binomial distributions with power-law expectationaccD(hn) = 0.9−0.4 \u0000 n 100 \u0001−0.3 . The gray dotted line represents the MLR crossing point j∗ F4,F5 (see definition 13). (Center) Crossing point survival si as a function of cost ci (see definition 15). It can be observed that the curve is concave. (Right) The min-budget contract implementing action 5. It is a threshold contract, as guaranteed by theorem 4. Compare to fig. 7, where the sufficiency condition does not hold. B.9 Min-budget contracts with guaranteed minimum payout Our problem setting assumes that the agent selects its action rationally, as described in eq. (2). However, in some practical settings the agent may be risk averse to some extent, and require guaranteed minimum payment from the contract. In this section, we show that the rationality assumption is made without loss of generality in such cases: Lemma 5, which we prove below, shows that any optimal contract with guaranteed minimum payout can be represented as the sum of a min-budget contract without guaranteed payout, and a constant representing the payout guarantee. Definition 18 (Guaranteed minimum payout). Let δ ≥ 0. A contract t ∈ R|Ω| ≥0 has guaranteed payout of size δ if tj ≥ δ for all j ∈ Ω. Lemma 5. A contract t is a min-budget contract with guaranteed payout δ requiring budget B if any only if there exist a min-budget contract t′ (without guaranteed payout) requiring budget B′ such that t = t′ + δ and B = B′ + δ. Proof. When agents require a guaranteed minimum payout δ ≥ 0, we add a constraint to the MIN-BUDGET linear program defined in eq. (6), such that an optimal min-budget contract t with guaranteed payout δ is an optimal solution to the following linear program: min t∈R|Ω| ≥0 ,B∈R≥0 B s.t. ∀j ∈ Ω : tj ≤ B (BUDGET) ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci (IC) ∀j ∈ Ω : tj ≥ δ (MINW AGE) (41) To prove the first direction of the equivalence, assume thatt is a min-budget contract with guaranteed payout δ and budget B, and thus an optimal solution of eq. (41). Define the variable transformation: t′ = t − δ B′ = B − t0 (42) By eq. (42), the (BUDGET) constraint in eq. (41) transforms into: ∀j ∈ Ω :tj ≤ B ⇔ t′ j + δ ≤ B′ + δ ⇔ t′ j ≤ B′ (43) 32The (IC) constraint transforms into: ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci ⇔ X j∈Ω Fi′,j(t′ j + δ) − ci′ ≤ X j∈Ω Fi,j(t′ j + δ) − ci ⇔ X j∈Ω Fi′,jt′ j − ci′ + δ X j∈Ω Fi′,j | {z } =1 ≤ X j∈Ω Fi,jt′ j − ci + δ X j∈Ω Fi,j | {z } =1 ⇔ X j∈Ω Fi′,jt′ j − ci′ ≤ X j∈Ω Fi,jt′ j − ci (44) And the (MINW AGE) constraint transforms into: ∀j ∈ Ω :tj ≥ δ ⇔ t′ j ≥ 0 (45) Plugging back the transformed constraints (44, 43, 45) into eq. (41), we obtain that (t′, B′) is an optimal solution of the original MIN-BUDGET linear program eq. (6), and therefore t′ is an optimal min-budget contract without minimum guaranteed payout. Conversely, assume that t′ is an optimal min-budget contract (without minimum guaranteed payout), satisfying the MIN-BUDGET linear program in eq. (17) with budget B′. Apply the inverse transfor- mation t = t′ + δ and B = B′ + δ, and the equivalence relations (44, 43, 45) in the inverse direction to obtain that t, B is an optimal solution to eq. (41). C Experimental details C.1 Data LCDB. We base our main experimental environment on the LCDB dataset [43], which includes a large collection of stochastic learning curves for multiple learning algorithms and classifica- tion benchmark datasets. For each learning method and benchmark dataset, the database in- cludes held-out accuracy measurements, obtained for exponentially-increasing training set sizes n ∈ \b 24 = 16, . . . ,round(2k/2), . . . ,215 = 32, 768 \t , with multiple repetitions per n obtained by cross-validation. For all learning curves we consider in our analysis, and for each n, the number of repetitions provided in the dataset is in the range R ∈ {5, . . . ,125}, where the specific number of repetitions in LCDB depends on the algorithm and benchmark dataset (see the dataset documentation for additional details). For each trained classifier, each accuracy point on the learning curve is estimated using 5,000 held-out samples. Formally, and using our notation, for each learning algorithm Alg (e.g. MLP), dataset D (e.g. MNIST), and training set size n, LCDB provides a set of accuracy estimates \b a1,Alg,D n , . . . , aR,Alg,D n \t , such that each an is distributed according to an ∼ accD(hn), where hn is a classifier trained using training set S ∼ Dn and learning algorithm Alg (see section 2 for definition of accD(hn)). Benchmark dataset and algorithms. In our main analysis, we focus on the popular MNIST dataset [39, OpenML 554], and on multilayer perceptrons (MLP) and gradient-boosted decision trees (GBDT) as representative classifiers. For all classifiers, results are obtained for the respective default scikit-learn [44] implementations (e.g. MLPClassifier and GradientBoostingClassifier for MLP and GBDT, respectively). Delegated learning settings from empirical data. For a given validation set sizem where m = |V |, we instantiate a contract design task (A, c,Ω, F) as follows: (i) the set of actions with the set of training set sizes provided by LCDB ( A = \b 24, 24.5, . . . ,215\t ); (ii) action costs are set to fixed per-unit cost, i.e., cn = n; and (iii) the distribution F over outcomes Ω is associated with a binomial mixture distribtuion, resulting from applying bootstrap sampling to empirical error measurements: fn = 1 R RX r=1 Binomial(m, ar,Alg,D n ) 33where an are the accuracy estimates defined above. Figure 2 (Left) shows an example of such a setting, obtained by applying the above procedure to data describing learning curves for the MLP algorithm trained on the MNIST benchmark dataset. C.2 Implementation details Code. We implement our code in Python. Our code relies on Pyomo [13, 27] and GLPK for solving linear and mixed-integer programs. Code is available at: https://github.com/edensaig/delegated-classification. Hardware. All experiments were run on a single laptop, with 16GB of RAM, M1 Pro processor, and with no GPU support. Runtime. A single run consisting the entire pipeline takes roughly 5 minutes. The main bottleneck is running the LP solvers, taking roughly 70% of runtime to compute. C.2.1 Contract design solvers To find the optimal contracts, we implement and compare several different solvers: • LP solver: To find min-budget contracts given outcome distributions {fn} and costs {cn}, we solve the MIN-BUDGET LP (Eq. (6)) directly using Pyomo and GLPK. Given budget B, budget- optimal contracts are obtained by iterating through the actions set A, invoking the min-budget solver on each target action n ∈ A, enforcing incentive compatibility against all actions n′ which satisfy αn′ < αn, and taking the action yielding the maximal expected accuracy within budget (see appendix B.1). In our code, the LP solver is implemented within the MinBudgetContract class. Typical running time for a single problem instance: 10−1s. • SBA solver: Implementation of the single binding action algorithm presented in Section 3.4. The local solver is implemented within the MinBudgetSingleBindingConstraintContract class. Typical running time for a single problem instance: 10−4s. • Hybrid solver: A meta-solver implementing the ‘try SBA first’ computational approach. The solver starts by invoking the SBA solver, and applies the LP solver if the former fails. In our code, the LP solver is implemented within the MinBudgetHybridContract class. Running time depends on whether SBA is applicable. • MIP solver: To find optimal all-or-nothing contracts, we solve the MIN-BUDGET LP in its statistical formulation (Eq. (11)) while restricting ϕj ∈ {0, 1}. This turns the LP into a mixed- integer program, which can also be solved using GLPK. In our code, the IP solver is implemented within the MinBudgetStatisticalContract class. Typical running time for a single problem instance: 10−1s. • Min-pay LP solver: To compare between min-budget and min-pay contracts (Appendix B.3), min-pay contracts were obtained by solving the MIN-PAY LP (Eq. (17)) usingPyomo and GLPK, similar to the full min-budget LP solver. In our code, the min-pay is implemented within the MinPayContract class. Running time is similar to the other LP-based methods. • Full enumeration solver: To find all-or-nothing contracts for low-dimensional problems and avoid numerical instabilities, we implement a solver which performs full enumeration of all-or-nothing contracts. By the statistical variable transformation in definition 3, the (IC) constraint in the MIN-BUDGET LP translates to P j∈Ω (Fi,j − Fi′,j) ϕj ≤ (ci −ci′)β, where i is the target action, i′ ∈ A \\ {i}, and the objective is minimize β (see eq. (11)). Thus, when ϕ ∈ {0, 1}Ω is fixed, the optimal β is given by: β∗(ϕ) = min i′∈A\\{i} P j∈Ω(Fi,j−Fi′,j)ϕj ci−ci′ and optimizing β∗ over ϕ ∈ {0, 1}Ω yields an optimal all-or-nothing contract. The full enumaration solver is implemented within the FullEnumerationContract class. For for enumeration of all- or-nothing contracts, this solver is mostly applicable for small problem instances ( m < 20) due to its exponential complexity. In contrast, for threshold contracts the number of possible ϕ configurations is linear in m, and therefore enumeration is more efficient. • Fixed-budget solver: To find budget-optimal threhsold contracts given a fixed budgetB, we imple- ment a simple solver which iterates through all possible threshold contracts tj0 (j) = B1 [j ≥ j0] for all j0 ∈ {0, . . . , m}. The solver then simulates the agent’s rational choice (Eq. (2)) and selects 34the value of j0 which incentivizes the best action. In case of ties between possible values of j0, they are broken in favor of larger values, as this was shown to lead to greater numerical stability. The fixed-budget solver is implemented within the FixedBudgetThresholdContract class. C.3 Empirical prevalence estimation Since our theoretical analysis relies on MLRP assumptions, we would like to understand whether the results are applicable to real-world learning curves. Towards this, we run an empirical prevalence evaluation on the MNIST dataset. For each learning algorithm Alg, and training set size n, we construct a delegated learning setting with m = 10 and collect the following statistics: • Structural properties: – Is MLRP? (% MLRP): Check whether all pairs n1, n2 ≤ n such that cn1 < cn2 satisfy the monotone likelihood ratio property (see definition 1). • Computational properties: – SBA successful? (% SBA): Check whether the SBA algorithm was successful on the given instance. • Functional form of optimal contract: – Is monotone? (% M): Check whether the resulting min-budget contract satisfies tj ≥ tj−1 for all j ∈ [m]. – Is all or nothing? (% AoN): Check whether the resulting min-budget contract is an all-or- nothing contract (i.e. whether there exists j0 ∈ Ω,B >0 such that tj = 0 for all j < j0 and tj = B otherwise). – Is threshold? (% T): Check whether the resulting min-budget contract is a threshold contract (i.e. whether there exists j0 ∈ Ω,B >0 such that tj = 0 for all j < j0 and tj = B otherwise). • Excess cost: – Min-budget: Optimal objective BLP of MIN-BUDGET LP without additional restrictions, implemented using the full LP solver. – All-or-nothing budget: Optimal objective of MIN-BUDGET LP, when the resulting contract is restricted to all-or-nothing form tj ∈ {0, BAoN}. Implemented using the full enumeration solver. – Threshold budget: Optimal objective of MIN-BUDGET LP, when the resulting contract is restricted to threshold form tj = BThr1 [j ≥ j0]. Implemented using the full enumeration solver. – The excess cost columns in Table 2 indicate the relative excess cost incurred by restricting the min-budget optimization space to simple contracts ( B{AoN,Thr}−BLP BLP ). As all-or-nothing contracts are a superset of threshold contracts, we expect this excess cost to be smaller than the one associated with threshold contracts. The averaging in table 2 is performed on problem instances where both all-or-nothing and threshold contracts are feasible. Averaging over all implementable actions, we obtain the results in Table 2. The results show that threshold contracts are relatively common in this dataset (more than 85%), and that excess cost of simple contracts is relatively low (around 1%), indicating that threshold contracts may provide good approximation to the optimal min-budget contracts in some cases. All simple optimal contracts in our dataset had a threshold functional form, and some simple optimal contracts were not recovered by SBA, indicating that a tighter characterization of simple contracts may be possible. Figure 10 provides qualitative interpretation of the analysis for a selected subset of learners. While the survival functions are not perfectly concave (and thus don’t satisfy Concave-MLRP), the SBA algorithm successfully terminates in three cases (MLP, Logistic, KNN). The binding action is aN−1 for two of the classifiers (MLP, KNN), similar to the condition implied by Theorem 4. Note that SBA also terminated successfully on Logistic Regression data, despite the learning curve having a distinctive non-concave shape. In the case of GBDT, the survival function is not concave, and the optimal contract does not assume a threshold form. 35Table 2: Empirical robustness estimation on the MNIST dataset. Each row corresponds to a learning algorithm, and columns are specified in appendix C.3. Results are averaged across all implementable actions. Structure Compute Functional form of opt. contract Excess cost % MLRP % SBA % M % AoN % T AoN T MLP 100% 94.4% 100% 94.4% 94.4% 0.04% 0.04% GBDT 100% 76.5% 100% 82.4% 82.4% 0.71% 0.71% Logistic 93.8% 81.2% 93.8% 93.8% 93.8% 0.00% 0.00% Perceptron 68.8% 75% 93.8% 93.8% 93.8% 0.00% 0.00% Linear SVM 71.4% 78.6% 85.7% 85.7% 85.7% 0.95% 1.45% Poly SVM 100% 94.4% 100% 94.4% 94.4% 0.24% 0.24% RBF SVM 100% 94.4% 100% 94.4% 94.4% 0.04% 0.04% KNN 100% 100% 100% 100% 100% 0.00% 0.00% Overall 92.6% 87.4% 97% 92.6% 92.6% 0.22% 0.26% 0.25 0.50 0.75 MLP Expected accuracy E[accD(hni)] 0.0 0.5 Qualitative comparison of delegation problems Survival at transition si 0 200000 Optimal contract t∗ j 0.25 0.50 0.75 GBDT 0.0 0.5 0 200000 0.25 0.50 0.75 Logistic 0.0 0.5 0 200000 0 5000 10000 15000 Action cost ci 0.25 0.50 0.75 KNN 0 5000 10000 15000 Action cost ci 0.0 0.5 0 5 10 15 20 25 Outcome j 0 200000 Learning algorithm Figure 10: Qualitative comparison of optimal contract computation on LCDB MNIST data ( n∗ = 16384, m = 25). Each row represents a learning algorithm. (Left column) Expected accuracy curves αn. (Center column) Survival function at transition si, whenever SBA is successful, the binding actions are marked with triangles. (Right column) Optimal contract t∗ j . 36",
      "references": [
        "Handbook of mathematical functions with formulas, graphs, and mathematical tables",
        "Revisiting neural scaling laws in language and vision.",
        "Multiagent evaluation mechanisms.",
        "Delegating data collection in decentralized machine learning.",
        "Computational principal–agent problems.",
        "Combinatorial agency.",
        "Principal-agent hypothesis testing.",
        "A theoretical-empirical approach to estimating sample complexity of dnns.",
        "Contract theory.",
        "A theory of universal learning.",
        "Monotone learning.",
        "Pyomo–optimization modeling in python, volume 67.",
        "Optimum statistical estimation with strategic data sources.",
        "Robustness and linear contracts.",
        "Selling data to a machine learner: Pricing via costly signaling.",
        "Learning approximately optimal contracts.",
        "Simple versus optimal contracts.",
        "Combinatorial contracts.",
        "Multi-agent contracts.",
        "Scaling laws for neural machine translation.",
        "Interactive proofs for verifying machine learning.",
        "An analysis of the principal-agent problem.",
        "Pyomo: modeling and solving mathematical programs in python.",
        "Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems.",
        "Learning curves for analysis of deep networks.",
        "Moral hazard and observability.",
        "Aggregation and linearity in the provision of intertemporal incentives.",
        "Learning curve theory.",
        "Scaling laws for neural language models.",
        "Delegated search approximates efficient search.",
        "How do classifiers induce agents to invest effort strategi- cally?",
        "Algorithmic classification and strategic effort.",
        "How to take the dual of a linear program.",
        "The MNIST database of handwritten digits.",
        "Testing statistical hypotheses, volume 3.",
        "How much more data do I need? Estimating requirements for downstream tasks.",
        "The theory of incentives: the principal-agent model.",
        "Lcdb 1.0: An extensive learning curves database for classification tasks.",
        "Scikit-learn: Machine learning in Python.",
        "High dimensional statistics.",
        "A constructive prediction of the generalization error across scales.",
        "Scientific background on the 2016 Nobel Prize in Economic Sciences,",
        "The Economics of Contracts: A Primer.",
        "The shape of learning curves: a review.",
        "Training trajectories of language models across scales.",
        "Data budgeting for machine learning.",
        "The sample complexity of online contract design."
      ],
      "meta_data": {
        "arxiv_id": "2306.11475v2",
        "authors": [
          "Eden Saig",
          "Inbal Talgam-Cohen",
          "Nir Rosenfeld"
        ],
        "published_date": "2023-06-20T11:59:03Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces “delegated classification,” modeling outsourced ML training as a principal–agent (moral hazard) Stackelberg game where the agent privately chooses training effort (sample size) and the principal pays based on validated accuracy under a hard budget. Defines budget-optimal contracts and shows (i) any budget-optimal solution can be taken to be min-budget for the induced action; (ii) in the two-action case, the optimal min-budget contract is all-or-nothing and characterized by total-variation distance between outcome distributions; (iii) optimal two-action contracting is equivalent to the Neyman–Pearson most-powerful hypothesis test (likelihood-ratio rule), yielding a formal link between contract design and hypothesis testing; (iv) for general actions, finding optimal all-or-nothing contracts is NP-hard, but under structural assumptions on learning curves (MLRP plus a concavity/diminishing-returns condition), globally optimal contracts are simple threshold rules and are found efficiently by a “single binding action” (SBA) algorithm; (v) empirically demonstrates that simple/threshold contracts often perform near-optimally and can be learned from limited pilot data via learning-curve extrapolation.",
        "methodology": "Formalizes delegation with actions A as discrete training-set sizes n with increasing costs c_n, outcomes as the number of correct predictions j on a held-out validation set of size m, and stochastic outcome distributions f_n(j)=P[acc_V(h_n)=j/m]. Principal commits to a contract t:{0..m}->R_{>=0}; agent best-responds by maximizing E_{j~f_n}[t(j)]-c_n. Solves budget-optimal contracting by reducing to min-budget contracting for each target action (linear program MIN-BUDGET minimizing ||t||_∞ subject to incentive-compatibility). Uses LP duality to obtain a closed-form optimal two-action contract: pay B iff f_2(j)>=f_1(j), with B=(c2-c1)/||f2-f1||_TV. Shows equivalence to Neyman–Pearson via a reparameterization t/B as a (possibly fractional) hypothesis test. For many actions, proposes SBA: for target n*, compute candidate all-or-nothing contracts that bind against a single alternative action using the two-action formula; if one satisfies all IC constraints, it is optimal. Establishes hardness for all-or-nothing optimization in general, and proves sufficiency of threshold contracts under MLRP and a concavity condition (C-MLRP) on survival probabilities at the likelihood-ratio crossing point. For partial information, estimates learning curves from small pilot data using parametric power-law scaling E[α_n]=a-b n^{-c} and induces stochastic curves via binomial (or mixtures) to design contracts on the estimated curve.",
        "experimental_setup": "Uses LCDB (Learning Curves Database) stochastic learning curves across datasets/algorithms, focusing primarily on MNIST with MLP and GBDT (scikit-learn defaults) and additional algorithms/datasets in appendices. Constructs delegated-classification instances where actions are LCDB training sizes n in {2^4,2^{4.5},…,2^{15}}, costs c_n=n (unit labeling cost), and outcomes are validation correct counts j on a randomly sampled validation set V of size m. Outcome distributions f_n are instantiated as binomial mixtures: for each n, bootstrap/average over LCDB repetitions of held-out accuracies and model j~Binomial(m,α_{n,r}). Evaluates (full-information) optimal/min-budget contracts via LP and SBA, compares constant/linear/threshold forms, studies effect of validation size m on required budget, and compares delegation costs across algorithms/accuracy regimes. For partial-information experiments, assumes a pilot dataset of size k, fits power-law curves using samples up to n0<=k under different sampling strategies (bias–variance tradeoff via repetitions r per n), extrapolates to larger n, designs contracts from estimated curves, and measures achieved accuracy, sample-size multiplier μ(k)=n(\\hat t)/k, and sensitivity to over- vs under-estimation. Reports prevalence of simple contracts (e.g., threshold) and SBA success rates (often >85%) and computational speedups over full LP.",
        "limitations": "Relies on classic full-information contract-theory assumptions for main optimality results: the principal is assumed to know the outcome distributions/learning curve F for each action (or can estimate it accurately), actions are discretized sample sizes, and costs c_n are known and monotone. Model is one-shot (no repeated interactions, competition, or reputation) with a single agent and assumes risk-neutral, fully rational behavior and tie-breaking; does not model bounded rationality, risk aversion, or strategic manipulation of validation beyond effort choice. Outcome modeling often assumes binomial validation noise dominates; real-world training randomness, distribution shift, non-i.i.d. sampling, changing pipelines, or multi-dimensional effort (compute/model size/data quality) may violate MLRP/C-MLRP, and MLRP alone is insufficient for threshold optimality (counterexamples provided). In partial-information setting, contract performance is highly asymmetric to estimation error: slight over-estimation can induce agent to choose minimal effort, indicating vulnerability to misspecified/extrapolated learning curves; curve-fitting choices and extrapolation assumptions (power laws, monotonicity) may not hold universally. NP-hardness indicates limited guarantees for all-or-nothing optimality in general action/outcome spaces.",
        "future_research_directions": "Develop robust/learning-based contract design under uncertainty about learning curves (distributionally robust, Bayesian, or online/bandit contract learning) with explicit safeguards against over-estimation and misspecification. Extend from sample-size effort to multi-dimensional efforts (data quality, labeling noise, compute, model size, hyperparameter tuning) and to richer outcome metrics (calibration, fairness, costs of errors) and multi-class/regression tasks. Study repeated-game and market settings with reputation, competition among providers, renegotiation, and dynamic contracts; incorporate risk aversion, limited liability, and behavioral factors. Address strategic behavior beyond shirking (e.g., gaming validation, data leakage, tampering) and design auditing/verification mechanisms jointly with contracts. Characterize necessity (not just sufficiency) conditions for threshold/simple optimality and tighter approximations when C-MLRP fails; explore computational methods for general min-budget LP at scale and for continuous action spaces.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs",
      "full_text": "Mathematical Modelling and Numerical Analysis Will be set by the publisher Mod´ elisation Math´ ematique et Analyse Num´ erique ANALYSIS AND IMPROVEMENT OF A SEMI-LAGRANGIAN EXPONENTIAL SCHEME FOR THE SHALLOW-WATER EQUATIONS ON THE ROTATING SPHERE Jo˜ao Guilherme Caldas Steinstraesser1, Martin Schreiber2, 3 and Pedro S. Peixoto1 Abstract. In this work, we study and extend a class of semi-Lagrangian exponential methods, which combine exponential time integration techniques, suitable for integrating stiff linear terms, with a semi-Lagrangian treatment of nonlinear advection terms. Partial differential equations involving both processes arise for instance in atmospheric circula- tion models. Through a truncation error analysis, we show that previously formulated semi-Lagrangian exponential schemes are limited to first-order accuracy due to the ap- proximation of the integration factor acting on the discretization of the linear term; we then formulate a new discretization leading to second-order accuracy. Also, a detailed stability study is conducted to compare several Eulerian and semi-Lagrangian exponen- tial schemes, as well as a well-established semi-Lagrangian semi-implicit method, which is used in operational atmospheric models. Numerical simulations of the shallow-water equations on the rotating sphere are performed to assess the orders of convergence, sta- bility properties, and computational cost of each method. The proposed second-order semi-Lagrangian exponential method was shown to be more stable and accurate than the previously formulated schemes of the same class at the expense of larger wall-clock times; however, the method is more stable and has a similar cost compared to the well- established semi-Lagrangian semi-implicit method; therefore, it is a competitive candidate for potential operational applications in atmospheric circulation modeling. 2020 Mathematics Subject Classification. 65M12, 65M22, 76U60. . Keywords and phrases: Exponential integrator, semi-Lagrangian, accuracy and stability analysis, shallow-water equations on the rotating sphere, atmospheric modeling 1 Applied Mathematics, Universidade de Sao Paulo, S˜ ao Paulo, S˜ ao Paulo, CEP 05508-090, Brazil 2 Universit´ e Grenoble Alpes / Laboratoire Jean Kuntzmann, Saint-Martin-d’H` eres, 38400, France; Inria AIRSEA team, Grenoble, 38058, France 3 Technical University of Munich, Garching, 85748, Germany © EDP Sciences, SMAI 1999 arXiv:2405.02237v4  [math.NA]  24 Apr 20252 TITLE WILL BE SET BY THE PUBLISHER 1. Introduction Exponential integration methods are a large class of time-stepping schemes that have raised increasing interest for the integration of time-dependent partial differential equations containing stiff linear operators, since they allow integrating exactly (or at least very accurately) the linear terms of the governing equations, thus overcoming stability constraints related to their stiffness [16, 24]. These methods require the evaluation of matrix exponentials, which is a main challenge for their efficient implementation. The recent development of accurate and efficient linear algebra methods has directly contributed to the development and popularization of exponential integration schemes [24], which have been increasingly seen as viable alternatives to overcome Courant-Friedrichs-Lewy (CFL)-related time step limitations usually found in complex fluid dynamics problems such as in atmospheric modeling [13]. The development of exponential integration methods goes back to 1960, by [6], in which the variation-of-constants formula was used to derive the scheme, leading to the family of Exponential Time Differencing (ETD) methods. An alternative approach, proposed by [19], consists of defin- ing a change of variables leading to a differential equation in which the linear operator appears only as arguments of exponential functions, a class of exponential schemes known as Integrating Factor (IF) methods. Both classes are derived considering a partition of the governing equations into linear and nonlinear terms. A third well-known class of exponential schemes is composed of the exponential Rosenbrock methods [15], which are derived from a linearization of the full time evolution operator, thus requiring the computation of its Jacobian. Since then, several ETD, IF and Rosenbrock exponential methods have been proposed, as well as novel classes of exponential schemes. Detailed reviews on exponential integration methods can be found, e.g., in [16,24]. These methods have been studied and applied in a very diverse range of contexts. In particular, sev- eral recent works have focused on geophysical fluid dynamics, in particular atmospheric circulation models, which is the application domain targeted in this work. [1,7] assessed good stability proper- ties and improved accuracy of Rosenbrock-type exponential schemes, compared to more traditional semi-implicit methods, when applied to the shallow-water equations (SWE) on the rotating sphere, but highlighted the need for more efficient approaches for computing the exponential of large ma- trices. Similar conclusions have been drawn by [2] comparing various time integration schemes in the context of the one-dimensional SWE. The challenge of improving the exponential computa- tion has been tackled by [13,21,42]. Applications to more complex, three-dimensional atmospheric models have been investigated by [28, 29], with stable and accurate integration of both slow and fast atmospheric dynamics. Finally, methods considering rational approximations to the exponen- tial functions, which lead to parallel-in-time methods, have been investigated for the SWE on the rotating sphere by [32,33]. More recently, [26] proposed a class of semi-Lagrangian Runge-Kutta-type ETD schemes (SL- ETDRK) to tackle advective PDEs, inspired by the Eulerian ETDRK methods proposed by [8]. Semi-Lagrangian schemes, which follow the trajectories of particles along each time step instead of relying on fixed spatial grids as in Eulerian approaches, are popular methods in atmospheric modeling due to their improved stability properties. This improved stability was indeed verified for the proposed SL-ETDRK schemes in [26], through the numerical simulation of the SWE on the bi- periodic plane with a spectral discretization in space. A semi-Lagrangian exponential method has also been developed by [35], based on the Rosenbrock exponential approach, leading to a second- order scheme whose accuracy was confirmed on numerical tests of the SWE on the rotating sphere considering a cubed-sphere discretization.TITLE WILL BE SET BY THE PUBLISHER 3 This work proposes a better analytical understanding of convergence and stability properties of the SL-ETDRK schemes, as well as improvements for these methods. Indeed, although these methods have been derived analogously to their respective Eulerian counterparts ETD1RK and ETD2RK (respectively first- and second-order accurate) [8], the expected second-order convergence of SL-ETD2RK is not verified in practice. The numerical simulations presented by [26] reveal that both SL-ETD1RK and SL-ETD2RK are first-order accurate. Despite some indications that it could be related to the approximation for the integration factor arising on the derivation of the schemes, its influence on the overall accuracy of SL-ETD2RK was not completely clear Here, we prove that the observed first-order accuracy of SL-ETD2RK is indeed related to the approximation of the integration factor, more specifically in the discretization of the linear term, which involves both an exponential operator and a spatial interpolation coming from the semi-Lagrangian approach. This issue is naturally absent in Eulerian schemes, in which the exponential integrates exactly the linear term. Then, we propose an alternative discretization, consisting of a splitting of the exponential operator, and prove that it leads indeed to second-order accuracy. We also conduct a detailed analytical stability study of both Eulerian and semi-Lagrangian ETDRK schemes, as well as the popular semi-Lagrangian semi-implicit SETTLS method [17] used in operational atmospheric mod- els; we show in particular that standard linear stability analyses are not able to provide concluding insights on the stability of semi-Lagrangian exponential schemes due to a lack of commutation be- tween exponential and interpolation operators; therefore, we conduct an empirical simulation-based stability study following a procedure described by [27]. Finally, we perform numerical experiments for the assessment of convergence and stability of the proposed improved schemes, which demon- strate its advantages compared to the previous Eulerian and semi-Lagrangian exponential methods and the semi-Lagrangian semi-implicit SETTLS. In terms of application, our focus here is on the use of semi-Lagrangian exponential methods in the context of atmospheric modeling. Atmospheric circulation models are characterized by a vast range of waves propagating at different speeds, with the fast ones being responsible for often severe stability constraints, which has motivated the development of various discretization methods searching to establish a good trade-off between stability, accuracy, and computational cost [43]; therefore, combining the semi-Lagrangian and exponential approaches could better handle both advection processes and the stiffness of the linear terms. In particular, we expect semi-Lagrangian exponential methods to be more stable than Eulerian exponential ones, and more accurate than traditional semi-Lagrangian schemes (at least when stiff, linear processes have relatively impor- tant magnitude), such that good compromises between computational cost and accuracy could be obtained. We consider, for the numerical experiments and stability analyses conducted here, the SWE on the rotating sphere, using challenging benchmark test cases commonly used in atmospheric modeling research; thus, this work not only analyses and improves the semi-Lagrangian exponential methods proposed by [26], but it also extends their application domain since [26] focused on the SWE on the plane. Finally, we also compare the exponential semi-Lagrangian scheme to the well- established SL-SI-SETTLS, in order to assess the competitiveness of semi-Lagrangian exponential methods. This paper is organized as follows: the shallow-water equations on the rotating sphere are in- troduced in Section 2; the time integrators considered in this work (the Eulerian ETDRK schemes by [8], the SL-ETDRK schemes by [26] and SL-SI-SETTLS by [17]) are presented in Section 3; a truncation error analysis indicating the sources of the first-order accuracy of the SL-ETD2RK scheme is developed in Section 4; an effectively second-order accurate semi-Lagrangian exponen- tial method is proposed in Section 5, as well as its truncation error and computational complexity4 TITLE WILL BE SET BY THE PUBLISHER analyses; a detailed linear stability study comparing the various schemes considered in this work is conducted in Section 6; numerical simulations of the SWE on the rotating sphere are performed in Section 7 in order to assess and compare stability, convergence and computational cost properties; and conclusions are presented in Section 8. 2. The shallow-water equations on the rotating sphere The shallow-water equations on the rotating sphere are a popular two-dimensional model in the early stages of atmospheric circulation research since it is simplified w.r.t. more complete, three- dimensional equations but still contains most of the challenges related to the spatial discretization of atmospheric models. Therefore, it allows an easier but insightful study of spatial and temporal discretization schemes for atmospheric models [44]. The equations read ∂ ∂t U = LG(U) + LC(U) + NA(U) + NR(U) + Lν(U) + b, (2.1) where the solution vector U = (Φ, ξ, δ)T contains the geopotential Φ = Φ + Φ′ = gh, vorticity ξ := z·(∇× V ) and divergence δ := ∇· V fields. Φ, ξ and δ are functions of (λ, µ, t), where λ is the longitude, µ = sin(θ), θ is the latitude andt is the time. g denotes the gravitational acceleration,h is the fluid depth, Φ and Φ′ are respectively the mean geopotential and the geopotential perturbation, V := (u, v)T is the horizontal velocity field and z is the unit vector in the vertical direction. The RHS of (2.1) is decomposed as a sum of linear and nonlinear terms accounting for different physical processes: LG accounts for linear processes related to the gravitation; LC contains linear rotation- related phenomena, described by the Coriolis parameterf = 2Ω sinθ, where Ω is the Earth’s angular velocity; NA is the nonlinear advection term, NR contains the remaining nonlinear terms, Lν(U) is a linear term accounting for an artificial (hyper-)viscosity of order q and coefficient ν ≥ 0, with q even, and b is related to the topography field b. These terms are given by LG(U) =   0 0 −Φ 0 0 0 −∇2 0 0  U, LC(U) =   0 −∇ ·(fV ) z · ∇ ×(fV )   NA(U) =   −V · ∇Φ −∇ ·(ξV ) −∇2 \u0000V ·V 2 \u0001 + z · ∇ ×(ξV )  , NR(U) =   −Φ′δ 0 0  , Lν(U) = (−1) q 2 +1ν   ∇qΦ′ ∇qξ ∇qδ  , b =   0 0 −g∇2b  . (2.2) 2.1. Spectral discretization In this work, the spatial dimension is discretized considering a spectral approach using spherical harmonics, which is briefly described below. Details can be found in [39]. Let Ym,n denote the spherical harmonic function of zonal and total wavenumbers m and n, respectively. The spherical harmonics expansion of a given smooth field ψ reads ψ(λ, µ, t) = ∞X m=−∞ ∞X n=|m| ψm,n(t)Ym,n(λ, µ) (2.3)TITLE WILL BE SET BY THE PUBLISHER 5 where ψm,n is the expansion coefficient corresponding to the wavenumber pair ( m, n). We consider a triangular truncation with resolution M: ψ(λ, µ, t) = MX m=−M MX n=|m| ψm,n(t)Ym,n(λ, µ) (2.4) As described in the next section, the use of a spectral discretization based on spherical harmonics, combined with a proper arrangement of the terms of the governing equations, is essential for ensuring an efficient computation of matrix exponentials in both Eulerian and semi-Lagrangian exponential methods. 3. Time integrators We present in this section the time stepping schemes considered in the work. We begin by briefly presenting the Eulerian exponential integration methods ETD1RK and ETD2RK [8], on which their semi-Lagrangian versions are based. Then, as an introduction to semi-Lagrangian schemes, we present the semi-Lagrangian semi-implicit SETTLS [17], which we will also compare in the numerical experiments. Finally, we present the semi-Lagrangian exponential schemes proposed by [26]. As explained below, we propose a modification to the name of this last class of methods in order to take into account their actual order of convergence. 3.1. Eulerian Runge-Kutta-type exponential integration methods (ETDRK) Exponential Time Differencing (ETD) schemes are a class of exponential integration methods to solve time-dependent problems of the form dU dt = LU + N(U), U(t = 0) = U0, (3.1) which are obtained, e.g., from a spatial discretization of a PDE, where U ∈ RN , L ∈ RN×N is a linear operator and N : RN → RN is a nonlinear function. As a common feature, exponential methods integrate the linear term LU very accurately (or exactly, down to machine precision, if the exponential terms can be evaluated exactly), by writing its evolution using the exponential of the linear operator, whereas an approximation is proposed to compute the nonlinear term. In the following paragraphs, we briefly describe the main ideas behind exponential methods, more specifically concerning the schemes considered in this work, namely ET1DRK and ETD2RK [8]. We refer the reader to [8,16] for further details on exponential integration methods and, in particular, to [26] for their derivation through the solution of an integration factor problem, which is also the starting point for the derivation of semi-Lagrangian exponential schemes in Section 3.3.. The exact solution of (3.1) at time tn+1 = tn + ∆t, integrated from U(tn), reads U(tn+1) = P−1 n (tn+1)U(tn) + P−1 n (tn+1) Z tn+1 tn Pn(s)N(U(s))ds = e∆tLU(tn) + Z tn+1 tn e−(s−tn)LN(U(s))ds, (3.2) which is the well-known variation-of-constants formula, where, assuming that L does not depend on time, the integration factor6 TITLE WILL BE SET BY THE PUBLISHER Pn(t) = e−(t−tn)L ∈ RN×N (3.3) is the solution of the integration factor problem dPn dt (t) = −Pn(t)L, P n(tn) = I. (3.4) The first term in (3.2) is the exact integration of the linear term. Different exponential methods can be formulated proposing different approximations to the integral containing the evolution of the nonlinear term. In particular, ETDRK methods are a class of Runge-Kutta type exponential schemes proposed by [8], with improved stability properties w.r.t. previous exponential methods. The first- and second-order ETDRK schemes, named respectively ETD1RK and ETD2RK, read Un+1 ETD1RK = φ0(∆tL)Un + ∆tφ1(∆tL)N(Un), (3.5a) Un+1 EDT2RK = Un+1 ETD1RK + ∆tφ2(∆tL) \u0002 N(Un+1 ETD1RK) − N(Un) \u0003 , (3.5b) where φ0(z) = ez, φk(z) = z−1 (φk−1(z) − φk−1(0)) , k ≥ 1 (3.6) with singularities at z = 0 being computed via series expansions in its neighborhood. The functions φk are defined both when z is a scalar or a matrix (in particular, we are interested in the case z = ∆tL). The integrals in the first- and second-order methods are approximated by considering respectively constant and linear approximations to the nonlinear function along [ tn, tn+1]: N(U(s)) ≈ N(U(tn)) N(U(s)) ≈ N(U(tn)) + s − tn ∆t [N(UETD1RK(tn+1)) − N(U(tn))] (3.7) As mentioned above, these schemes solve the linear term exactly, if the exponential terms can be computed exactly (down to machine precision), which is the case,e.g., of scalar equations. However, in the case of systems of equations, this is possible only if the matrix exponentials ez = P∞ k=0 zk/k! can be computed exactly. In general, the matrix exponential must be evaluated approximately, and the efficiency of traditional algorithms limits their practical implementation in geophysical contexts [13, 30] thus requiring the development of more efficient algorithms. Some examples, that have been applied to atmospheric modeling, are Krylov subspace methods with incomplete orthogonalization [13] and rational approximation to exponential integrators [14,33]. Nevertheless, if z is diagonalizable, then the matrix exponential can be computed exactly [26]: if z = QΛQ−1, where the columns ofQ are the eigenvectors ofz and Λ is a diagonal matrix containing the respective eigenvalues, then φk(z) = Qφk(Λ)Q−1, k ≥ 0, (3.8) where φk(Λ) can be computed elementwise since Λ is diagonal. We make use of this property to evaluate the matrix exponentials in a relatively efficient manner. We remark that, in the case of the SWE on the rotating sphere, the linear term LG + LC is not diagonalizable, since the Coriolis parameter is spatially dependent. In the case where f is constant (e.g., in the so-called f-plane approximation), LG + LC is diagonalizable, which is taken intoTITLE WILL BE SET BY THE PUBLISHER 7 account by [26] to apply exponential schemes to the SWE on the plane. In this work, we overcome this issue by including the Coriolis effects into the nonlinear term, i.e., we consider as linear and nonlinear terms respectively L := LG and N := LC +NA +NR +b. Alternative splitting between linear and nonlinear terms is also done in operational models, e.g., the Integrated Forecast System of the European Centre for Medium-Range Weather Forecast (IFS-ECMWF) [11], in which the linear Coriolis term is incorporated into the nonlinear advection. Since we consider a spectral discretization using spherical harmonics, the time integration (3.1) is performed for the spectral coefficients Um,n := (Φm,n, ξm,n, δm,n)T with zonal and total wavenum- bers m and n, respectively. In the spectral space, LU becomes, for each wavenumber pair ( m, n), LUn,m, where L is the symbol matrix of L = LG, given by: L =   0 0 −Φ 0 0 0 −Dn 0 0   (3.9) where Dn := −n(n + 1)/a2 and a is the Earth’s radius. The eigenvalues of the symbol matrix, which are the diagonal elements of Λ in (3.8), read − p −DnΦ, 0 and p −DnΦ. An alternative approach to obtain a diagonalizable linear operator would be to use spectral methods considering Hough harmonics [20], which are eigenmodes of LG + LC, as basis functions. This is done by [42] to solve the SWE on the rotating sphere using exponential methods. However, implementing the Hough decomposition is much less straightforward and computationally more expensive than the spherical harmonics one, for which there are efficient available open-source libraries, e.g., SHTns [31]; therefore, we consider the latter in this work. In terms of computational efficiency, it is important to notice that, for a fixed problem ( i.e., for a fixed linear operator L), the φk functions evaluated at ∆ tL, as used in the ETDRK schemes, depend only on the time step. Therefore, if the time step is constant during a numerical simulation, these functions can be precomputed. This may lead to a significant reduction in terms of computing time, since the φk functions need to be computed for every pair of wavenumbers ( m, n). 3.2. SL-SI-SETTLS Semi-Lagrangian (SL) schemes are spatio-temporal discretization methods for advective PDEs that combine the Eulerian and Lagrangian approaches. The former is based on fixed spatial grids and usually has restrictions on the time step size imposed by Courant-Friedrichs-Lewy (CFL) stability conditions, and the latter consists of following fluid particles along their trajectories, which allows the use of larger time steps but is more complex in terms of implementation compared to fixed grid approaches. The SL approach follows the Lagrangian trajectories only along individual time steps: the trajectories arriving at each point of the fixed spatial grid at time tn+1 are estimated, and a spatial interpolation procedure is performed to retrieve the solution at the departure points at time tn. This simplified Lagrangian approach, which still allows the use of relatively large time step sizes [37], explains the popularity of SL methods, which are used by several weather and climate agencies [23]. An important example of SL methods is the semi-Lagrangian semi-implicit SETTLS (SL-SI- SETTLS), a second-order scheme used in the IFS-ECMWF [11]. This scheme uses a semi-implicit (Crank-Nicolson) approach for the linear terms and the Stable Extrapolation Two-Time-Level Scheme (SETTLS), proposed by [17], for estimating the Lagrangian trajectories ( t, x(t)) at each time step [ tn, tn+1]. The trajectories between the departure point ( tn, xd) and the arrival (grid)8 TITLE WILL BE SET BY THE PUBLISHER point (tn1 , xj) are considered to be linear and determined by the velocity field v at its midpoint (tn+1/2, xm), where tn+1/2 := tn + ∆t/2. This velocity field, on its turn, is approximated by a lin- ear interpolation between v(tn, xd) and v(tn+1, xj), with this last one being estimated via a linear extrapolation using the two previous time steps: xj − xd = ∆tv(tn+1/2, xm) = ∆t 2 [2v(tn, xd) − v(tn−1, xd) + v(tn, xj)] . (3.10) For each grid point xj, Eq. (3.10) is solved iteratively to obtain the departure point xd. Then, SL-SI-SETTLS applied to (2.1) reads Un+1 − Un ∗ ∆t = 1 2 \u0000 LUn+1 + (LUn)n ∗ \u0001 + 1 2 \u0010 [2 ˜N(Un) − ˜N(Un−1)]n ∗ + ˜N(Un) \u0011 (3.11) where quantities denoted by the subscript ∗ are interpolated to the departure point xd at time instant tn. As in the IFS-ECMWF model, we consider the linear term to include only gravity- related processes, i.e., L := LG; also, we consider both the Coriolis- and the bathymetry-related linear terms to be treated as nonlinear ones, such that ˜N := NR + LC + b. 3.3. SL-ETDRK A semi-Lagrangian version of the ETDRK schemes is proposed by [26], aiming to tackle problems of the form DU Dt = LU + ˜N(U), U(t = 0) = U0, (3.12) where D/Dt := ∂/∂t + V · ∇is the material derivative, V is a velocity field, ˜N is a nonlinear term not containing the nonlinear advection V · ∇and U = U(t, x(t)) is the solution defined on the Lagrangian trajectories. The schemes are derived analogously to the Eulerian case (Section 3.1): the exact solution of (3.12) reads U(tn+1, x(tn+1)) = P−1 n (tn+1)U(tn, x(tn)) + P−1 n (tn+1) Z tn+1 tn Pn(s) ˜N(U(s, x(s)))ds, (3.13) where Pn(t) is the solution of an integration factor problem along the Lagrangian trajectory in [tn, tn+1], assumed to exist and be invertible for all times. If L depends on the space, then it depends on the specific Lagrangian trajectory ( t, x(t)). Note that this is the case of the SWE on the rotating sphere (2.1), since L is a differential operator; if L were constant, then the derivative (i.e., the application of L) would be constant along the trajectory. Let Lx(t) denote the trajectory- dependent linear operator. If it satisfies Lx(t)Lx(s) = Lx(s)Lx(t) for every t, s, then Pn reads Pn(t) = e− R t tn Lx(s)ds. (3.14) In [26], the linear term is assumed to be constant along each trajectory and each time step, such that the integration factor assumes the same form as in the Eulerian exponential schemes presented in Section 3.1. The schemes analogous to ETD1RK and ETD2RK were named respectively SL- ETD1RK and SL-ETD2RK, in which first- and second-order approximations are proposed to the nonlinear term. However, as will be shown in Section 4, the approximation of the linear term isTITLE WILL BE SET BY THE PUBLISHER 9 actually a first-order one, such that both schemes are globally first-order. Therefore, we rename these methods respectively as SE11 and SE12, where the first and second numbers refer to the orders of approximation of the linear and nonlinear terms, respectively. These schemes read Un+1 SE11 = φ0(∆tL) h Un + ∆tψ1(∆tL) ˜N(Un) in ∗ , (3.15a) Un+1 SE12 = Un+1 SE11 + ∆tφ0(∆tL) h ψ2(∆tL) ˜N(Un+1 SE11) − \u0010 ψ2(∆tL) ˜N(Un) \u0011n ∗ i , (3.15b) where the functions ψk also satisfy the property (3.8) and are defined by ψk(z) := (−1)k+1φk(−z) + k−1X l=1 φl(−z), k ≥ 1. (3.16) The semi-Lagrangian trajectories are estimated as in SL-SI-SETTLS, by solving (3.10) iteratively. Finally, in (3.15a) and (3.15b), the nonlinear term does not contain the advection effects, which are treated by the SL approach. Thus, the only nonlinear term included in ˜N is NR, such that ˜N = NR + LC + b. As in the Eulerian exponential schemes, the treatment of the Coriolis effects as a nonlinear term allows us to compute the matrix exponentials accurately using (3.8), since L := LG. We remark that, in the SL framework, the Coriolis term could also be dealt with by considering it as an advected quantity [40]. 4. Source of the first order accuracy of SE12 We demonstrate in the following paragraphs that the term φ0(∆tL)[Un]n ∗ is responsible for the first-order accuracy of SE12, despite a second-order approximation to the nonlinear term. More specifically, it is due to the approximation of the integral in the solution (3.14) of the integration factor problem by considering the integrand to be constant in [ tn, tn+1]. For that, we compute the truncation error of the scheme Un+1 = φ0(∆tL)[Un]n ∗ (4.1) approximating the linear PDE, Du Dt = Lu, D Dt := ∂ ∂t + v ∂ ∂x , (4.2) where v = v(t, x) is a velocity field and L = L(x) is a space-dependent linear operator. For the sake of conciseness, we restrict ourselves to the one-dimensional case in space, with a homogeneous spatial discretization with mesh size ∆ x, and we assume that all quantities are scalars. In Section 5, in which we propose a second-order method, some remarks are necessary in the case of systems of PDEs. Along each Lagrangian trajectory ( t, x(t)), (4.2) reduces to the ODE du dt (t, x(t)) = L(x(t))u(t, x(t)). (4.3) Consider a spatial discretization of (4.3), with P + 1 points x0, . . . , xP and homogeneous step ∆x. Let Γ n+1 j (t) := (t, xj(t)) be the Lagrangian trajectory in [ tn, tn+1], arriving at the grid point xj at time tn+1, such that xj(tn+1) = xj. Under this notation, we also define un ej := u(tn, xej ) :=10 TITLE WILL BE SET BY THE PUBLISHER u(tn, xj(tn)) the exact solution computed at the exact departure point xej of the trajectory Γ n+1 j ; and un dj := u(tn, xdj ) the exact solution computed at the approximated departure point xdj of the trajectory Γ n+1 j . The linear operator defined along Γ n+1 j is denoted by Lxj(t). Some of these definitions are presented in Figure 1. x t tn tn+1 xj xdj xej • • • s = ∆tV n+1/2 m Γn+1 j Figure 1. Exact Lagrangian trajectory Γn+1 j (in blue) in [tn, tn+1], and estimated trajectory provided by SETTLS (in red), where V n+1/2 m is an estimation of the velocity field on its midpoint ( tn+1/2, xm) (see Eq. (3.10)). This discretization leads to a system of uncoupled ODEs which are approximated by (4.1) as Un+1 j = φ0(∆tLj)Un ∗j , j = 0, . . . , P (4.4) where w∗j , for a given function w defined on the spatial grid, denotes interpolation to the estimated departure point xdj , and Lj := Lxj(tn+1) = L(xj). Using cubic Lagrange interpolation to the approximated departure point xd, it is well known that w∗j = wdj + O(∆x4) if w is smooth enough. It is also well known that SETTLS (3.10) provides a second-order approximation to the solution computed at the true departure point xej , more precisely, udj = uej + O(∆t3) + O(∆t∆x2) [10, Chapter 7]. Combining these results, we get, in the case of SETTLS using cubic Lagrange interpolation, u∗j = udj + O(∆x4) = uej + O(∆t3) + O(∆t∆x2) + O(∆x4) (4.5) From (4.4), it is clear that the exponential function is computed at the arrival point xj = xj(tn+1) of each Lagrangian trajectory. In other words, the integral in Pn(tn+1) (see Eq. (3.14)) is ap- proximated by considering the integrand to be constant in [ tn, tn+1] and equal to Lxj(tn+1). Note that it is coherent with the interpretation of the order in which the operators are applied in (4.1): the solution is obtained at tn+1 through interpolation, then the linear operator, also evaluated at tn+1, is applied to it. This statement also allows us to evaluate the error in the approximation of the integral in (3.14). From the theory of numerical integration using Newton-Cotes formulae (see e.g., [38, chapter 3]), it is well known thatTITLE WILL BE SET BY THE PUBLISHER 11 Z tn+1 tn Lxj(s)ds = ∆tLxj(tn+1) + C1,j∆t2 = ∆tLj + C1,j∆t2 (4.6) for a given constant C1,j depending on the first derivative ofL along the trajectory Γn+1 j . Replacing (4.6) in (3.14) and denoting by Pnj (t) the integration factor (3.14) defined along the Lagrangian trajectory Γn+1 j , we obtain Pnj (tn+1) = e−∆tLj e−C1,j∆t2 . (4.7) We are now able to obtain the local truncation error τn+1 j at (tn+1, xj) of (4.1) applied to (4.2). The exact solution of (4.3) along Γ n+1 j reads un+1 j = u(tn+1, xj(tn+1)) = P−1 nj (tn+1)u(tn, xj(tn)) = P−1 nj (tn+1)un ej . (4.8) Replacing the exact solution into the j-th equation of (4.4), and using (4.5), (4.7) and (4.8), yields τn+1 j = 1 ∆t h un+1 j − φ0(∆tLj)un ∗j i = 1 ∆t h P−1 nj (tn+1)un ej − e∆tLj un ∗j i = 1 ∆t h e∆tLj eC1,j∆t2 un ej − e∆tLj \u0010 un ej + O(∆t3) + O(∆t∆x2) + O(∆x4) \u0011i = 1 ∆t h e∆tLj \u0000 1 + C1,j∆t2 + O(∆t4) \u0001 un ej − e∆tLj \u0010 un ej + O(∆t3) + O(∆t∆x2) + O(∆x4) \u0011i = O(∆t) + O(∆x2) + O \u0012∆x4 ∆t \u0013 , (4.9) confirming that the scheme (4.1) provides a first-order approximation to (4.2). Note that this first- order is due exclusively to the term eC1,j∆t2 , which arises from the approximation to the integral in the integration factor Pnj (tn+1); all other approximations are second-order accurate. 5. Second-order semi-Lagrangian exponential integration method The truncation error analysis presented above indicates that the approximation of the integral in the integration factor (3.14) is responsible for the first-order accuracy of the SE12 scheme. Therefore, we provide an alternative formulation for this approximation which is effectively a second- order one. A natural approach is to consider an O(∆t3) approximation the integral; for instance, the trapezoidal rule. In the scalar case, using the notation defined in the previous section, it reads Z tn+1 tn Lxj(s)ds = ∆t 2 \u0002 Lxj(tn+1) + Lxj(tn) \u0003 + C2,j∆t3 = ∆t 2 \u0002 Lj + Lej \u0003 + C2,j∆t3 (5.1) for a constantC2,j depending on the second derivative ofL along Γn+1 j , and Lej := Lxj(tn) = L(xej ). Then, the integration factor (3.14) along the Lagrangian trajectory Γ n+1 j satisfies12 TITLE WILL BE SET BY THE PUBLISHER Pnj (tn+1) = e− R t tn Lxj(s)ds = e−∆t 2 [Lj+Lej ]−C2,j∆t3 = e− ∆tLj 2 e− ∆tLej 2 e−C2,j∆t3 , (5.2) yielding the scheme Un+1 = φ0 \u0012∆tL 2 \u0013\u0014 φ0 \u0012∆tL 2 \u0013 Un \u0015n ∗ (5.3) Note that the last equality in (5.2) holds because Lj and Lej are scalars. The order in which the operators are applied in (5.3) (application of the exponential inside the brackets, followed by a spatial interpolation, followed by a second application of the exponential) arises from (5.2) combined with a similar interpretation as in the first-order scheme: half time step with the linear operator, evaluated at tn (i.e., at the departure point, Lej ), is applied to the solution computed at tn; then the solution is obtained at tn+1 through interpolation; finally, half time step with the linear operator at tn+1 (i.e., at the arrival point, Lj) is applied to the solution. In the vectorial case DU Dt = LU (5.4) with U ∈ RN , L ∈ RN×N , we propose an analogous scheme, constructed directly from (5.3): Un+1 = φ0 \u0012∆tL 2 \u0013\u0014 φ0 \u0012∆tL 2 \u0013 Un \u0015n ∗ (5.5) Note, however, that an analogous derivation is not necessarily valid. Indeed, we have, in general, e−∆t 2 [Lj+Lej ] ̸= e−∆t 2 Lj e−∆t 2 Lej (5.6) except if Lj and Lej commute. In Section 5.1 we demonstrate the second-order accuracy of (5.3), in the scalar case, and of (5.5) in the vectorial case when Lj and Lej commute; the numerical simulations presented in Section 7 indicate that the second-order accuracy is preserved even if this commutation hypothesis does not hold. Applying (5.5) to the full PDE (3.1), we obtain the methods: Un+1 SE21 = φ0 \u0012∆tL 2 \u0013\u0014 φ0 \u0012∆tL 2 \u0013 Un \u0015n ∗ + φ0(∆tL) h ∆tψ1(∆tL) ˜N(Un) in ∗ , (5.7a) Un+1 SE22 = Un+1 SE21 + ∆tφ0(∆tL) h ψ2(∆tL) ˜N(Un+1 SE21) − \u0010 ψ2(∆tL) ˜N(Un) \u0011n ∗ i (5.7b) where the Lagrangian trajectories are estimated as in the SE11 and SE12 schemes, using SETTLS (Eq. (3.10)). As proved in Section 5.1 and/or verified through numerical simulations in Section 7, (5.7b) provides a second-order approximation to (3.1). Therefore, the names of the methods were chosen following the same reasoning as before. In (5.7a), a second- and a first-order discretizations of the linear and nonlinear terms, respectively, resulting in a global first-order approximation; in (5.7b), a global second-order accuracy is ensured by second-order discretizations of both linear and nonlinear terms. Note that the proposed discretization, using the approximation (5.2) to the integration factor, is applied only to the linear term, i.e., the nonlinear one is discretized as in SE11 (eq. (3.15a))TITLE WILL BE SET BY THE PUBLISHER 13 and SE12 (eq. (3.15b)). As verified in the numerical experiments in Section 7, the discretization of the nonlinear term proposed by [26] in SE12 has second-order accuracy, such that SE22 achieves second-order with a relatively simple approximation of the exponential functions; the application of (5.2) to the nonlinear term would lead to a much more complex formulation, since the integration factor acts both inside and outside the integral in (3.13). In this section, we prove analytically the second-order convergence of the proposed discretization considering only the linear term. In order to provide a more complete comparative study with the original formulation of the SL-ETDRK schemes, as well as with SL-SI-SETTLS and the Eulerian ETDRK methods, we also present an estimate of the computing complexity and, in Section 6, a detailed linear stability analysis. In Section 7, we provide a numerical verification of these aspects considering both a one-dimensional advection equation and, with more details, the full nonlinear SWE on the rotating sphere. It is worth noting that linear and interpolation operators do not commute in general (see Ap- pendix of [26]), such that, even with φ0(∆tL/2)φ0(∆tL/2) = φ0(∆tL), we may have φ0(∆tL)[Un]n ∗ ̸= φ0 \u0012∆tL 2 \u0013\u0014 φ0 \u0012∆tL 2 \u0013 Un \u0015n ∗ . (5.8) Finally, we highlight the similarities and differences of the proposed method w.r.t. the semi- Lagrangian Rosenbrock-type exponential schemes proposed by [35]. In both methods, a splitting of the time step integration is performed in order to achieve second-order accuracy. In [35], this is made by evolving the fluid particles along the Lagrangian trajectory from tn to tn+1/2 := tn +∆t/2, applying the exponential operator at tn+1/2, and evolving the fluid particles fromtn+1/2 to tn. Here, we consider a different splitting, with two applications of the exponential operator and a single application of the semi-Lagrangian approach; also, both applications of the exponential operator are identical, corresponding to a left multiplication by the constant matrix φ0(∆tL/2). Moreover, as described in Section 3.1, we consider a spectral discretization in space, which, with a proper rearrangement of the terms of the governing equations, allows for a straightforward computation of the matrix exponentials, while these are approximated in [35] using Krylov-subspace methods. 5.1. Truncation error analysis The truncation error analysis of the proposed scheme is analogous to the first order scheme. We begin by considering the scalar case, i.e., (5.3) applied to (4.1). It leads to the system of uncoupled ODEs Un+1 j = φ0 \u0012∆tLj 2 \u0013\u0014 φ0 \u0012∆tL 2 \u0013 Un \u0015 ∗j , j = 0, . . . , P (5.9) We begin by inserting the exact solution u into the j−th equation of (5.9) and using the error estimate (4.5) for SETTLS using cubic Lagrange interpolation: un+1 j = φ0 \u0012∆tLj 2 \u0013\u0014 φ0 \u0012∆tL 2 \u0013 un \u0015n ∗j = φ0 \u0012∆tLj 2 \u0013 φ0 \u0012∆tLej 2 \u0013 un ej + O(∆t2) + O(∆t∆x2) + O(∆x4) (5.10) Then, using the exact solution (4.8) and the expression (5.2) for the integration factor along the Lagrangian trajectory Γ n+1 j , we get the local truncation error14 TITLE WILL BE SET BY THE PUBLISHER τn+1 j = 1 ∆t \u0014 un+1 j − φ0 \u0012∆tLj 2 \u0013\u0012 ∆tLej 2 \u0013 un ej + O(∆t2) + O(∆t∆x2) + O(∆x4) \u0015 = 1 ∆t \u0014 eC2,j∆t3 e ∆tLej 2 e ∆tLj 2 un ej − e ∆tLj 2 e ∆tLej 2 un ej + O(∆t3) + O(∆t∆x2) + O(∆x4) \u0015 = 1 ∆t \u0014 (C2,j∆t3 + O(∆t6))e ∆tLej 2 e ∆tLj 2 un ej + O(∆t3) + O(∆t∆x2) + O(∆x4) \u0015 = O(∆t2) + O(∆x2) + O \u0012∆x4 ∆t \u0013 (5.11) confirming that the proposed method is second-order accurate. In the vectorial case (Eq. (5.5) applied to (5.4)), this result is also true if Lj and Lej commute. Indeed, in this case, the last equality in (5.2) would hold and φ0(∆tLj/2) and φ0(∆tLej /2) would commute, such that the third equality in the truncation error analysis (5.11) would be true. In the general case of a non-commutative linear operator L along each Lagrangian trajectory, a more careful analysis would be required. For non-commuting square matrices A, Bof same size, it holds eA+B − eAeB = 1 2 [BA − AB] + third-order terms (5.12) such that, with A = −∆tLj/2 and B = −∆tLej /2, the proposed approximation (5.2) to the integration factor would have a leading error term of order O(∆t2), which would contribute with O(∆t) to the local truncation error, thus ensuring only first-order accuracy. Note, however, that if L is smooth enough, we can expand Lj around Lej along Γn+1 j to get Lej Lj − LjLej = Lej \u0012 Lej + ∆tdL dt |ej + O(∆t2) \u0013 − \u0012 Lej + ∆tdL dt |ej + O(∆t2) \u0013 Lej =∆t \u0012 Lej dL dt |ej − dL dt |ej Lej \u0013 + O(∆t2) = O(∆t) (5.13) such that e−∆t 2 [Lj+Lej ] − e−∆t 2 Lj e−∆t 2 Lej = ∆t2 8 \u0002 Lej Lj − LjLej \u0003 + O(∆t3) = O(∆t3) (5.14) This implies that, if the integration factor can be written in the matrix exponential form (5.2), then it actually satisfies Pnj (tn+1) = e− R tn+1 tn Lxj(s)ds = e−∆t 2 [Lj+Lej ]−C2,j∆t3 = \u0012 e− ∆tLj 2 e− ∆tLej 2 + Cj∆t3 \u0013 e−C2,j∆t3 (5.15) for a given constant Cj accounting for the non-commutation of Lj and Lej . In other words, the splitting of the exponential would introduce an O(∆t3) term that would contribute with O(∆t2) to the local truncation error, thus preserving the second-order accuracy. Nevertheless, it is importantTITLE WILL BE SET BY THE PUBLISHER 15 to notice that, in the case where Lx(t) and Lx(s) do not commute, we would have to additionally assume that the integration factor exists and can be written in the matrix exponential form (3.14), otherwise, the development of the local truncation error (5.11) would not be valid, since we would not be able to write an explicit form to the exact solution un+1 j in (4.8). However, the numerical simulations presented in Section 7 indicate that second-order accuracy is achieved by (5.5) even if this commutation is not asserted. Finally, note that commutation-related problems are known issues in semi-Lagrangian schemes ( e.g., the non-commutation between Laplace transforms and linear differential operators along Lagrangian trajectories, as studied in [22]). 5.2. Computational complexity We now provide an estimate of the computational complexity of the proposed modified semi- Lagrangian exponential methods and the other time integration schemes considered here. In each time step, the following computations are performed: • In the case of exponential methods, the evaluation of the φk and/or the ψk functions. Since these functions are defined recursively, their cost increases for larger k. These functions need to be computed for each pair of wavenumber ( m, n), and we recall that the matrix exponentials are computed in this work considering the diagonalization of the matrix (eq. (3.8)), such that we need only to compute the exponential of the eigenvalues of L = LG (two per wavenumber pair); • In the case of semi-Lagrangian schemes, the trajectories need to be estimated, and spatial interpolations to the departure points need to be performed; • In the case of SL-SI-SETTLS (3.11), the semi-implicit discretization for the linear term requires both the application of L and L−1, the latter through the solution of a Helmholtz equation [40]; • Finally, all schemes require the evaluation of at least one nonlinear term. We take into account reutilizations of the computed terms between consecutive time steps. Table 1 summarizes the number of each operation listed above performed by each time integration scheme. In this table, each φk/ψk function is depicted individually; xd and ()∗ denote respectively the evaluation of the Lagrangian trajectories and the interpolation to the departure points; L and L−1 correspond to the semi-implicit scheme described above; and NA and NR indicate the evaluation of the (respectively advection and remaining) nonlinear terms. Scheme φ0 φ1 φ2 ψ1 ψ2 xd ()∗ L L−1 NA NR ETD1RK 1 1 0 0 0 0 0 0 0 1 1 ETD2RK 1 1 1 0 0 0 0 0 0 2 2 SE11 1 0 0 1 0 1 1 0 0 0 1 SE12 2 0 0 1 2 1 2 0 0 0 2 SE21 3 0 0 1 0 1 2 0 0 0 1 SE22 4 0 0 1 2 1 3 0 0 0 2 SL-SI-SETTLS 0 0 0 0 0 1 2 1 1 0 1 Table 1. Number of operations performed per time step by each time integration scheme. xd and ()∗ denote respectively the evaluation of the Lagrangian trajecto- ries and the interpolation to the departure points; L and L−1 correspond to the semi-implicit scheme.16 TITLE WILL BE SET BY THE PUBLISHER It is clear from this table that the proposed modifications of the semi-Lagrangian exponential schemes introduce additional complexity per time step, with two more exponential evaluations and one more spatial interpolation w.r.t. their respective original formulations. However, as assessed in the numerical simulations presented in Section 7, the improved accuracy and stability of SE22 allow us to choose larger time step sizes. Moreover, it has been identified in the numerical simulations that the computation of the exponential functions is responsible for an important fraction of the computing time in our implementation, such that important savings are possible by precomputing them; additional savings could also be obtained with a more efficient implementation (e.g., by using vectorization). 6. Linear stability analysis In the following paragraphs, we conduct a stability study comparing exponential and semi- Lagrangian exponential schemes, as well as SL-SI-SETTLS. Linear stability analyses of Eulerian exponential schemes have been conducted, e.g., by [3,8,9]. The last two cited works show notably that these methods are in general unstable when applied to non-diffusive problems, but depending on configurations of the problem and the exponential scheme, the amplifications are small such that instabilities may remain unnoticed within the simulation time. In this section, we focus on comparing stability regions of both Eulerian and semi-Lagrangian schemes. The stability study proposed here is inspired by the procedure presented by [8]. Consider a generic nonlinear ODE in the form ˙u = λLu + N(u) (6.1) Substituting u = u + u′ in (6.1), where u is a steady state ( i.e., λLu + N(u) = 0) and u′ is a perturbation, expanding N(u + u′) around u, truncating at first order and defining λN := N′(u), we obtain the linearized equation ˙u′ = λLu′ + λNu′ (6.2) which we consider hereafter, with the primes omitted for conciseness. However, we need to take into account that we are comparing both Eulerian (ETDRK) and semi-Lagrangian (SL-SI-SETTLS and SE**) methods, and this study should be able to capture the influence of advective effects and interpolation procedures. In the case of SL schemes, the advection is incorporated into the material derivative, and we can directly use the same linearized ODE as above: du dt = λLu + λNu (6.3) where u, λL, λN ∈ C. This equation arises, e.g., when (3.1) is solved using a spectral method, in which case u = ˆuk(t) is a spectral coefficient of the solution, and λL, λN are modes associated respectively to L and the linearized term N. If L and N are diagonalizable in the spectral space with the same set of eigenvectors, then (3.1) leads to a system of uncoupled ODEs of the form (6.3), each one corresponding to a wavenumber k. On the other hand, in the case of Eulerian schemes, we need to consider the advection explicitly, leading to a linearized one-dimensional scalar advection equation with source term: ∂u ∂t + v ∂u ∂x = λLu + N(u) =⇒ ∂u ∂t + v ∂u ∂x = λLu + λNu (6.4)TITLE WILL BE SET BY THE PUBLISHER 17 in which the spatial derivative becomes a scalar multiplication when solved in the spectral space, and v is a scalar velocity field assumed to be constant for simplicity. The stability region of a given numerical scheme with stability function A := \f\fun+1\f\f/ |un| is defined as the region in C2 in which |A(∆tλL, ∆tλN)| ≤1, where un = u(tn, x) = Aneiκx is the computed approximation to u(tn), κ = 2πk/L is the normalized wavenumber and L is the length of the spatial domain. A complete stability study should be performed in a four-dimensional space, since A depends on the real and imaginary parts of λL and λN; however, since a hyperbolic problem such as the SWE propagates purely hyperbolic modes, we simplify this study by considering both ξL := ∆tλL and ξN := ∆tλN to be purely imaginary (Figure 2), and, in particular, by fixing values of ξL ∈ iR and plotting the amplification factors as functions of ℑ(ξN) (Figure 3). We present below the stability functions of each time integration scheme considered in this work. 6.1. Stability of ETD1RK and ETD2RK When applying (3.5a) and (3.5b) to (6.4), the advection term could be treated either as a linear or nonlinear term, yielding different stability functions. We consider only the latter case, but the overall conclusions on the stability of ETD1RK and ETD2RK compared to the other schemes considered in this work are similar. We have un+1 ETD1RK = φ0(ξL)un + ∆tφ1(ξL)λNun − ∆tφ1(ξL)ivκun (6.5a) = [φ0(ξL) + φ1(ξL)(ξN − iκs)] un un+1 EDT2RK = un+1 ETD1RK + ∆tφ2(ξL) \u0002 (λN − ivκ)un+1 ETD1RK − (λN − viκ)un\u0003 (6.5b) = un+1 ETD1RK + φ2(ξL) (ξN − iκs) \u0000 un+1 ETD1RK − un\u0001 where s := v∆t = xj − xdj is the spatial displacement of each particle from tn to tn+1 (see Figure 1). Note that we have considered the spatial derivative to be computed exactly (which can be achieved, e.g., by using a spectral method, as considered in this work), i.e., (un)x = iκun, since we want to analyze exclusively the stability properties due to the temporal discretization. Thus, the stability functions of the Eulerian ETD1RK and ETD2RK schemes read AETD1RK = φ0(ξL) + φ1(ξL)(ξN − iκs) (6.6a) AETD2RK = AETD1RK + φ2(ξL) (ξN − iκs) [AETD1RK − 1] (6.6b) Note that the presence of the term −iκs makes the stability regions dependent on the spatial information. This implies that the schemes may not be simultaneously stable for a large range of wavenumbers, since the intersection of their stability regions may be small or even empty. Following [17], we compute stability functions Aκs = Aκs(ξL, ξN) for κs ∈ K:= {0, π/10, 2π/10, . . . ,2π}; then, for each (ξL, ξN), the amplification factor is A(ξL, ξN) = maxκs∈K Aκs(ξN, ξL). 6.2. Stability of SL-SI-SETTLS We now develop a linear stability analysis for the semi-implicit semi-Lagrangian SETTLS [17] described in Section 3.2. In the stability analysis of semi-Lagrangian schemes, we also need to take into account the spatial dependence of the solution. Using u(tn, x) = Aneiκx in (3.11) applied to (6.3), we obtain18 TITLE WILL BE SET BY THE PUBLISHER An+1eiκxj −Aneiκxdj = ξL 2 \u0010 An+1eiκxj + Aneiκxdj \u0011 + ξN 2 \u0010\u0002 2An − An−1\u0003 eiκxdj + Aneiκxj \u0011 (6.7) which, after division by An−1eiκxj and recalling that s := xj −xdj , leads to the quadratic equation A2 \u0012 1 − ξL 2 \u0013 − A \u0012 e−iκs \u0012 1 + ξL 2 + ξN \u0013 + ξN 2 \u0013 + ξN 2 e−iκs = 0 (6.8) whose roots A = ASL-SI-SETTLS define the stability function of SL-SI-SETTLS; its stability region is the intersection between the stability regions defined by each of the two roots of (6.8). Moreover, as in the case of ETD1RK and ETD2RK considering the advection term, these roots depend on the spatial wavenumber through the term e−iκs, and the stability region is defined as the intersection of the individual stability regions for each κs. 6.3. Stability of SE11 and SE12 We now study the stability of the semi-Lagrangian exponential schemes proposed by [26] and described in Section 3.3. Using u(x, tn) = Aneiκx in (3.15a) and (3.15b) applied to (6.3) and the fact that φk(z) = φ0(z)ψk(z) ∀k >0, we get ASE11 = φ0(ξL)e−iκs + ξNφ1(ξL)e−iκs = e−iκsφ0(ξL)[1 + ξNψ1(ξL)] (6.9a) ASE12 = ASE11 + ξNφ2(ξL) \u0002 ASE11 − e−iκs\u0003 (6.9b) = e−iκs [(1 + ξNφ2(ξL)) φ0(ξL) (1 +ξNψ1(ξL)) − ξNφ2(ξL)] Since |e−iks| = 1, the stability regions of both SE11 and SE12 do not depend on κs. This is a re- markable feature of these schemes, since, contrary to what happens to SL-SI-SETTLS and ETDRK, there is no reduction of the global stability region when intersecting the regions corresponding to each wavenumber. We compare in Figure 2 the stability functions of ETD1RK and ETD2RK applied to (6.4), and SL-SI-SETTLS, SE11 and SE12 applied to (6.3), in the plane ( ℑ(ξN), ℑ(ξL)). In Figure 3, we present the amplification factor along two horizontal slices of these planes, with ξL chosen as integer multiples of ˜ξL = i q Φ/a2 ≈ 2.5 × 10−4i, which is obtained from an approximation to the eigenvalues λL = i q Φn(n + 1)/a2 of L = LG considering relevant physical parameters and an f-plane approximation (see [5] for details). The scaling of ˜ξL can be seen as a result of different choices of time step size ∆ t and/or wavenumbers: for instance, the plot considering ξL = 20000˜ξL in Figure 3 corresponds to the stability region of the maximum wavenumber in a spectral resolution M = 256 and a reasonable time step size of approximately ∆ t = 80s. The plots reveal notably an enhancement of the stability of the semi-Lagrangian exponential schemes when compared to their Eulerian counterparts, which suffer from a small intersection of the stability regions of each κs, indicating that the schemes have a lack of stability common to a large range of combinations between wavenumbers and spatial displacement. The semi-Lagrangian exponential scheme with second-order discretization of the nonlinear term (SE12) also presents a considerably larger stability region compared to SL-SI-SETTLS and SE11, whose stability regions coincide, despite of having different stability functions.TITLE WILL BE SET BY THE PUBLISHER 19 −5 0 5 ℑ(ξN) −6 −4 −2 0 2 4 6 ℑ(ξL) 0.0 0.2 0.4 0.6 0.8 1.0 (a) ETD1RK −5 0 5 ℑ(ξN) −6 −4 −2 0 2 4 6 ℑ(ξL) 0.0 0.2 0.4 0.6 0.8 1.0 (b) ETD2RK −5 0 5 ℑ(ξN) −6 −4 −2 0 2 4 6 ℑ(ξL) 0.0 0.2 0.4 0.6 0.8 1.0 (c) SL-SI-SETTLS −5 0 5 ℑ(ξN) −6 −4 −2 0 2 4 6 ℑ(ξL) 0.0 0.2 0.4 0.6 0.8 1.0 (d) SE11 −5 0 5 ℑ(ξN) −6 −4 −2 0 2 4 6 ℑ(ξL) 0.0 0.2 0.4 0.6 0.8 1.0 (e) SE12 Figure 2. Stability regions (in red) of ETD1RK and ETD2RK applied to (6.4) (considering a nonlinear treatment of the advection term), and SL-SI-SETTLS, SE11 and SE12 applied to (6.3), in the ( ℑ(ξN), ℑ(ξL)) plane, considering both ξN and ξL to be purely imaginary. 6.4. Stability of SE21 and SE22 Using the same procedure described above to assess stability properties of the proposed schemes SE21 and SE22 (Section 5) does not provide insights on potential stability improvements or de- creases induced by the splitting of the matrix exponential. In particular, this approach is not able to capture the fact that the matrix exponential and the interpolation do not commute [26]. Indeed, using u(x, tn) = Aneiκx in (5.7a) and (6.3), analogously to (6.9a), provides φ0 \u0012ξL 2 \u0013 e−iκsφ0 \u0012ξL 2 \u0013 = eξL/2e−iκseξL/2 = eξLe−iκs = φ0(ξL)e−iκs (6.10) such that ASE21 ≡ ASE11, and, similarly, ASE22 ≡ ASE12. However, different stability properties are verified in the numerical simulations presented in Section 7. Next, we develop in detail how the differences between the two sets of schemes arise. Consider the ODE (6.3) with only the linear term:20 TITLE WILL BE SET BY THE PUBLISHER −5.0 −2.5 0.0 2.5 5.0 ℑ(ξN) 0.8 1.0 1.2 1.4 1.6 1.8 2.0 |A| ETD1RK ETD2RK SL-SI-SETTLS SE11 SE12 (a) ξL = 0 −5.0 −2.5 0.0 2.5 5.0 ℑ(ξN) 0.8 1.0 1.2 1.4 1.6 1.8 2.0 |A|  (b) ξL = 20000˜ξL Figure 3. Stability functions of ETD1RK and ETD2RK applied to (6.4) (consid- ering a nonlinear treatment of the advection term), and SL-SI-SETTLS, SE11 and SE12 applied to (6.3), as a function of ℑ(ξN) for fixed values of ξL and ℜ(ξN) = 0. The stability regions of ETD1RK and ETD2RK are empty in the case ξL = 0. The horizontal, dashed line indicates the boundary of the stability region requiring |A| ≤1. The same legend holds for both plots. du dt = λLu (6.11) We first consider the first-order discretization used in (3.15a). Since the problem is solved in the spectral space, we may write ˆun+1 k = φ0(ξLk )[ˆun k]∗, k = −M, . . . , M (6.12) where u(t, x) ≈ PM k=−M ˆun keiκx is the truncated Fourier decomposition of u and ξLk := ∆tλLk may depend on k. Note, however, that the interpolation is performed on the grid. Therefore, using the direct and inverse discrete Fourier transforms ˆun k = F(un) = 1 P P−1X j=0 u(tn, xj)e−iκxj , u (tn, xj) = F−1(ˆun) = MX k=−M ˆun keiκxj (6.13) with κ = 2πk/L, L the length of the periodic spatial domain and P + 1 the number of equidistant grid points xi = i∆x, i = 0, . . . , P, ∆x = L/P, we may rewrite (6.12) more rigorously as ˆun+1 k = φ0(ξLk )F([F−1(ˆun)]∗) = φ0(ξLk )F([un j ]∗) = φ0(ξLk ) 1 P P−1X j=0 u(tn, xdj )e−iκxj = φ0(ξLk ) 1 P P−1X j=0 u(tn, xj−pj )e−iκxj = φ0(ξLk ) 1 P P−1X j=0 u(tn, xrj )e−iκxpj+rjTITLE WILL BE SET BY THE PUBLISHER 21 = φ0(ξLk ) 1 P P−1X j=0 u(tn, xrj )e−iκ(pj+rj)∆x, (6.14) where xdj is the approximate departure point at tn of the Lagrangian trajectory arriving at xj at tn+1, rj := j − pj, and, for simplification, xdj = xj−pj , pj ∈ Z, is supposed to belong to the grid (otherwise u(tn, xdj ) may be written as a linear combination ( i.e., interpolation) of the solution computed at grid points). If the velocity field is constant in space, then ∀j, pj = p, thus ˆun+1 k = φ0(ξLk )e−iκp∆x 1 P P−1X j=0 u(tn, xrj )e−iκxrj = φ0(ξLk )e−iκp∆x 1 P P−1X r=0 u(tn, xr)e−iκxr = φ0(ξLk )ˆun ke−iκp∆x = φ0(ξLk )ˆun ke−iκs, (6.15) with s := xj − xdj = xj − xj−p = p∆x. Therefore, for each k, the stability function |ˆun+1 k |/|ˆun k| = φ0(ξLk )e−iκs is the same as derived above (see Eq. (6.9a)), which corresponds, in fact, the exact solution for wavenumber k. However, if the velocity field is not constant, we are not able to factor out e−iκpj∆x from the summation, and the stability function may be different, and, in particular, the exact solution for wavenumber k is no longer obtained; this suggests an intricate relation between spectral transforms, matrix exponentiation, and spatial interpolation. Consider now the proposed modification to the discretization of the linear term. In the spectral space, we solve ˆun+1 k = φ0 \u0012ξLk 2 \u0013 F([F−1( ˆw)n]∗), k = −M, . . . , M (6.16) where ˆwn k := φ0 \u0012ξLk 2 \u0013 ˆun k. (6.17) Conducting the same development as above, ˆun+1 k = φ0 \u0012ξLk 2 \u0013 1 P P−1X j=0 w(tn, xrj )e−iκ(pj+rj)∆x = φ0 \u0012ξLk 2 \u0013 ˆwn k e−iκs = φ0(ξLk )ˆun ke−iκs, (6.18) where the second equality holds if the velocity field is constant. We conclude that, if v is constant, the original and the proposed discretizations of the linear term yield the same stability function. However, if the velocity field is not constant, we have ˆun+1 k = φ0 \u0012ξLk 2 \u0013 1 P P−1X j=0 w(tn, xrj )e−iκ(pj+rj)∆x = φ0 \u0012ξLk 2 \u0013 1 P P−1X j=0 \" MX l=−M ˆwn l eiκlxrj # e−iκ(pj+rj)∆x22 TITLE WILL BE SET BY THE PUBLISHER = φ0 \u0012ξLk 2 \u0013 1 P P−1X j=0 \" MX l=−M φ0 \u0012ξLl 2 \u0013 ˆun l eiκlxrj # e−iκ(pj+rj)∆x ̸= φ0(ξLk ) 1 P P−1X j=0 u(tn, xrj )e−iκ(pj+rj)∆x (6.19) where κl := 2πl/L. Note that the dependency of ξLl on l prevents us from factoring out the expo- nential term from the inner summation, such that the inverse Fourier transform into the brackets does not yield u(tn, xrj ). As a consequence, different stability functions are provided by the two variants of the semi-Lagrangian exponential scheme. Therefore, by explicitly writing the spectral transforms involved in the integration of the semi- Lagrangian exponential schemes, it becomes clear why different discretizations of the linear term lead to different stability functions. We remark, however, that these differences are not necessarily linked to the fact that the equations are solved using a spectral method, but rather to the lack of commutation between the matrix exponential and the interpolation. For illustration, consider the equation du dt = λLu (6.20) defined on the (periodic) physical space, withu = (u1, . . . , uP )T ∈ RP and λL = diag(λ1, . . . , λP ) ∈ RP×P a diagonal matrix with at least one element λi different from the others. Consider also that the interpolation procedure consists simply of a one grid point shift to the right. Therefore, the integration of one time step of (6.20) using the two variants of the semi-Lagrangian exponential scheme yields respectively un+1 = φ0(∆tλL)[un]∗ =   φ0(∆tλ1) 0 ... 0 φ0(∆tλP )     un P un 1 ... un P−1   =   φ0(∆tλ1)un P φ0(∆tλ2)un 1 ... φ0(∆tλP )un P−1   (6.21) and un+1 = φ0 \u0012∆tλL 2 \u0013\u0014 φ0 \u0012∆tλL 2 \u0013 un \u0015 ∗ =   φ0(∆tλ1/2) 0 ... 0 φ0(∆tλP /2)       φ0(∆tλ1/2)un 1 φ0(∆tλ2/2)un 2 ... φ0(∆tλP /2)un P     ∗ =   φ0(∆tλ1/2) 0 ... 0 φ0(∆tλP /2)       φ0(∆tλP /2)un P φ0(∆tλ1/2)un 1 ... φ0(∆tλP−1/2)un P−1     ∗TITLE WILL BE SET BY THE PUBLISHER 23 =   φ0(∆tλ1/2)φ0(∆tλP /2)un P φ0(∆tλ2/2)φ0(∆tλ1/2)un 1 ... φ0(∆tλP /2)φ0(∆tλP−1/2))un P−1   (6.22) with the two schemes being equivalent only if λL is a multiple of the identity matrix. Note that, except for the case in which the velocity field is constant, determining the stability regions of SE** analytically, using (6.14) and (6.19), is not feasible. In particular, we are not able to obtain an expression in the form ˆ un+1 k = A(ξL, ξN)ˆun k for each k separately, and ˆun+1 k depends a priori on all wavenumber modes. It means that we are no longer in the framework of a linear stability analysis, even if the problem (6.3) itself is linear, with this nonlinearity being a consequence of the lack of commutation between the exponential and interpolation operators. We choose therefore to assess the stability properties of the schemes through numerical simulations, as presented in Section 7.3. 7. Numerical tests In this section, we perform a set of numerical simulations of the SWE on the rotating sphere to evaluate the proposed modifications of the semi-Lagrangian exponential methods and to com- pare them with their original versions and with other relevant numerical schemes in this study, namely SL-SI-SETTLS and the Eulerian exponential integration methods. We begin by verifying the numerical orders of convergence using benchmark test cases with increasing complexity; then, we conduct an empirical stability study; finally, we consider a relatively challenging test to evaluate the methods qualitatively and in terms of computational cost, including a study of the application of hyperviscosity approaches. 7.1. Numerical verification of the convergence order considering the advection equation The goal of this section is to provide an initial numerical validation of the truncation error analyses developed in Sections 4 and 5.1, considering the governing equations in the Lagrangian framework and containing only the linear term on the right-hand side. We consider first the scalar, one-dimensional equation (4.2), in the periodic domain Ω = [0 , 10], integrated from t = 0 to t = 10, and a Gaussian curve centered at x = 5 as initial solution. We consider time step sizes ranging between ∆ t = 2 −5 and ∆ t = 1, and the spatial domain is discretized considering P + 1 = 2049 equally spaced points. We consider the first- and second-order schemes SE12 and SE22 (without the nonlinear terms), and, for comparison, we also integrate using SL-SI-SETTLS. A reference solution is computed using a 4th-order explicit Runge Kutta method with time step size ∆ t = 2−5/10 and the same spatial resolution P + 1 = 2049, with the spatial derivatives being computed using the Fourier spectral method with spectral resolution M = 1024. Figure 4a presents the relative L2 errors in two cases, respectively with constant L(x) ≡ 1, and variable L(x) = sin( x). In the former, both semi-Lagrangian exponential methods compute the exact solution (up to the error due to the spectral resolution in the reference simulation), since the integration factor (3.14) is exactly Pn(tn+1) = φ0(∆tL). In the latter case, in which L depends on space (thus on time, along each Lagrangian trajectory), the first- and second-order accuracies derived analytically are observed.24 TITLE WILL BE SET BY THE PUBLISHER In Figure 4b, we present the same results in the vectorial case (5.4), with N = 2 and one spatial dimension. The same discretization parameters as in the scalar examples are considered. We perform simulations for two choices of L(x), respectively L(x) = L1(x) := \u0012 sin(x) cos( x) cos(x) sin( x) \u0013 , L(x) = L2(x) := \u0012 sin(x) sin( x) sin(x) cos( x) \u0013 (7.1) with the property thatL1(x) and L1(y) commutes for everyx, y, which does not hold forL2. In both cases, SE12 provides first-order accuracy, due to the approximation of the integrals considering the space-dependent linear operators to be constant along the trajectories. On the other hand, SE22 is second-order accurate in both cases, including the one in which the linear operator does not commute along the trajectory. The simulations presented in the following paragraphs, considering the SWE on the rotating sphere, for which the commutation assumption does not hold, confirm these results. 10−2 10−1 100 Timestep size 10−13 10−10 10−7 10−4 10−1 Relative L2 error (∆t)1 (∆t)2 Timestep size vs. error SL-SI-SETTLS SE12 SE22 L(x) = 1 L(x) = sin(x) L(x) = 1 L(x) = sin(x) (a) Scalar case 10−2 10−1 100 Timestep size 10−6 10−4 10−2 100 Relative L2 error (∆t)1 (∆t)2 Timestep size vs. error SL-SI-SETTLS SE12 SE22 L(x) = L1(x) L(x) = L2(x) L(x) = L1(x) L(x) = L2(x) (b) Vectorial case (N = 2) Figure 4. Numerical verification of the convergence order of the semi-Lagrangian schemes considered in this work (relative L2 error vs time step size), considering governing equations with only advective and linear terms. 7.2. Numerical verification of the convergence order considering the SWE on the rotating sphere We consider here three test cases to verify the orders of convergence determined analytically. To provide a more detailed insight into the stability properties of the time integration schemes, we present, for each simulation, the CFL numbers related to gravity wave propagation and advection. We consider the characteristic speed c = Φ 1/2 +|u|, where the square root of the mean geopotential, Φ 1/2 = p gh, defines the mean external gravity wave speed, andu is the maximum advective velocity magnitude defined by the initial conditions. A reference CFL number is then defined as c∆t/∆x, where ∆x is the longitudinal grid size along the equator. The test cases considered here are:TITLE WILL BE SET BY THE PUBLISHER 25 • The Williamson’s nonlinear test case 2 [44], consisting of a geostrophically balanced steady solution, with mean geopotential Φ = 2.94×104m2/s2, mean advective velocity |u| ≈39m/s and characteristic speed c ≈ 210m/s. The authors recommend error evaluations after 5 days of integration; here, we execute simulations for 7 days. • The geostrophic balance test with bathymetry proposed by [27], which is a modification of Williamson’s test case 2. In this test case, the fluid depth is constant ( h = h0) and the bathymetry profile is constructed to ensure geostrophic balance. By choosing a relatively small h0, nonlinear effects are not negligible, allowing us to evaluate the influence of the discretization of the nonlinear terms on the convergence order. We consider h0 = 100m and h0 = 1m, with mean geopotential Φ = gh0, mean advective velocity |u| and characteristic speed c respectively (Φ, |u|, c) ≈ (981, 39, 70) and (Φ, |u|, c) ≈ (9.81, 39, 42) (in m2/s2, m/s and m/s). The simulations are also executed for 7 days. • The unstable jet test case proposed by [12], which is a popular and challenging benchmark in atmospheric modeling. In this test, with null bathymetry and whose initial solution is a stationary zonal jet, a small Gaussian bump perturbation on the geopotential field leads to the formation of characteristic vortices and important nonlinear processes that transfers energy along the spectra, and allows enstrophy cascade to the small-scale components of the solution. The mean fluid depth and geopotential are respectively h = 10000m and Φ ≈ 98100 m2/s2, and the advective velocity is |u| = 80m/s, resulting in the characteristic speed c ≈ 393m/s. A similar test case has been used for numerical validation of SE11 and SE12 on the plane by [26] and, as done in that work, errors are measured after one day of simulation, before the formation of the vortices. We also conduct qualitative analyses considering longer times of simulation of this test case. In all test cases and for all schemes, we perform simulations by keeping the ratio between the temporal and spectral resolutions constant, with ∆ t ∈ {60, 120, 240, 480, 960} (s) and M ∈ {512, 256, 128, 64, 32}, such that the CFL numbers are kept constant for each test case (namely, 0.48, 0.16, 0.10 and 0.91, respectively for the Williamson’s test case 2, the geostrophically balanced test case with bathymetry ( h0 = 100m and h0 = 1m), and the unstable jet test case). The errors in each simulation are computed w.r.t. a reference solution integrated using a fourth-order explicit Runge-Kutta scheme, with a time step four times smaller and the same spectral resolution (thus allowing us to analyze exclusively the errors due to the temporal discretization without impos- ing too severe stability constraints by considering a fixed high spectral resolution). None of the simulations use artificial viscosity, i.e., ν = 0. Figure 5 reveals that, among the semi-Lagrangian exponential methods, the second-order accuracy is achieved only by the proposed SE22 scheme, with all the other semi-Lagrangian exponential schemes being of first order. This is observed in all test cases, including those with relatively important nonlinear effects (geostrophic balance with topography and small h0). It shows that both our proposed modification of the linear term and the second-order discretization of the nonlinear one proposed by [26] are required to ensure global second-order accuracy. Figure 5 also confirms our hypotheses on the stability and accuracy of semi-Lagrangian expo- nential methods. These schemes, notably those with a second-order discretization of the nonlinear term, have improved stability properties compared to the Eulerian ones; indeed, ETD2RK and mainly ETD1RK are unstable in most of the simulations. The only simulations in which ETD2RK is stable are those with less important nonlinear effects, meaning that a weaker enstrophy cas- cade takes place, namely the geostrophically balanced steady-state solution (Figure 5a) and the short-run unstable jet simulation (Figure 5d). In both simulations, ETD2RK and SE22 outperform26 TITLE WILL BE SET BY THE PUBLISHER 100.0 1000.0 Timestep size (seconds) 1.0e-08 1.0e-06 1.0e-04 1.0e-02 1.0e+00 1.0e+02 1.0e+04L2 norm on ′ (res. normalized) ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 Timestep size vs. error ETD1RK ETD2RK SL-SI-SETTLS SE11 SE12 SE21 SE22 (a) Geostrophic balance 100.0 1000.0 Timestep size (seconds) 1.0e-07 1.0e-05 1.0e-03 1.0e-01 1.0e+01 1.0e+03L2 norm on ′ (res. normalized) ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 Timestep size vs. error ETD1RK ETD2RK SL-SI-SETTLS SE11 SE12 SE21 SE22 (b) Geostr. balance w/ topography; h0 = 100m 100.0 1000.0 Timestep size (seconds) 1.0e-08 1.0e-06 1.0e-04 1.0e-02L2 norm on ′ (res. normalized) ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 Timestep size vs. error ETD1RK ETD2RK SL-SI-SETTLS SE11 SE12 SE21 SE22 (c) Geostr. balance w/ topography; h0 = 1m 100.0 1000.0 Timestep size (seconds) 1.0e-01 1.0e+00 1.0e+01 1.0e+02L2 norm on ′ (res. normalized) ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 Timestep size vs. error ETD1RK ETD2RK SL-SI-SETTLS SE11 SE12 SE21 SE22 (d) Unstable jet Figure 5. Numerical verification of the convergence order of the schemes con- sidered in this work (normalized L 2 norm of the error on the geopotential field perturbation vs the time step size). Similar results are obtained with the L ∞ norm. the accuracy of the also second-order accurate SL-SI-SETTLS (which develops instabilities in the geostrophic balance test case), reflecting the accurate integration of the linear term by the exponen- tial schemes. Possibly, the observed instabilities in the geostrophic balance test case (which arise after 3-4 days of simulation) are due to the amplification of small departures from the steady-state solution, which may be caused by the time-splitting approach for integrating equations including linear and nonlinear terms; a detailed study on these amplifications is conducted in Section 7.3. On the other hand, SL-SI-SETTLS is more accurate in the geostrophic balance test case with topog- raphy (Figures 5b and 5c), in which the nonlinear effects are more relevant due to the small mean depth h0. In this case, although still convergent with the expected accuracy, the semi-Lagrangian exponential schemes produce larger errors. We remark, however, that the observed second-order accuracy of SE22 in this test case confirms the discussion in Section 4 in the sense that the source ofTITLE WILL BE SET BY THE PUBLISHER 27 100.0 1000.0 10000.0 Timestep size (seconds) 1.0e-01 1.0e+00 1.0e+01 1.0e+02 1.0e+03L2 norm on ′ (res. normalized) ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 ( t)1 ( t)2 Timestep size vs. error ETD1RK ETD2RK SL-SI-SETTLS SE11 SE12 SE21 SE22 Figure 6. Numerical verification of the convergence order of the schemes con- sidered in this work (normalized L 2 norm of the error on the geopotential field perturbation vs the time step size). All simulations consider the same spectral res- olution (M = 256); the reference solution is computed using a fourth-order explicit Runge-Kutta scheme, with M = 512 and ∆t = 2s. the first-order accuracy of SE12 is indeed the discretization of the linear term, and no modification of the discretization of the nonlinear term is required for improving its order of accuracy. Finally, note that some simulations reach an error plateau around 10 −8 − 10−9, which is a consequence of our formulation considering the geopotential perturbation, Φ = Φ + Φ′, with Φ being several orders of magnitude larger than Φ ′; therefore, we consider these solutions to have converged. Since our focus in these first results was on the convergence properties of the schemes, we ensured stability in almost all simulations by considering a fixed CFL number. However, we now highlight the advantages of the proposed SE22 w.r.t. the other schemes. We repeat the simulations of the unstable jet test case, but now with a fixed spectral resolution ( M = 256) and time step sizes chosen in [60, 8192] (s), with approximate smallest and largest CFL numbers c∆t/∆x of 0.45 and 62, respectively. The reference solution is computed using a fourth-order explicit Runge-Kutta scheme, with M = 512 and ∆t = 2s, resulting in an approximate CFL number of 0.03. The results are presented in Figure 6. It is clear that the proposed second-order semi-Lagrangian exponential scheme SE22 “extends” the error curve of the second-order Eulerian exponential method ETD2RK, i.e., it has a largely improved stability range, maintaining approximately the same accuracy; indeed, the second-order accuracy of SE22 is observed with time step sizes as large as approximately 2000s, corresponding to CFL numbers as large as 15. (the small peak in the SE22 curve will be discussed in Sections 7.3 and 7.4), whereas the Eulerian scheme is unstable for ∆ t >120s. It indicates a better ability of the proposed method to stably propagate fast gravity waves. We also observe that, in this test case, SE22 is much more accurate than SL-SI-SETTLS and the first-order semi-Lagrangian exponential schemes. 7.3. Simulation-based stability analysis As discussed in Section 6, the lack of commutation between the exponential and interpolation operators does not allow us to compare the stability properties of the semi-Lagrangian exponential28 TITLE WILL BE SET BY THE PUBLISHER schemes through a classical linear stability analysis. Therefore, we conduct an empirical simulation- based stability study, as proposed by [27], in which simulations of the governing equations (here, the SWE on the rotating sphere), considering a steady-state solution, are performed in order to estimate the largest eigenvalue of the Jacobian of the approximate time evolution operator, which allows us to estimate the largest growth rate of perturbations to the steady state. This procedure, based on the power method for the estimation of the largest eigenvalue of a matrix, is briefly recalled below. We refer the reader to Appendix B of [27] for details and parameters to be used. Let Un+1 = G(Un) (7.2) represent the evolution of a given state vector U between times tn and tn+1 using a given time discretization method. Let U be a steady state, such that Rn := Un − U is the perturbation at time n. Through linearization of (7.2), the perturbation satisfies, to first order Rn+1 = G′(U)Rn (7.3) As in the power method, the procedure is to iterate over (7.3). If this method converges, then Rn converges to the eigenvector associated with the largest (in absolute value) eigenvalue λ of G′. This converged dominant eigenvalue can also be computed, as well as the growth rateν := log λ/∆t and the e-folding time τ, defined as the time in which R increases by a factor equal to e. We have e = ∥Rn∥\r\rR0\r\r = |λ|n = eνn∆t =⇒ τ = n∆t = 1 ν = ∆t log λ (7.4) The e-folding time is a relevant parameter, since, even if the scheme is unstable ( i.e., |λ| > 1), a large e-folding time may indicate that instabilities will remain reasonably small within simulation times used in practical applications, e.g., in numerical weather prediction, such that the time integration scheme can be successfully used in such contexts. We conduct this stability study by considering the geostrophic balance test case (Williamson’s test case 2), to which we introduce small initial perturbations to trigger instabilities. The spectral resolution is set to M = 512, and we compute λ and τ for various time step sizes and for the various time integration schemes considered here, with CFL numbers c∆t/∆x ranging from 0.12 to 62, approximately. Results are presented in Figure 7, which only includes simulations for which the iterative procedure described above converges. The smallest reported eigenvalues are approximately 1.001. We do not observe, in this test case, clear differences in the stability behavior depending on the discretization of the linear term in the semi-Lagrangian exponential schemes, with very similar results between SE11 and SE21, and also between SE12 and SE22, i.e., the stability seems to be mostly determined by the discretization of the nonlinear term. The SE12 and SE22 schemes clearly outperform the other methods, including SL-SI-SETTLS, for which the dominant eigenvalue rapidly increases when the time step sizes get larger. Also, unlikely the other schemes, SE12 and SE22 present e-folding times of more than one day for some time step sizes as large as approximately ∆t = 1000s (approximate CFL number of 8.1), indicating that simulations can be conducted for relatively large durations with instabilities remaining quite controlled. We recall that no artificial viscosity is considered in this study. Finally, we highlight the unnatural peak on the dominant eigenvalue (corresponding to the abrupt reduction of the e-folding time) of SE12 and SE22 for the tested time step sizes ∆ t = 240s and ∆ t = 480s. As described in Section 7.4, this behavior is also observed in the simulation of the unstable jet test case, but the reasons are unknown; it may be caused, e.g., by possible bad behaviors of the exponential functions when computed at time stepTITLE WILL BE SET BY THE PUBLISHER 29 sizes close to the mentioned values. As identified by [9] through linear stability analyses, exponential integration methods usually have quite erratic stability behaviors, including asymmetric stability regions and discontinuities of the amplification factor as a function of the time step size. 102 103 104 Timestep size (seconds) 1.0 1.2 1.4 1.6 1.8Dominant eigenvalue | | SL-SI-SETTLS SE11 SE12 SE21 SE22 102 103 104 Timestep size (seconds) 10 2 10 1 100 101 E-folding time  (days) Figure 7. Simulation-based stability analysis considering the geostrophic bal- anced test case: dominant eigenvalues (left) and corresponding e-folding times (right) as a function of the time step size and the time integration scheme. The same legend holds for both plots. The smallest reported eigenvalues are approxi- mately 1.001. 7.4. Qualitative comparison We consider the unstable jet test case to conduct a detailed qualitative comparison of the solutions provided by SE11, SE12, their proposed modifications and SL-SI-SETTLS. This test case is standard in atmospheric modeling research due to its complex dynamics after some days of simulation, and guidelines for evaluating numerical methods applied to it have been proposed by [34]. For this study, we consider a fixed spectral resolution M = 512 and various time step sizes in the same range [60, 960] (s) considered in the convergence analysis, with respective CFL numbers between 0.91 and 14, approximately. A reference solution is obtained from a simulation using a fourth-order explicit Runge-Kutta method, with M = 1024 and ∆t = 2s, corresponding to an approximate CFL number of 0.06. Results using the Eulerian exponential schemes are not considered due to their highly unstable behavior for all tested time step sizes. We first present, in Figures 8a and 8b, the kinetic energy spectra produced after six days of simulation by the four semi-Lagrangian exponential schemes, as well as SL-SI-SETTLS, with ∆ t = 120s and ∆ t = 960s, and compared to the reference solution. All spectra, including the reference one, present upturned tails, indicating the accumulation of energy on the highest wavenumbers due to the enstrophy cascade, which may lead to instabilities in longer executions if there is not enough dissipation in the numerical scheme [36]; thus, an appropriate application of artificial viscosity or hyperviscosity could be considered in order to mitigate unstable behaviors. In the case ∆ t = 120s, the SE11 scheme presents a clear unstable behavior, with almost the entire spectrum being strongly overamplified. This behavior is partially controlled with the pro- posed modified discretization of the linear term, and SE21 presents a smaller overamplification of intermediate wavenumbers; concerning SE12 and SE22, their spectra, which almost coincide30 TITLE WILL BE SET BY THE PUBLISHER visually, accurately reproduce the reference one in small and intermediate wavenumbers, but are damped out in the largest ones, indicating a stronger diffusion due to second-order discretization of the nonlinear term; finally, we observe an unstable behavior of SL-SI-SETTLS at the end of the wavenumber spectrum, being outperformed by the semi-Lagrangian exponential schemes using a second-order discretization of the nonlinear term. When the larger time step size ∆ t = 960s is considered, all time integration schemes present overamplifications of at least a portion of the wavenumber spectrum, notably SL-SI-SETTLS. The most stable simulation is provided by our proposed second-order scheme SE22, whose spectrum presents only a small peak at medium wavenumbers, being able to well reproduce the reference one at the large scales and remaining below it at the fine ones. Note that SE12, which provides nearly identical results to SE22 when ∆ t = 120s, has now a much more pronounced unstable behavior, with overamplification of almost the entire spectrum. 105106107 Wavelength (m) 10 7 10 5 10 3 10 1 101 Amplitude n 3 ref SL-SI-SETTLS SE11 SE12 SE21 SE22 (a) ∆t = 120s, 6 days 105106107 Wavelength (m) 10 6 10 4 10 2 100 102 Amplitude n 3 ref SL-SI-SETTLS SE11 SE12 SE21 SE22 (b) ∆t = 960s, 6 days 105106107 Wavelength (m) 10 24 10 20 10 16 10 12 10 8 10 4 100 Amplitude n 3 ref SL-SI-SETTLS SE11 SE12 SE21 SE22 (c) ∆t = 240s, 1 day Figure 8. Kinetic energy spectra, truncated at resolution M = 512, for the un- stable jet test case.TITLE WILL BE SET BY THE PUBLISHER 31 We remark that, among the tested time step sizes, the four semi-Lagrangian exponential schemes present very early unexpected unstable behaviors when ∆t is chosen around 240s and 480s, which has been identified in the stability analysis presented above (Figure 7). Figure 8c illustrates the kinetic energy spectrum after a single day of simulation ( i.e., long before the vortices start to appear), in which we observe unnatural peaks in the spectrum of the solutions of the semi-Lagrangian exponential methods. This behavior is not observed, at least not this early, for reasonably larger time step, e.g., ∆t = 960s as seen above. The reasons for this are not yet understood. A possible explanation is that one or more of the exponential functions φk, ψk may be not too well behaved when computed at ∆tL, with L defined by the parameters of the SWE on the rotating sphere and ∆t around the values mentioned above. Therefore, the use of artificial viscosity or hyperviscosity would be required to ensure further stability. This is studied in Section 7.6. We now compare the obtained results using the guidelines provided by [34] for the study of the unstable jet test case. First, we present, in Figure 9, the contour lines of the potential vorticity field of each simulation after six days on the region [240 ◦, 300◦] × [15◦, 75◦], compared to the reference solution provided by [34], which is obtained considering a spectral resolution M = 2730. The potential vorticity is defined by ξp := ξa/h, where ξa := 2Ω sinθ+ξ = f +ξ is the absolute vorticity. For the sake of conciseness, among the semi-Lagrangian exponential methods, only results produced by SE12 and SE22 are presented. With ∆ t = 120s, small-scale oscillating errors are observed in the solution produced by SL-SI-SETTLS, which is also slightly displaced w.r.t. the reference one; both SE12 and SE22 produce solutions close to the reference contour, with a better representation of some small-scale details by the latter scheme. With ∆ t = 960s, small-scale and displacement errors are still present in the SL-SI-SETTLS solution; concerning SE22, it still represents well the general profile and position of the reference solution, indicating smaller dispersion errors compared to the other schemes, but clear oscillations reveal the formation of unstable behaviors; finally, the solution of SE12 is strongly deteriorated both in shape and position. [34] also propose a set of metrics for a quantitative evaluation of the convergence of numerical solutions of the unstable jet test case. Among them, the maximum absolute value of the vorticity (∥ξ∥∞) and the relative error of the maximum potential vorticity (( ∥ξp∥∞ − ∥(ξp)0∥∞)/ ∥(ξp)0∥∞, where (ξp)0 is the initial potential vorticity), are identified to be proper convergence diagnostics. These quantities at every day of simulation are plotted in Figure 10 for the solutions computed with SL-SI-SETTLS, SE12 and SE22, being compared to the reference solution, as well as to reference values provided by [34] at day 5. With the small time step ∆ t = 120s, the semi-Lagrangian exponential schemes outperform SL-SI-SETTLS, and they even provide better approximations to the reference value of the potential vorticity deviation compared to our reference solution, which, as discussed above, presents a slightly unstable behavior at the very end of the wavenumber spectrum. Under the larger time step ∆ t = 960s, the instabilities of SE12 are clear, with a fast divergence from the reference values from the first day of simulation; SL-SI-SETTLS and SE22 have a much later divergence, mainly the latter, which stays relatively close to our reference solution until day 6 or 7. 7.5. Evaluation of computational cost We compare, in Table 2, the wall-clock times for the integration of the simulations whose kinetic energy spectra are presented in Figures 8a and 8b. Each simulation is executed with threaded spatial parallelization in 16 physical cores of Intel Xeon Gold 6130 2.10GHz in the GRICAD cluster from the University of Grenoble Alpes. As expected from the complexity analysis depicted in Table 1, the proposed modification of the discretization of the linear term strongly increases the computing time32 TITLE WILL BE SET BY THE PUBLISHER 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (a) SL-SI-SETTLS, ∆t = 120s 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (b) SE12, ∆t = 120s 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (c) SE22, ∆t = 120s 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (d) SL-SI-SETTLS, ∆t = 960s 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (e) SE12, ∆t = 960s 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (f) SE22, ∆t = 960s Figure 9. Contour lines of the potential vorticity field after six days of simulation of the unstable jet test case, using selected time integration methods (black lines), compared to the reference solution provided by [34] (red lines). Contour levels are separated by 0.2Ω/h. of the semi-Lagrangian exponential schemes, by factors ranging approximately between 40% and 65%. However, the proposed SE22 scheme, which provided the most stable and accurate results, presents a comparable cost with the well-established SL-SI-SETTLS. Moreover, the evaluation of the exponential functions is not fully optimized in our implementation, such that further wall clock time improvements could be obtained. SL-SI-SETTLS SE11 SE21 SE12 SE22 ∆t = 120s 1378 757 1051 1244 1538 ∆t = 960s 178 97 132 149 190 Table 2. Approximate wall-clock times (in seconds) for the integration of the unstable jet test case (10 days of simulation) by selected schemes. 7.6. Application of artificial viscosity and hyperviscosity No artificial viscosity or hyperviscosity was applied in the numerical simulations presented above. Although relatively stable simulations were indeed obtained, mainly with the proposed SE22 scheme, we identified some indications that viscosity approaches should be used for further stability, e.g., the lack of convergence in the geostrophic balance test case (Figure 5a), the upturned tails atTITLE WILL BE SET BY THE PUBLISHER 33 0.0 2.5 5.0 7.5 10.0 t (days) 10 4 2 × 10 4 3 × 10 4 4 × 10 4 || ||  (1/s) 0.0 2.5 5.0 7.5 10.0 t (days) 0.0 0.2 0.4 0.6 0.8 || p|| ||( p)0|| ||( p)0|| SE12 SE22 SL-SI-SETTLS t = 120 (s) t = 960 (s) ref SHP Figure 10. Evolution of diagnostic quantities in the unstable jet test case, mea- sured at every day of simulation. “ref” indicates the reference solution (4th order explicit Runge-Kutta, with M = 1024 and ∆ t = 2s) and “SHP” indicates the ref- erence values provided by Scott, Harris and Polvani [34] at day 5. Left: maximum absolute value of the vorticity field; right: deviation of the maximum absolute value of the potential vorticity field. the end of the wavenumber spectra (Figure 8) and the localized peaks in the spectra of semi- Lagrangian exponential methods when specific time step sizes are used (Figures 7 and 8c). The goal of this section is to verify if artificial viscosity and hyperviscosity approaches are able to mitigate these issues and to identify a compromise between stability and accuracy when choosing the viscosity parameters, namely the viscosity order q and the viscosity coefficient ν. In practice, artificial viscosity and/or numerical diffusion inherent to the discretization schemes are required in operational global atmospheric circulation models [41]. Ideally, one would use higher-order viscosity approaches ( q >2), in order to damp only the largest wavenumbers (the ones in which instabilities may be triggered first due to the enstrophy cascade induced by the nonlinearity of the problem) and preserve the largest ones, thus maintaining relatively high accuracy. Fourth-, sixth- and eight-order hyperviscosity approaches are usually considered in models using spectral discretization [18]; for instance, the spectral models IFS-ECMWF [11] and GFS-NCEP/NOAA [25] use respectively fourth- and eight-order viscosities. The determination of viscosity coefficients depends on the spectral resolution and the damping timescale, i.e., the time necessary for damping a given wavenumber by a given fraction, which varies between units and tenths of hours. We refer the reader to Appendix A of [5] for details and to the references therein for reports of viscosity parameters usually applied in spectral methods for atmospheric circulation models. In all simulations presented here, the viscosity term Lν is solved at the end of each time step using a first-order backward Euler scheme. We consider the unstable jet test case with ∆ t = 240s, for which clear instabilities were observed in the integration of the semi-Lagrangian exponential schemes, with ( q, ν) = (4, 1015) and ( q, ν) = (6, 1024). Smaller viscosity coefficients were not able to fully prevent the formation of peaks in the kinetic energy spectra. As presented in Figure 11, both viscosity configurations ensure stability at the cost of important damping of the highest34 TITLE WILL BE SET BY THE PUBLISHER wavenumber modes (and stronger damping of intermediate modes with the fourth-order viscosity), with very close results for all tested time integration methods. As illustrated in Figure 12 for the SE22 scheme (qualitatively similar results are obtained using SE12 or SL-SI-SETTLS), the sixth-order viscosity produces a better approximation to the reference solution provided by [34]. 105106107 Wavelength (m) 10 17 10 13 10 9 10 5 10 1 103 Amplitude n 3 ref SL-SI-SETTLS SE11 SE12 SE21 SE22 (a) (q, ν) = (4, 1015) 105106107 Wavelength (m) 10 18 10 14 10 10 10 6 10 2 102 Amplitude n 3 ref SL-SI-SETTLS SE11 SE12 SE21 SE22 (b) (q, ν) = (6, 1024) Figure 11. Kinetic energy spectra after six days, with ∆ t = 240s (and ∆ t = 2s for the reference solution), truncated at resolution M = 512, for the unstable jet case with artificial viscosity. 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (a) (q, ν) = (4, 1015) 250 260 270 280 290 Longitude (degrees) 20 30 40 50 60 70Latitude (degrees) (b) (q, ν) = (6, 1024) Figure 12. Contour lines of the potential vorticity field after six days of the unstable jet test case with artificial viscosity, using the SE22 scheme and ∆ t = 240s (black lines), compared to the reference solution provided by [34] (red lines). Contour levels are separated by 0 .2Ω/h. By comparing these results with the simulations without viscosity (Figure 9), we observe that SE22 with ∆t = 120s, ν = 0 is qualitatively superior to the simulation with ∆ t = 240s and (q, ν) = (6, 1024), but the latter outperforms the simulation with ∆ t = 960s, ν = 0, which presents several small-scale oscillations despite a relatively stable simulation compared to other time integrationTITLE WILL BE SET BY THE PUBLISHER 35 schemes. On the other hand, SL-SI-SETTLS and SE12 clearly benefit from the use of artificial viscosity, with considerably more stable and accurate results compared to the inviscid case with both ∆t = 120s and ∆t = 960s (in the case of SL-SI-SETTLS) or ∆ t = 960s (in the case of SE12). 8. Conclusion Semi-Lagrangian exponential methods, proposed by [26], have a potential for application in at- mospheric circulation models, since they have enhanced stability compared to Eulerian exponential schemes and, contrary to traditional semi-Lagrangian methods, are able to integrate the linear terms very accurately; therefore, this class of time stepping scheme is more suitable for the integra- tion of advection-dominated models containing stiff linear terms. The work presented here aimed to extend [26] in several aspects: first, we identified and explained the accuracy limitations of their methods, showing that the combination of exponential and interpolation operators adopted in the discretization of the linear term, which corresponds to a first-order discretization of the integration factor, produces a first-order leading term in the truncation error. Second, inspired by this initial study, we proposed a new method, with an alternative discretization of the linear term, leading to an effective second-order accuracy. Also, we conducted a detailed stability study comparing several Eulerian and semi-Lagrangian methods, both through a linear stability analysis and an empirical simulation-based stability study, which was required because the standard stability analysis is not able to provide detailed insights in the stability of semi-Lagrangian exponential schemes. Finally, while [26] developed their work in the context of the SWE on the biperiodic plane, we applied the schemes to the integration of the SWE on the rotating sphere, thus providing results that are more closely related to real-world operational atmospheric circulation models. Promising results were obtained using the proposed second-order semi-Lagrangian exponential scheme. Its theoretical convergence order was confirmed in several benchmark test cases, and, considering the especially challenging unstable jet simulation, it proved to be more stable and accurate than Eulerian exponential schemes, other semi-Lagrangian exponential schemes and the well-established second-order SL-SI-SETTLS, even without the use of artificial viscosity approaches, which is a consequence of the combined accurate integration of the stiff linear terms and the semi- Lagrangian treatment of the advection. The latter approach largely extends the range of stable time step sizes compared to a second-order Eulerian exponential method, maintaining approximately the same accuracy, while the former leads to larger accuracy, compared to SL-SI-SETTLS, in simulations dominated by linear processes; however, our proposed scheme is outperformed by SL- SI-SETTLS (if stable) when nonlinear effects are more important. In terms of computational cost, a drawback of the proposed method is its higher complexity compared to the previously formulated first-order semi-Lagrangian exponential schemes, as a consequence of the splitting of the exponential operator applied to the linear term, which is required to achieve second-order accuracy. We highlight, however, that this larger cost can be mitigated by a straightforward computation of matrix exponentials, by combining a spectral discretization with a convenient arrangement of the terms of the governing equations, and by precomputing these exponential terms, which is possible in simulations with a constant time step size. Also, the observed wall-clock times are comparable to those of SL-SI-SETTLS, which is used in operational atmospheric circulation models; therefore, our proposed method seems to be a competitive candidate for operational applications. Important aspects still need to be considered in future work for further improvement of semi- Lagrangian exponential methods. As illustrated in the numerical simulations, these schemes pre- sented unexpected unstable behaviors in a specific range of time step sizes, which can possibly be36 TITLE WILL BE SET BY THE PUBLISHER caused by badly behaved exponential functions; even if these issues have been easily avoided by using high-order artificial viscosity, with little damage to the accuracy of the numerical solution, a more detailed understanding needs to be achieved. Also, higher-order semi-Lagrangian exponential schemes can be formulated to achieve improved accuracy in simulations dominated by nonlineari- ties; from the truncation error analyses conducted here, they could probably be obtained in a quite straightforward way following the derivation of higher-order Eulerian exponential methods by [8] and using higher-order approximations to the integral involved in the integration factor term; how- ever, the hypothesis on the commutation of the linear operator along the Lagrangian trajectories, which was proved here to introduce a second-order error, should be carefully studied. Moreover, the discussion on the derivation of linear stability analyses of semi-Lagrangian exponential schemes com- bined with spectral methods, conducted in Section 6.4, suggests that more intricate studies should be necessary to fully understand the relation between spectral transforms, matrix exponentiation, and spatial interpolation, and the influence on accuracy properties of semi-Lagrangian exponential methods, which would certainly contribute to further developments of this class of methods. Finally, applications to more complex atmospheric models would be a natural and challenging extension of this work. Acknowledgements This work was supported by the S˜ ao Paulo Research Foundation (FAPESP) grants 2021/03777-2 and 2021/06176-0, as well as the Brazilian National Council for Scientific and Technological Devel- opment (CNPq), Grant 303436/2022-0. This project also received funding from the Federal Ministry of Education and Research and the European High-Performance Computing Joint Undertaking (JU) under grant agreement No 955701, Time-X. The JU receives support from the European Union’s Horizon 2020 research and innovation programme and Belgium, France, Germany, Switzerland. Most of the computations presented in this paper were performed using the GRICAD infras- tructure (https://gricad.univ-grenoble-alpes.fr), which is supported by Grenoble research communities. Data availability statement The research code, benchmark tests and postprocessing scripts used in this article are available in the repository https://gitlab.inria.fr/sweet/sweet/-/tree/paper_2nd_order_SLETDRK/ benchmarks_sphere/paper_2nd_order_sletdrk [4]. References [1] L. Bonaventura. Exponential integrators for multiple time scale problems of environmental fluid dynamics. Innovative Integrators Workshop 27-30 October 2010. Universit¨ at Innsbruck, Austria. [2] M. Brachet, L. Debreu, and C. Eldred. Comparison of exponential integrators and traditional time integration schemes for the shallow water equations. Applied Numerical Mathematics, 180:55–84, Oct. 2022. [3] T. Buvoli and M. L. Minion. On the stability of exponential integrators for non-diffusive equations. Journal of Computational and Applied Mathematics , 409:114126, Aug. 2022. [4] J. G. Caldas Steinstraesser, P. d. S. Peixoto, and M. Schreiber. Code, benchmark tests and postprocessing scripts for “Analysis and improvement of a semi-Lagrangian exponential scheme for the shallow-water equations on the rotating sphere”, 2024. https://gitlab.inria.fr/sweet/sweet/- /tree/paper 2nd order SLETDRK/benchmarks sphere/paper 2nd order sletdrk. [5] J. G. Caldas Steinstraesser, P. d. S. Peixoto, and M. Schreiber. Parallel-in-time integration of the shallow water equations on the rotating sphere using parareal and mgrit, Jan. 2024.TITLE WILL BE SET BY THE PUBLISHER 37 [6] J. Certaine. The solution of ordinary differential equations with large time constants. Mathematical methods for digital computers, pages 128–132, 1960. [7] C. Clancy and J. A. Pudykiewicz. On the use of exponential time integration methods in atmospheric models. Tellus A: Dynamic Meteorology and Oceanography, 65(1):20898, Dec. 2013. [8] S. Cox and P. Matthews. Exponential time differencing for stiff systems. Journal of Computational Physics , 176(2):430–455, 2002. [9] N. Crouseilles, L. Einkemmer, and J. Massot. Exponential methods for solving hyperbolic problems with appli- cation to collisionless kinetic equations. Journal of Computational Physics , 420:109688, Nov. 2020. [10] D. Durran. Numerical methods for fluid dynamics : with applications to geophysics . Springer, New York, 2010. [11] ECMWF and P. White. IFS Documentation CY25R1 - Part III: Dynamics and Numerical Procedures. 2003. [12] J. Galewsky, R. K. Scott, and L. M. Polvani. An initial-value problem for testing numerical models of the global shallow-water equations. Tellus A: Dynamic Meteorology and Oceanography, 56(5):429–440, Jan. 2004. [13] S. Gaudreault and J. A. Pudykiewicz. An efficient exponential time integration method for the numerical solution of the shallow water equations on the sphere. Journal of Computational Physics , 322:827–848, Oct. 2016. [14] T. S. Haut, T. Babb, P. G. Martinsson, and B. A. Wingate. A high-order time-parallel scheme for solving wave propagation problems via the direct construction of an approximate time-evolution operator. IMA Journal of Numerical Analysis, 36(2):688–716, June 2015. [15] M. Hochbruck and A. Ostermann. Exponential integrators of rosenbrock-type. Oberwolfach Reports, 3:1107– 1110, 2006. [16] M. Hochbruck and A. Ostermann. Exponential integrators. Acta Numerica, 19:209–286, May 2010. [17] M. Hortal. The development and testing of a new two-time-level semi-lagrangian scheme (settls) in the ecmwf forecast model. Quarterly Journal of the Royal Meteorological Society , 128(583):1671–1687, July 2002. [18] C. Jablonowski and D. L. Williamson. The pros and cons of diffusion, filters and fixers in atmospheric general circulation models. In P. Lauritzen, C. Jablonowski, M. Taylor, and R. Nair, editors, Numerical Techniques for Global Atmospheric Models, chapter 13, pages 381–493. Springer Berlin Heidelberg, 2011. [19] J. D. Lawson. Generalized runge-kutta processes for stable systems with large lipschitz constants. SIAM Journal on Numerical Analysis, 4(3):372–380, Sept. 1967. [20] M. S. Longuet-Higgins. The eigenfunctions of laplace’s tidal equations over a sphere. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences , 262(1132):511–607, Feb. 1968. [21] V. T. Luan, J. A. Pudykiewicz, and D. R. Reynolds. Further development of efficient and accurate time inte- gration schemes for meteorological models. Journal of Computational Physics , 376:817–837, Jan. 2019. [22] P. Lynch. A lagrange–laplace integration scheme for weather prediction and climate modelling. Meteorology, 1(4):355–376, Sept. 2022. [23] G. Mengaldo, A. Wyszogrodzki, M. Diamantakis, S.-J. Lock, F. X. Giraldo, and N. P. Wedi. Current and emerg- ing time-integration strategies in global numerical weather and climate prediction. Archives of Computational Methods in Engineering, 26(3):663–684, Feb. 2018. [24] B. V. Minchev and W. M. Wright. A review of exponential integrators for first-order semi-linear problems. Technical report, Norwegian University of Science and Technology, 2005. [25] U. N. Oceanic and A. A. N. . N. C. for Environmental Prediction (NCEP). Global Forecast System - Global Spectral Model (GSM) - V13.0.2. https://vlab.noaa.gov/web/gfs/documentation, May 2016. [26] P. S. Peixoto and M. Schreiber. Semi-Lagrangian Exponential Integration with Application to the Rotating Shallow Water Equations. SIAM Journal on Scientific Computing , 41(5):B903–B928, 2019. [27] P. S. Peixoto, J. Thuburn, and M. J. Bell. Numerical instabilities of spherical shallow-water models considering small equivalent depths. Quarterly Journal of the Royal Meteorological Society , 144(710):156–171, Dec. 2017. [28] J. A. Pudykiewicz and C. Clancy. Convection experiments with the exponential time integration scheme.Journal of Computational Physics , 449:110803, Jan. 2022. [29] G. Rainwater, K. C. Viner, and P. A. Reinecke. Exponential time integration for 3d compressible atmospheric models, 2023. [30] Y. Saad. Variations on arnoldi’s method for computing eigenelements of large unsymmetric matrices. Linear Algebra and its Applications , 34:269–295, Dec. 1980. [31] N. Schaeffer. Efficient spherical harmonic transforms aimed at pseudospectral numerical simulations. Geochem- istry, Geophysics, Geosystems, 14(3):751–758, Mar. 2013. [32] M. Schreiber and R. Loft. A parallel time integrator for solving the linearized shallow water equations on the rotating sphere. Numerical Linear Algebra with Applications , 26(2), Oct. 2018.38 TITLE WILL BE SET BY THE PUBLISHER [33] M. Schreiber, N. Schaeffer, and R. Loft. Exponential integrators with parallel-in-time rational approximations for the shallow-water equations on the rotating sphere. Parallel Computing, 85:56–65, July 2019. [34] R. K. Scott, L. M. Harris, and L. M. Polvani. A test case for the inviscid shallow-water equations on the sphere. Quarterly Journal of the Royal Meteorological Society , 142(694):488–495, Nov. 2015. [35] V. V. Shashkin and G. S. Goyman. Semi-lagrangian exponential time-integration method for the shallow wa- ter equations on the cubed sphere grid. Russian Journal of Numerical Analysis and Mathematical Modelling , 35(6):355–366, Dec. 2020. [36] W. C. Skamarock. Evaluating mesoscale nwp models using kinetic energy spectra. Monthly Weather Review , 132(12):3019–3032, Dec. 2004. [37] A. Staniforth and J. Cˆ ot´ e. Semi-lagrangian integration schemes for atmospheric models—a review. Monthly Weather Review, 119(9):2206–2223, Sept. 1991. [38] J. Stoer and R. Bulirsch. Introduction to numerical analysis . Texts in Applied Mathematics. Springer, New York, NY, 3 edition, Aug. 2002. [39] P. N. Swarztrauber. Spectral transform methods for solving the shallow-water equations on the sphere. Monthly Weather Review, 124(4):730–744, Apr. 1996. [40] C. Temperton. Treatment of the coriolis terms in semi-lagrangian spectral models. Atmosphere-Ocean, 35(sup1):293–302, Jan. 1997. [41] J. Thuburn. Numerical wave propagation on the hexagonal c-grid. Journal of Computational Physics , 227(11):5836–5858, May 2008. [42] S. Vasylkevych and N. ˇZagar. A high-accuracy global prognostic model for the simulation of rossby and gravity wave dynamics. Quarterly Journal of the Royal Meteorological Society , 147(736):1989–2007, Mar. 2021. [43] D. L. WILLIAMSON. The evolution of dynamical cores for global atmospheric models. Journal of the Meteo- rological Society of Japan. Ser. II , 85B:241–269, 2007. [44] D. L. Williamson, J. B. Drake, J. J. Hack, R. Jakob, and P. N. Swarztrauber. A standard test set for numer- ical approximations to the shallow water equations in spherical geometry. Journal of Computational Physics , 102(1):211–224, Sept. 1992.",
      "references": [
        "Exponential integrators for multiple time scale problems of environmental fluid dynamics",
        "Comparison of exponential integrators and traditional time integration schemes for the shallow water equations",
        "On the stability of exponential integrators for non-diffusive equations",
        "Code, benchmark tests and postprocessing scripts for “Analysis and improvement of a semi-Lagrangian exponential scheme for the shallow-water equations on the rotating sphere”",
        "Parallel-in-time integration of the shallow water equations on the rotating sphere using parareal and mgrit",
        "The solution of ordinary differential equations with large time constants",
        "On the use of exponential time integration methods in atmospheric models",
        "Exponential time differencing for stiff systems",
        "Exponential methods for solving hyperbolic problems with application to collisionless kinetic equations",
        "Numerical methods for fluid dynamics : with applications to geophysics",
        "IFS Documentation CY25R1 - Part III: Dynamics and Numerical Procedures",
        "An initial-value problem for testing numerical models of the global shallow-water equations",
        "An efficient exponential time integration method for the numerical solution of the shallow water equations on the sphere",
        "A high-order time-parallel scheme for solving wave propagation problems via the direct construction of an approximate time-evolution operator",
        "Exponential integrators of rosenbrock-type",
        "Exponential integrators",
        "The development and testing of a new two-time-level semi-lagrangian scheme (settls) in the ecmwf forecast model",
        "The pros and cons of diffusion, filters and fixers in atmospheric general circulation models",
        "Generalized runge-kutta processes for stable systems with large lipschitz constants",
        "The eigenfunctions of laplace’s tidal equations over a sphere",
        "Further development of efficient and accurate time integration schemes for meteorological models",
        "A lagrange–laplace integration scheme for weather prediction and climate modelling",
        "Current and emerging time-integration strategies in global numerical weather and climate prediction",
        "A review of exponential integrators for first-order semi-linear problems",
        "Global Forecast System - Global Spectral Model (GSM) - V13.0.2",
        "Semi-Lagrangian Exponential Integration with Application to the Rotating Shallow Water Equations",
        "Numerical instabilities of spherical shallow-water models considering small equivalent depths",
        "Convection experiments with the exponential time integration scheme",
        "Exponential time integration for 3d compressible atmospheric models",
        "Variations on arnoldi’s method for computing eigenelements of large unsymmetric matrices",
        "Efficient spherical harmonic transforms aimed at pseudospectral numerical simulations",
        "A parallel time integrator for solving the linearized shallow water equations on the rotating sphere",
        "Exponential integrators with parallel-in-time rational approximations for the shallow-water equations on the rotating sphere",
        "A test case for the inviscid shallow-water equations on the sphere",
        "Semi-lagrangian exponential time-integration method for the shallow water equations on the cubed sphere grid",
        "Evaluating mesoscale nwp models using kinetic energy spectra",
        "Semi-lagrangian integration schemes for atmospheric models—a review",
        "Introduction to numerical analysis",
        "Spectral transform methods for solving the shallow-water equations on the sphere",
        "Treatment of the coriolis terms in semi-lagrangian spectral models",
        "Numerical wave propagation on the hexagonal c-grid",
        "A high-accuracy global prognostic model for the simulation of rossby and gravity wave dynamics",
        "The evolution of dynamical cores for global atmospheric models",
        "A standard test set for numerical approximations to the shallow water equations in spherical geometry"
      ],
      "meta_data": {
        "arxiv_id": "2405.02237v4",
        "authors": [
          "João Guilherme Caldas Steinstraesser",
          "Martin Schreiber",
          "Pedro da Silva Peixoto"
        ],
        "published_date": "2024-05-03T16:48:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "- Problem: Semi-Lagrangian Exponential Time-Differencing Runge–Kutta (SL-ETDRK) schemes previously proposed for advective PDEs (notably SWE) were expected to be second-order (SL-ETD2RK) but observed to be only first-order in practice.\n- Key finding (analysis): The accuracy loss is caused by a first-order approximation of the integrating factor acting on the linear term when combined with semi-Lagrangian interpolation; the exponential and interpolation operators do not commute, so the linear term is not integrated “exactly” as in Eulerian ETD.\n- Contribution 1: Truncation-error proof that the original SL-ETD2RK is globally first-order due to the linear-term discretization; the paper renames original schemes as SE11 (1st-order linear, 1st-order nonlinear) and SE12 (1st-order linear, 2nd-order nonlinear).\n- Contribution 2: New semi-Lagrangian exponential discretization that splits the exponential for the linear term across the interpolation step, yielding a genuinely second-order method (SE22) for SWE on the rotating sphere.\n- Contribution 3: Stability analysis comparing Eulerian ETDRK, semi-Lagrangian exponential variants, and operational semi-Lagrangian semi-implicit SETTLS. Shows classical linear stability analysis is insufficient for SL-exponential schemes (due to non-commutation), motivating an empirical simulation-based stability assessment.\n- Contribution 4: Extensive numerical evaluation on rotating-sphere SWE benchmarks (convergence, stability, spectra, cost). SE22 is more stable/accurate than earlier SL-exponential schemes and competitive with SL-SI-SETTLS, though with higher wall-clock time than first-order SL-exponential variants.",
        "methodology": "- Governing model: Shallow-water equations (SWE) on the rotating sphere, written in vorticity–divergence–geopotential form U=(Φ, ξ, δ)^T, with RHS split into linear gravity term LG, Coriolis term LC, nonlinear advection NA, remaining nonlinear terms NR, optional hyperviscosity Lν, and topography forcing b.\n- Spatial discretization: Global spectral method using spherical harmonics with triangular truncation at total wavenumber M. Linear operator chosen as L := LG (gravity-wave part) to remain diagonalizable in spectral space; Coriolis is treated as “nonlinear” (included in N) to preserve efficient exponential evaluation.\n- Time integration families:\n  - Eulerian ETDRK: ETD1RK and ETD2RK (Cox–Matthews) using φ-functions φ0, φ1, φ2 applied to ΔtL.\n  - Semi-Lagrangian semi-implicit baseline: SL-SI-SETTLS (operational two-time-level scheme) using SETTLS trajectory estimation and Crank–Nicolson for L.\n  - Original semi-Lagrangian exponential: SL-ETDRK from Peixoto & Schreiber (2019), here renamed SE11/SE12, combining SETTLS trajectories + interpolation to departure points with φ/ψ functions.\n- Core analytical technique:\n  - Local truncation-error analysis on a linear advective PDE in Lagrangian form Du/Dt = L(x(t))u, showing that using φ0(ΔtL) evaluated at the arrival point corresponds to a first-order (rectangle rule) approximation of ∫ L(x(s)) ds along trajectories.\n  - Proposed fix: Use a second-order quadrature idea (trapezoidal rule) for the integrating factor by splitting the exponential across departure and arrival: apply exp(ΔtL/2) before interpolation (departure-side) and exp(ΔtL/2) after interpolation (arrival-side).\n  - New schemes: SE21 (2nd-order linear, 1st-order nonlinear) and SE22 (2nd-order linear, 2nd-order nonlinear). SE22 combines exponential splitting for the linear term with the second-order nonlinear correction already present in SE12.\n- Stability methodology:\n  - Standard linear stability functions for Eulerian ETDRK and for SL-SI-SETTLS/SE11/SE12 under simplifying assumptions.\n  - For SE21/SE22, argue analytic stability functions can miss effects because exponential and interpolation do not commute.\n  - Use a simulation-based stability study (power-method style) on a steady geostrophic balance state to estimate dominant eigenvalue/growth rates of the discrete evolution operator.\n- Practical implementation aspects: φ/ψ functions computed via diagonalization of the per-wavenumber symbol matrix for LG; functions can be precomputed for fixed Δt; SL trajectories computed with SETTLS; interpolation uses cubic Lagrange.",
        "experimental_setup": "- PDE/application: SWE on the rotating sphere.\n- Spatial resolution: Spherical harmonics triangular truncations M ranging from 32 to 512 (and up to 1024 for a high-fidelity reference in one test). Physical-space interpolation on associated grids; cubic Lagrange interpolation for semi-Lagrangian departure-point evaluation.\n- Linear/nonlinear split used for exponentials: L := LG; nonlinear term includes LC + NR + b; advection handled by the semi-Lagrangian material derivative.\n- Benchmarks/test cases:\n  1) Williamson et al. (1992) test case 2 (geostrophically balanced steady state), run ~7 days for convergence tests.\n  2) Geostrophic balance with bathymetry (Peixoto–Thuburn–Bell 2017 variant): constant depth h=h0 and topography chosen for balance; two regimes h0=100 m and h0=1 m to vary nonlinearity strength; run ~7 days.\n  3) Galewsky et al. (2004) unstable jet: error after 1 day for formal convergence; qualitative analyses up to ~6–10 days to assess vortex formation, spectra, and stability.\n- Error/validation methodology:\n  - Temporal convergence: vary Δt in {60,120,240,480,960} s while scaling M inversely to keep CFL roughly constant; compute L2 (and mention similar L∞) errors of geopotential perturbation Φ′ versus a reference solution produced by classical 4th-order explicit Runge–Kutta with 4× smaller Δt at the same M.\n  - Additional robustness test: unstable jet at fixed M=256 with large Δt sweep (up to 8192 s) to probe stability/accuracy limits.\n  - Stability assessment: (i) classical stability regions plotted in (Im(ξN), Im(ξL)) slices for simplified linear models; (ii) empirical power-method procedure to estimate dominant eigenvalues/e-folding times on the balanced state for M=512 and a wide Δt range.\n  - Qualitative diagnostics: kinetic energy spectra after 6 days (energy accumulation at high wavenumbers), potential vorticity contour comparisons against high-resolution reference from Scott–Harris–Polvani (2015), and time series of vorticity/potential-vorticity metrics.\n- Baselines compared: Eulerian ETD1RK/ETD2RK, SL-SI-SETTLS, original SE11/SE12, and new SE21/SE22.\n- Hyperviscosity experiments: add Lν with orders q=4 and q=6 using chosen coefficients (e.g., ν=10^15 for q=4; ν=10^24 for q=6) to damp high-wavenumber instabilities; viscosity step solved with backward Euler at end of each time step.\n- Computational cost: wall-clock timings on 16 Intel Xeon Gold 6130 cores (GRICAD cluster) for unstable jet runs (10 days) at M=512 for Δt=120 s and 960 s; compare runtimes across schemes.",
        "limitations": "- Order limitation source is tied to non-commutation between exponential and interpolation operators; analysis is clean in scalar/commuting cases, but a fully general rigorous second-order proof for non-commuting matrix operators along trajectories is not fully established (the paper provides arguments/expansions and relies on numerical evidence).\n- Stability analysis: classical linear stability tools are not fully applicable to semi-Lagrangian exponential schemes; stability must be assessed empirically, limiting theoretical guarantees and making parameter-dependent behaviors harder to predict.\n- Observed “erratic”/unexpected instabilities at specific time-step ranges (around Δt≈240–480 s) for SL-exponential schemes in nonlinear SWE tests; root cause not fully identified (possibly φ/ψ evaluation or operator-interpolation interactions).\n- Computational overhead: SE22 requires additional exponential evaluations and an extra interpolation versus original schemes, increasing wall-clock time; performance relies on precomputation and efficient implementation.\n- Linear/nonlinear splitting choice (treating Coriolis as nonlinear to keep L diagonalizable) may reduce the advantage of exponential integration in regimes where Coriolis stiffness is important; results may depend on this modeling decision.\n- Hyperviscosity tuning: stability improvements with q,ν come at accuracy/diffusion costs; parameter choices are empirical and resolution-dependent.\n- Experiments focus on SWE (2D) with spectral discretization; transferability to 3D primitive equations, other grids (finite-volume/element), or non-spectral discretizations is not demonstrated here.",
        "future_research_directions": "- Develop higher-order semi-Lagrangian exponential schemes (beyond second order) by combining higher-order ETDRK constructions with higher-order quadrature/approximations for the trajectory-dependent integrating factor.\n- Provide a rigorous convergence theory for non-commuting linear operators along trajectories (matrix case), possibly using Magnus expansions or commutator bounds, to formally justify second-order (and higher) accuracy.\n- Explain and mitigate the time-step “instability windows” observed (Δt≈240–480 s): investigate φ/ψ numerical evaluation, spectral–interpolation interactions, resonance/aliasing mechanisms, and improved computation of φ-functions.\n- Improve stability/accuracy control via adaptive hyperviscosity, scale-selective filtering, or physically consistent dissipation; study optimal damping strategies for SL-exponential schemes.\n- Explore alternative L/N splittings (including partial Coriolis in L, or Hough-harmonics-based diagonalization) and assess impact on stiffness handling and cost.\n- Extend to more complex atmospheric models (3D compressible/primitive equations), including coupling with physical parameterizations and variable time stepping.\n- Derive better analytical stability frameworks for SL-exponential methods that explicitly account for non-commuting interpolation and exponential operators, especially in spectral-transform contexts.\n- Optimize implementation: vectorization, precomputation strategies, alternative exponential evaluation (Krylov/rational/parallel-in-time), and scalability studies for operational use.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "StablePrompt : Automatic Prompt Tuning using Reinforcement Learning for Large Language Model",
      "full_text": "HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging Preprint, compiled February 21, 2024 Dennis Laurijssen ID 1,2∗ , Walter DaemsID 1,2, and Jan SteckelID 1,2 1Cosys-Lab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium 2Flanders Make Strategic Research Centre, Lommel, Belgium Abstract Airborne 3D imaging using ultrasound is a promising sensing modality for robotic applications in harsh environments. Over the last decade, several high-performance systems have been proposed in the literature. Most of these sensors use a reduced aperture microphone array, leading to artifacts in the resulting acoustic images. This paper presents a novel in-air ultrasound sensor that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array, in combination with a distributed embedded hardware design to perform the data acquisition. Using a broadband Minimum Variance Distortionless Response (MVDR) beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able to create both 2D and 3D ultrasound images of the full-frontal hemisphere with high angular accuracy with up to 70dB main lobe to side lobe ratio. This paper describes both the hardware infrastructure needed to obtain such highly detailed acoustical images, as well as the signal processing chain needed to convert the raw acoustic data into said images. Utilizing this novel high-resolution ultrasound imaging sensor, we wish to investigate the limits of both passive and active airborne ultrasound sensing by utilizing this virtually artifact-free imaging modality. 1 I ntroduction While microphone arrays have been around for more than 50 years [1], the landscape of microphone array sensors and its technology have advanced tremendously in the last two decades with the rise of MEMS (micro-electro-mechanical system) tech- nology. Furthermore, the last decade has given rise to many novel 3D in-air ultrasound sensors which allow the formation of acoustic images in 3D. These sensors hold great promise for robotic applications in harsh environments, as ultrasound signals are minimally affected by medium distortions such as dust, fog and water spray. However, the sensors developed in the past typically use a reduced aperture due to cost and complexity limitations, with microphone counts typically ranging from 1 to 64. These reduced apertures inevitably cause either artifacts in the resulting 3D images, or images with a limited dynamic range and spatial resolution. Ultrasound signals often exhibit a large Helmholtz number in relationship to the environments where they are applied, imply- ing that the reflected energy impinging on the sensor is mainly specular in nature. On the other hand, diffraction echoes should arise from acoustic theory [2], but so far these echos have been mostly neglected due to their low intensity. In order to assess the relative importance of these echoes in real-world environ- ments, as well as to investigate what the virtual upper limit is of ultrasound sensing in real-world environments, a sensor with a high spatial resolution and high dynamic range is necessary. This paper tries to address the need for a sensor with high spatial resolution and dynamic range, and introduces a dense, large aperture in-air ultrasound microphone array which should pro- vide these high spatial resolutions, dynamic ranges and signal to noise ratios. The system is consists of 1024 synchronously sampled microphones, increasing the number of microphones of our previously developed systems by a factor of 32 [3, 4, 5]. This sharp rise in microphone channel count is achieved by leveraging a distributed hardware architecture, which is built upon a decade of ultrasound sensor development. In this pa- per, we demonstrate a successful implementation of this novel acoustic sensor, and demonstrate its functionality through both simulation and real-world measurements. In order for the readers to accurately follow the developments, we encourage them to get familiar with our previous work in which we describe in detail the development of a single 32- channel microphone array [ 3, 6], as the sensor in this paper consists of a distributed version of that single 32-channel module. However, this paper still stands on its own, allowing the reader to follow the development of the data-acquisition methods and signal processing techniques and understand the performance analysis of the system where we compare it with our previously developed 32-channel microphone array [3, 6]. In the pages that follow, we will touch on the design choices that were made to achieve the hardware architecture of the devel- oped ultrasound sensor unit that we named the High Resolution Imaging Sonar (HiRIS) sensor, together with a more detailed description of the implementation. In the subsequent section, the data acquisition and signal processing are described followed by a section on the experimental setup and its results. In the final section, we will present the conclusions of the proposed system and its envisioned applications as future work. 2 H ardware Architecture Achieving the envisioned objective of constructing a synchro- nized ultrasound sensor array featuring 1024 microphone chan- nels, coupled with a versatile yet timely data transfer interface, poses a nontrivial challenge. It requires considering several trade-offs in different design aspects, such as the choice of com- ponents and their associated costs, design time influenced by arXiv:2402.13110v1  [eess.SP]  20 Feb 2024Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 2 familiarity with a platform, as well as the time allocated for implementation and testing. This section aims to delve into the deliberations behind these design choices, exploring considera- tions related to component types, cost implications, design famil- iarity impact on time, and the overall implementation process. Additionally, we will introduce the selected implementation and provide an overview of the proposed system. The hardware design of the HiRIS sensor is a highly complex one, which warrants its own extensive description as the devil is in the details. Indeed, the overall system consists of 1024 mi- crophones, 33 microcontrollers, 4963 electronical components, distributed over 2 PCBs, and over 127m of PCB traces. The road to a successful implementation of such a system is riddled with pitfalls, which we aim to clarify in the subsequent sections. 2.1 Design Choices Over the last decade, the embedded products market has witnessed a notable surge in diversity, propelled by swift technological advancements. The integration of highly capable and feature-rich (ARM) microcontrollers, along with Field- Programmable Gate Arrays (FPGAs) and System on Chips (SoCs), has been instrumental in enhancing the capabilities of embedded sensor systems. The emergence and widespread adoption of Single Board Computers (SBCs), coupled with the proliferation of Internet of Things (IoT) devices, have been spurred by the demand for technological progress in the era of Industry 4.0. Concurrently, the growing community of online hobbyists in the domain of embedded electronics hardware has contributed to the development of tools and libraries, facilitating the rapid creation of embedded platforms. While the three aforementioned types of embedded de- vices, being FPGAs [ 4, 3, 7, 8, 9], SoCs [ 10, 11, 12] and ARM microcontrollers [13, 14, 5, 15, 16], have been used for the construction of high-resolution ultrasound sensing arrays, each of these device types have their distinct advantages and disadvantages. Field-Programmable Gate Arrays (FPGAs) o ffer notable advantages in terms of flexibility and customization. Their field-configurable nature, in combination with very high GPIO pin counts, allows for their rapid adaptation to diverse tasks, especially in real-time applications with very tight timing constraints. However, the complexity of FPGA design, coupled with the relatively higher power consumption and component cost can be considered drawbacks. Indeed, developing complex hardware designs in FPGAs is complicated due to the need for tight timing closures, in order to yield stable data-acquisition systems. Systems-on-a-Chip (SoCs) integrate multiple components, pro- cessing cores and peripherals onto a single chip, streamlining the design and reducing the need for external components. This leads to space and power e fficiency. While the large amount of diverse peripherals on the SoC is attractive, the amount of customization options are significantly more limited when com- pared to FPGAs. Therefore, complexity in the envisioned design may lead to significant challenges during the development. On the other hand, timing closure is guaranteed by design, leading to far less potential for race conditions compared to FPGA-based designs. ARM microcontrollers excel in power e fficiency and simplic- ity, due to their standardized architecture and interface design. The low cost of ownership, coupled to wide industry adoption make them accessible for a wide range of applications, from automotive, over consumer goods, and indeed, to high-speed data-acquisition. This high degree of standardisation leads to a reduced customization potential when compared to FPGAs, or the integrated capabilities of SoCs, limiting their suitability for certain high-performance or specialized tasks. Despite the apparent drawbacks of ARM-based microcontroller systems, we deemed this to be the most promising candidate for the development of the hardware architecture for HiRIS. While the other two options (being SoCs and FPGAs) are certainly viable options for implementing such a systems, we chose for an ARM-based architecture, because of the fact that a) the pe- ripherals on the chosen ARM platform are ideally suited for our intended application, b) a distributed architecture is more error robust than a single monolithic implementation, and c) our group is well versed in the development of ARM-based systems, which is a non-neglectable reason for choosing a particular approach. 2.2 Distributed Architecture When considering the design of complex hardware systems, it often pays o ff to approach the implementation using a distributed architecture. Indeed, when using a distributed architecture, robustness increases due to the lack of single point-of-failures. In the case of the embedded hardware design of HiRIS, it can be beneficial to split up the hardware over multiple printed circuit boards (PCBs) that are tied together using one or multiple appropriate connectors. The hardware components can be grouped by functionality and can hence be isolated in the design process, which in turn can have advantages during the implementation and testing phase. This is especially important for testing individual boards with high component counts, as this distributed approach allows them to be tested without inducing dangerous voltages or currents to other parts of the device. Furthermore, sections of the design can easily be redesigned if deemed necessary after testing (i.e., it facilitates an iterative design approach), without having to reassemble the non-faulty parts of the system. This modular approach also has the added benefit of being able to make use of an extra spatial dimension in the hardware design by connecting multiple PCBs on top of each other, reducing the surface of the total design to its volume. When designing acoustic array sensors of the proposed complexity encountered in HiRIS, we often separate the microphones and some of their essential peripherals to a front-end PCB, and place the rest of the electronic components to a so called back-end PCB. As a beneficial side e ffect, this creates the potential for leaving front-facing side of the front-end PCB component-less, which is essential for eliminating distorting multipath e ffects in the acoustic reception pathways. Another advantage of the distributed architecture can be found in reusing known, verified and tested schematic and component layouts, used extensively in previous designs (i.e. the so-called battle-tested designs). By reusing parts of bothPreprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 3 32x Subordinate Node  Primary Node  External SDRAM (IS42S16320D) ARM-M4 Microcontroller (STM32F429) High-Speed USB PHY (USB3300) Micro-USB Connector Linear Voltage Regulator Linear Voltage Regulator Micro-USB Connector Clock x16 GPIO Trigger 32x Trigger output ... ... USB - UART Bridge (FT230X) Full-Duplex  RS-485 Driver (SN65HVD77DR) BNC Output 32x Clock Output Clock Buﬀer Driver (CDCLVC1108PWR) x4 Clock ARM-M4 Microcontroller (STM32F429) Ext. I/O Clock x16 Stereo PDM  Mic Data 32x 32-element array Linear Voltage Regulator 16x VReg 32-Element Microphone Array (SPH0641LU4H-1) ...... +5V Front-end Back-end +3.3V +3.3V +3.3V b) c)a) d d d d e e f Figure 1: Overview of the HiRIS hardware architecture. Panel a) show a schematic representation of the system architecture, distinguishing between the front-end and back-end PCBs. On the front-end, there are 32 groups of 32 microphones (each arranged in an 8x4 grid). The back-end PCB has one primary node which does clock distribution, triggering and synchronization of the subordinate nodes. The subordinate nodes each sample a 32-channel microphone group, using 16 IO pins with the microphones operating in dual-channel stereo mode. Panel b) shows the assembled front-end PCB measuring 180 mm by 170mm. Box e) indicates a single 8x4 group of microphones. Boxes labeled d) indicate the interconnect connectors between the front-end and back-end PCBs. Panel c) shows the assembled back-end PCB, which has the same dimensions as the front-end PCB. Panel e) indicates the primary node, and box f) shows a single subordinate node. the schematics and component layouts from previously built hardware, these parts can be distilled into design blocks, which then can be combined in the larger overarching design. These design blocks allowed us to quickly create a distributed hard- ware architecture of 32 microphone nodes by 33 microcontroller nodes, where every 32-element microphone node on the front end PCB is connected with an ARM microcontroller node with its peripherals on the back-end PCB. To orchestrate the 32 microcontroller nodes, an additional primary node was added on the back end. Employing this distributed design method of reusing existing design blocks has proved to be a highly productive and cost-efficient design methodology. 2.3 Front end The front-end board mainly incorporates 1024 Knowles SPH0641LU4H-1 MEMS microphones, sixteen AP2112K-3.3V linear voltage regulators that convert +5V to +3.3V that is used as the power source for the sensing elements. While each of the microphones typically only consumes 850 µA, we have provided for ample headroom in the power budget of the voltage regulators, which is why these regulators each power a group of 64 microphones. The aforementioned MEMS microphones are configured in a 32-by-32 uniform rectangular array with regular grid spacing of 3.9mm. When designing phased arrays, the impact of its configuration on the resulting frequency dependent steerable beam directivity pattern is determined by the sensing element spacing d. We can apply the Nyquist theorem to the spatial domain [17] and determine that grating lobes [ 18] are introduced into the directivity pattern, as spatial aliases, when the sensing element spacing d ⩾ λ 2 . Rearranging this simple equation, we can calculate the maximum frequency fmax for which this array geometry can be used for beam steering without spatial aliases: d = λ 2 = v 2 f ⇔ f = v 2d assuming the speed of sound in air v is approximately 343m/s, fmax is found to be 43.974kHz. Besides the small form factor, low power consumption and a frequency response curve reaching far into the ultrasonic spectrum [4], the key advantage of the SPH0641LU4H-1 micro- phones is their built-in Σ∆ ADC (analog-to-digital converter) that converts the captured acoustic wavefront into a PDM (pulse density modulation) 1-bit signal. The aforementioned Σ∆ ADC uses an external clock signal of 4.5MHz for sampling the analog signals to their digital 1-bit representation. Given the relatively large wavelength of an electrical wave of 4.5MHz in copper being approximately 44.4m, we can distribute these clock signals in phase to all 1024 microphones, thus allowing simultaneous sampling all of the microphones distributed over the PCB. This synchronous sampling is important for the subsequent operation of the sensor in an array fashion, which often leads to significant complications in RADAR-based sensing applications [19, 20, 21]. Using microphones with a built-in 1-bit ADC has as a major advantage a significant reduction in board complexity. Indeed, if microphones with an analog voltage output were to be used, each of these microphone signals would need amplification and a dedicated ADC chip, which adds a significant amount of complexity (as demonstrated in our earlier designs [22, 23]). Interfacing with 1-bit signals can be easily done using a wide GPIO register on a microcontroller. Further reduction of the necessary GPIO lines can be realizedPreprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 4 by utilizing the stereo-capability of the used PDM microphones. When using this implemented feature, one microphone will deliver its data on the rising edge of the clock signal where the other will deliver it on the falling edge. The latching of the data will occur on the opposite edges of the clock signal of the microphones. This stereo setup halves the number of data lines that are required from 1024 (without using the stereo feature) to 512 (when using the stereo feature). It should be noted that the while the induced phase di fferences of sampling on both the rising and falling edge of the data sampling is 180◦, this equates to 10µs, which equates to a negligible phase di fference when compared to the targeted acoustic signals between 25kHz and 100kHz. In addition to the aforementioned voltage regulators, mi- crophones and various passive components e.g. resistors and decoupling capacitors, eight high density FX10A-120P connectors can also be found on this PCB to connect the power, multiple synchronous clock lines and data lines to the back-end PCB of this design. These connectors ensure a high-fidelity link of the digital signals from the front end to the back end, ensuring robust operation of the sensor during field-trials. 2.4 Back end The back end of the HiRIS sensor can be split up in a primary node and 32 subordinate nodes. The latter are 32 identical design blocks with the STM32F429 ARM Cortex M4 microcon- troller at its core, in combination with an IS42S16320D external SDRAM memory of 64 MB, a USB3300 high-speed USB PHY , an AP2112K-3.3V linear voltage regulator, a micro USB connector and passive connectors for decoupling and impedance matching of the transmission lines. These nodes each have 16 GPIO pins respectively connected to 32 microphones in their stereo configuration, together with a separate GPIO pin acting as clock signal. Based on the rising and falling edges of the clock signal, the 16 GPIO pins are synchronously sampled and temporarily stored in memory until the next clock period. Upon receiving a trigger signal from the primary node, the subordinate nodes will store these temporarily into the SDRAM until a predetermined number of samples have been recorded. Given the 64MB of SDRAM memory capacity that every subordinate node has and a data stream of 18 MB/s per 32 microphones that are connected to it, a single continuous measurement of approximately 3.55s can be recorded. The primary node, that also has a STM32F429 ARM Cortex M4 microcontroller at its core, uses a single timer periph- eral that generates 4 synchronous square wave signal outputs at 4.5MHz that in turn each are connected to a Texas Instruments CDCLVC1108PWR low jitter, 1:8 LVCMOS fan-out clock buffer IC, effectively yielding 32 synchronous 4.5 MHz clock signals. These clock signals are distributed to the 32 subordinate node on the back end and their respective microphones on the front-end PCB. This ensures on-the-clock-true sampling of all microphones, which is important for the subsequent processing pipeline. While every subordinate node is equipped with a bi-directional high speed USB interface for establishing a connection with a computer, this interface does not su ffice to initiate a synchronous start of a measurement for all nodes, due to the unpredictability of the timing of data transfer over 32 separate USB channels. In order to establish a synchronous trigger to start a single measurement, the primary node uses 16 GPIO pins that are connected to the 32 subordinate nodes to initiate measurements on the latter by synchronously asserting a short pulse on these pins. Since the primary node does need a communication interface to a computer, it does not need to transfer large quantities of data in a short amount of time. Therefore an FTDI FT231X USB-to-UART bridge has been used in the primary node to establish a low-speed but reliable interface. Besides a USB connection to the primary node for initi- ating measurements, an external TTL-input can be used to trigger measurements, which allows for easy integration of the HiRIS sensor in measurement pipelines. For increased robust- ness, e.g. long cable lengths or noisy environments, the option of using differential signaling for external I /O was chosen by incorporating a SN65HVD77DR RS-485 driver into the design. Since this is a full-duplex interface, a pulse can also be gener- ated to trigger external devices along with the subordinate nodes. While the HiRIS is designed as a passive measurement device, a BNC connector was also fitted to the back end that is connected to the analog DAC-output of the primary node. The DAC peripheral can be triggered simultaneously with the subordinate nodes where it will generate an analog signal on its output based on a sequence that was either pre-defined in the firmware or uploaded to the primary node through its USB interface. This enables us to use this sensor as a high-channel pulse-echo sonar device when combined with an external amplifier and ultrasound transducer, similar to the sensors described in our earlier work [3, 4, 5, 14, 22, 23]. 2.5 Physical Realisation of the HiRIS Sensor The proposed 1024-microphone ultrasound array sensor, referred to as the HiRIS sensor (High-Resolution Imaging Sonar), comprising of the front and back end PCBs measures 180mm by 170mm by 20mm (including its protruding micro USB connectors). The two boards combined feature nearly 5000 components, 127m of PCB traces and costs e3962 to produce. Once powered up and operational, the overall sensor system consumes approximately 65W. This provided an unanticipated heat output of the sensor, which called for an adequate cooling solution. Indeed, when measuring the surface temperature of the PCBs, we noticed areas that reached up to 80◦C in a room with an ambient temperature of approximately 22 ◦C. While electronics are often rated to cope with higher temperatures it was deemed that including a method for passively cooling the sensor would be beneficial for its lifespan and the safety of its users. As a cooling solution, a copper slab of 250 mm by 170mm by 5mm was used between the front end and back end with a thermal interface on both sides for providing an optimal surface contact between the components and copper. To further increase the heat dissipation of the cooling solution, extra heat sinks were bolted onto the protruding ends of the copper slab. The effect of this cooling solution yielded a temperature decrease of 35◦C with maximum surface temperatures reaching up to 45◦C. As mentioned in the previous subsection, the HiRIS sen-Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 5 Figure 2: Coordinate system in relationship to the HiRIS sensor, showing the X, Y and Z axis, and the azimuth angle θ and elevation angle ϕ. sor can be expanded by connecting external devices through its external I/O or BNC connections but could also be further expanded with an additional PCB that stacks on the backside of the back end. This envisioned additional PCB would incorporate multiple USB3 hub ICs in order to reduce the amount of cable clutter. Another feature that would be integrated is a JTAG SWD programmer in combination with a multiplexer to alleviate the tedious work of plugging and unplugging the programmer when pushing firmware changes to the microcontrollers on the back end. 3 D ata acquisition andProcessing Chain In this section, we will detail the process of initiating and cap- turing a set of waveform data from the microphone array, and the subsequent processing using a bank of adaptive spatial filters (MVDR beamforming) for 3D image generation. 3.1 DAQ The HiRIS sensor comprises of 33 microcontrollers (1 primary node and 32 subordinate nodes), each connected over a USB2.0 connection to a host PC. To aggregate all the USB connections, a chain of USB3 hubs is used, which aggregates all the USB connections to a single USB3.0 connection. The USB protocol is a CDC Virtual Com Port (VCP) emulation [ 24], each initializing a virtual serial port on the host PC. A custom Python script using the Multiprocessing API [ 25], looks for specific serial ports connected to the system with specific Vendor ID and Product ID combinations and opens all these ports, which allows for bidirectional communication with all the sensor nodes. As stated before, the primary-subordinate architecture of our sensor implies that the single primary node of the HiRIS sensor listens to a command originating from a controlling PC over the VCP. In turn, the primary node asserts a trigger pulse to the 32 subordinate nodes, which each perform a measurement of a set duration (typically 70 ms). This data is then sent by each subordinate node over its serial port to the host PC, which combines all the data and stores them in a binary format on a mass storage device for subsequent processing. A single measurement of 70 ms is around 1.25 MB of data per subordinate node, equating to 40MB for a single measurement. Reworking this implies a datastream of 1GB per second when measuring with a 100 percent duty cycle. In practice, a duty cycle of 20 percent is more realistic, leading to a datarate of 200MB per second. 3.2 Image Formation using MVDR To process the massive amount of microphone data into a spacial spectrum, we follow an approach similar to the one outlined in [14]. The microphone signals are PDM modulated using single-bit Σ∆ modulation. In order to demodulate these sig- nals, we pass them through a low-pass filter, and decimate the resulting signal: sM,i(t) = hPDM ∗sPDM,i(t) The cutoff frequency of hPDM equals to 100kHz, which is well before the rise of the colored quantization noise, induced by a noise shaping on the MEMS microphones. This results in the i-th microphone signal sM,i, of which there are 1024 in the case of the HiRIS sensor. These 1024 microphone signals are all passed through a matched filter. The base signal sb(t) is the emitted signal in the case of an active sonar measurement (where the sensor emits a signal), or a Dirac delta function in case of a passive measurement (where the sensor listens to environmental signals: sMF,i(t) = F−1 \u0014 F[sb(t)]∗·F[sMF,i(t)] \u0015 Next, these signals are converted into a time-frequency distribu- tion using the short-time Fourier Transform (STFT), yielding a spectrum S MF,i(t, f ) for each i-th microphone signal. We choose a certain operating frequency f , in this case 42 kHz, and select the column of the STFT according to that frequency. This yields a complex signal xi(t) for each microphone. These signals are all concatenated into an observation matrix called X( f,t): X( f,t) = h x1( f,t) x2( f,t) ... xk( f,t) i For the spatial filtering, we apply a function on X( f,t), depend- ing on the kind of beamforming we want to achieve. We limit the scope of this paper to Minimum Variance Distortionless Response beamforming (MVDR) [6, 14, 26]. We use forward- backward spatial smoothing with diagonal loading in order to overcome the limitations posed by the sonar sensing modality in that only a single snapshot can be used to perform spatial filtering. Indeed, in many applications such as radar or mobile communications, multiple snapshots are available. However, due to the limited speed of sound, this is not possible in the HiRIS application [6, 14]. Therefore, we apply spatial smoothing by selecting sub-arrays of size 28x28, yielding 25 virtual snapshots from the 25 subarrays formed during the spatial smoothing pro- cess. From this, we build the sample covariance matrix Rb, and then calculate the weights of the MVDR beamformer: wMVDR(ψ) = R−1 b ·A(ψ) A(ψ)H ·R−1 b ·A(ψ)Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 6 where A(ψ) is the array manifold matrix for spatial direction ψ given the subarray geometry and frequency of operation. Using these MVDR-weights wMVDR we can then apply the spatial filter on the observation matrix X( f,t) by complex multiplication, yielding a beamformed signal xψ(t, f ) in direction ψ. To obtain spatial images of the surrounding of the sensor, various sampling strategies for ψcan be derived [ 27]. Indeed, in order to form a 2D image in the horizontal plane, we sample the azimuth angle θ in a regular manner from -90 ◦ to 90◦ (e.g., in steps of 1 ◦), while keeping the elevation angle ϕconstant and zero. For 3D images, we sample the direction vector ψuniformly on a sphere using a recursive zonal sphere partitioning algorithm [ 28], as this is the optimal sampling strategy for 3D scenes without prior knowledge. 4 V erification ofHiRIS 4.1 Simulation of Point Spread Functions In order to verify the operation of the HiRIS sensor, a simulation model of the sensor was built, following the equations derived in [26]. We calculate a so-called Point-Spread Function [29, 5] of the sensor system, which describes the image obtained by the sensor in response to a Dirac-like point source in space. We placed the point source in three spatial locations, defined by their azimuth angle (θ) and elevation angle (ϕ): (θ,ϕ) = (0◦,0◦), (30◦,0◦) and (−45◦,45◦). The resulting Point Spread Functions (PSFs) can be found in figure 3. Panels a-c show the point-spread function calculated using conventional Bartlett beamforming, and panels d-f show the PSF when using MVDR beamforming with spatial smoothing, a sub-array size of 28x28 (yielding 25 subarray snapshots), a signal to noise ratio of 5 dB and a diagonal loading of 0.1. What becomes clear from these point- spread functions is their extremely narrow opening angle, and, especially in the case of the MVDR beamformer, excellent peak to sidelobe ratio (approaching 70dB), which in turn will allow the construction of high-resolution acoustic images. 4.2 Real-world validation: Setup The realized prototype of HiRIS can be seen in figure 4. Panel a) shows the front-view of the sensor with the microphone port- holes and the copper slabs used for cooling. Panel b) shows the backside of the back-end PCB, with the USB cables connecting all the nodes to the USB hubs. These four USB hubs are then connected to an aggregate USB hub, which is connected to the host computer. 4.3 Real-world validation: Passive measurement In order to validate the Point-Spread Function of the realized prototype, we performed a passive acoustic measurement using an 40-kHz ultrasonic source placed in front of the microphone array, emitting this pure tone at approximately 70 dB SPL. A recording was made and the resulting data processed by the processing pipeline outline previously. The resulting images of the Point-Spread functions can be seen in figure 5. Panels a and b) show the resulting PSF when the data is processed using a broadband time-domain beamformer (delay-and-sum [3]), both on a logarithmic (a) and linear (b) scale. Panels c) and d) show the response of the system when using an MVDR beamformer, -60-70 a) b) c) -50 -40 -30 -20 -10 0 d) e) f) Figure 3: Point Spread Functions of point sources placed at different spatial locations: panel a & d) (θ,ϕ) = (0◦,0◦), panel b & e) (θ,ϕ) = (30◦,0◦), and panel c & f) (θ,ϕ) = (−45◦,45◦). Panels a-c) show the response of the system using Bartlett beam- forming, and panels d-f) show the response when using the MVDR beamformer. The PSFs are shown on a logarithmic scale. outlined in the previous section. High Peak-to-sidelobe ratios can be noted in these PSFs, however with some deviations from the simulated PSFs shown in figure 5. The reason for this dis- crepancy is most-likely the slight phase di fferences between the simulation models (which assumes a zero-phase transfer function of each microphone) and the real-world microphones (where a slight variation might occur in the phase response). However, calibration techniques to compensate these transfer function differences exist and can be easily incorporated into the processing pipeline [21, 30, 31, 32]. Finally, panels e) and f) show the response of the system to conventional Bartlett beam- forming, again corresponding to the responses simulation in figure 3. A much higher ’noise floor’ can be observed, caused by the more prominent sidelobes present in conventional beam- forming. 4.4 Active Measurements As a final experiment, we performed an active measurement. In this case, the HiRIS sensor uses a Senscomp 7000 trans- ducer [3, 29, 22, 23, 5, 33] to emit a broadband hyperbolic chirp. This chirp is generated by the DAC of the primary node and amplified using a custom high-voltage amplifier to a signal with an amplitude of 200V, superimposed on a bias of 200V. This emitted signal is reflected by the environment, and the subse-Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 7 d) c) a) USB Cables b) USB Hubs Cooling Figure 4: The realized prototype of HiRIS. Panel a) shows the front-view of the sensor with the microphone port-holes, and the 33 USB cables used to connect the nodes to the USB hubs. Panel b) shows the backside of the back-end PCB, with the USB cables connecting all the nodes to the USB hubs,and shows the copper cooling solution provisioned for heat management. The four USB hubs are then connected to an aggregate USB hub, which is connected to the host computer. Panel c) shows the front-view of the HiRIS sensor, where the component-less front-side is visible, with the exception of the holes for the bottom-mounted MEMS microphones. Panel d) shows the cluttered office space which has been ensonified during the active measurement experiment. quent reflections are recorded by the microphones. The signals are then processed into a 2D image using the method outlined before. The resulting 2D image can be found in figure 5, panel g). It shows the Cartesian representation of the 2D polar image (range and azimuth), on a logarithmic intensity scale. These images are often referred to as Energyscapes [ 22] or B-mode images [34, 35]. They show narrow responses both in angle (due to the MVDR beamforming using the 1024 microphone array), as well as narrow range localization (due to the broadband signal used in the matched filtering step), and a high signal-to-noise ratio due to the high number of microphones used. 5 D iscussion andFuture Work In this paper, we have presented HiRIS, the High Resolution Imaging Sonar, a sonar sensor with 1024 microphones. This microphone array is, to the best of the knowledge of the authors, the largest microphone array developed for ultrasound imaging in air to date of writing. We detailed the hardware architecture of the HiRIS sensor, indicating design choices and potential pitfalls when reproducing the hardware system. We provided a reasoning on why certain design choices have been made, and which can be used to inform future decisions when building similar hardware systems. Furthermore, we detailed the data-acquisition pipeline and signal processing approach, and tried to develop an intuition about the scales involved when dealing with a sensor of this complexity. We validated the operation of the system first by simulating the Point-Spread Function of the HiRIS sensor, and compared these resulting PSFs to real-life measurements. Furthermore, we performed an ensonification experiment of a cluttered office environment, and generated B-mode images of the resulting datastreams. With HiRIS, we have developed a novel sensor system which is a step change in imaging capabilities of in-air sonar sensors, and which will allow virtually artifact-free imaging of real-world scenes. Therefore, we see the HiRIS as a virtual upper limit of in-air sonar imaging: more complex sensors could indeed be implemented, but the industrial relevance of systems of this complexity can be debated. Evidently, the approach we have taken during the development of HiRIS is in stark contrast to the developments of our eRTIS line of sensors [3, 14], during which component cost reduction was the major driving force during development. These sensors have been utilized in real-world applications under industrial constraints [36, 37, 38, 39, 40], which has lead us to the ultimate question: what is the upper limit of ultrasound sensing that can be achieved, given the specular reflection model [2] under which the majority of ultrasound sensors operate. With HiRIS, we take the opposite approach: what is the upper limit that, given unrestricted sensing capabilities, can be achieved with in-air ultrasonic imaging, which in turn should lead to answers about the validity of the specular reflection model, the relative importance of diffraction echoes, and how semantic information about the environment is being translated into the ultrasonic sensing domain. To conclude, we believe that the HiRIS sensor will al- low us to uncover the underlying mechanics of in-air ultrasound sensing in a previously unobtainable level of detail, which will then inform the development of future installments of 3D ultrasound sensors for industrial applications. In future work, we aim to further quantify the perfor- mance of the HiRIS sensor, both in laboratory settings as well as real-world measurements. We will produce high-resolution datasets, which will be made open-source for the sensing community to evaluate and use. Using these datasets it should become possible to quantify how information-rich real-world ultrasound measurements really are, and how this information can be leveraged to provide robots with a rich understanding of their environments using ultrasound as a primary sensing modality. From these measurements, the e ffect of applying reduced-aperture microphone arrays instead of the large 1024 element array can be accurately calculated, as virtually any reduced aperture can be adequately simulated using the HiRIS array.Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 8 -10 -5 0 5 10 Cross Range (m) 0 2 4 6 8 10 12Range (m) -30 -25 -20 -15 -10 -5 0 Image Intensity (dB) -70 -60 -50 -40 -30 -20 -10 0 (a, c, e): Image Energy (dB) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (b, d, f):Image Energy (Linear) a) c) e) f) d) b) g) Figure 5: Experimental results of the HiRIS sensor. Panels a-c show the response of a 40-kHz source placed in front of the HiRIS sensor, using various processing techniques (a: Delay and Sum, b: MVDR and c: Bartlett beamforming), on a logarithmic scale. Panels b-f) show the response on a linear scale. Panel g) shows the B-mode image of a scene ensonified using a broadband chirp, and processed using the algorithm described in this paper. Acknowledgements The authors would like to thank the Bijzonder OnderzoeksFonds (BOF) of the University of Antwerp for funding this research project. References [1] Ulf Michel. History of acoustic beamforming. In 1st. Berlin Beamforming Conference, 2006. [2] Allan D Pierce. Acoustics: an introduction to its physical principles and applications. Springer, 2019. [3] Robin Kerstens, Dennis Laurijssen, and Jan Steckel. ertis: A fully embedded real time 3d imaging sonar sensor for robotic applications. In 2019 International Conference on Robotics and Automation (ICRA), pages 1438–1443, 2019. doi: 10.1109/ICRA.2019.8794419. [4] Robin Kerstens, Dennis Laurijssen, and Jan Steckel. Low- cost one-bit mems microphone arrays for in-air acoustic imaging using fpga’s. In 2017 IEEE SENSORS, pages 1–3. IEEE, 2017. [5] Robin Kerstens, Dennis Laurijssen, and Jan Steckel. An optimized planar mimo array approach to in-air synthetic aperture sonar. IEEE Sensors Letters, 3(11):1–4, 2019. [6] Thomas Verellen, Robin Kerstens, and Jan Steckel. High- resolution ultrasound sensing for robotics using dense mi- crophone arrays. IEEE Access, 8:190083–190093, 2020. doi: 10.1109/ACCESS.2020.3032177. [7] Gianni Allevato, Jan Hinrichs, Matthias Rutsch, Jan Philipp Adler, Axel Jäger, Marius Pesavento, and Mario Kupnik. Real-time 3-d imaging using an air-coupled ultrasonic phased-array. IEEE transactions on ultrason- ics, ferroelectrics, and frequency control, 68(3):796–806, 2020. [8] Gianni Allevato, Matthias Rutsch, Jan Hinrichs, Christoph Haugwitz, Raphael Müller, Marius Pesavento, and Mario Kupnik. Air-coupled ultrasonic spiral phased array for high-precision beamforming and imaging. IEEE Open Journal of Ultrasonics, Ferroelectrics, and Frequency Con- trol, 2:40–54, 2022. [9] Gianni Allevato, Christoph Haugwitz, Matthias Rutsch, Raphael Müller, Marius Pesavento, and Mario Kupnik. Two-scale sparse spiral array design for 3d ultrasound imaging in air. IEEE Open Journal of Ultrasonics, Ferro- electrics, and Frequency Control, 2023. [10] Erik Verreycken, Walter Daems, and Jan Steckel. Dis- tributed low-cost microphone array for 3d localization for bio-acoustic applications. In 2017 IEEE SENSORS, pages 1–3. IEEE, 2017. [11] Erik Verreycken, Walter Daems, and Jan Steckel. Passive acoustic sound source tracking in 3d using distributed mi- crophone arrays. In 2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN), pages 1–8. IEEE, 2018. [12] Erik Verreycken, Ralph Simon, Brandt Quirk-Royal, Wal- ter Daems, Jesse Barber, and Jan Steckel. Bio-acoustic tracking and localization using heterogeneous, scalable microphone arrays. Communications biology, 4(1):1275, 2021. [13] Dennis Laurijssen, Robin Kerstens, Girmi Schouten, Wal- ter Daems, and Jan Steckel. A flexible low-cost biolog- ically inspired sonar sensor platform for robotic applica-Preprint – HiRIS: an Airborne Sonar Sensor with a1024 Channel Microphone Array forIn-Air Acoustic Imaging 9 tions. In 2019 International Conference on Robotics and Automation (ICRA), pages 9617–9623. IEEE, 2019. [14] Thomas Verellen, Robin Kerstens, Dennis Laurijssen, and Jan Steckel. Urtis: A small 3d imaging sonar sensor for robotic applications. In ICASSP 2020-2020 IEEE Interna- tional Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), pages 4801–4805. IEEE, 2020. [15] Dennis Laurijssen, Anthony Schenck, Girmi Schouten, Robin Kerstens, Sebastiaan Aussems, Eric Paillet, Randy Gomez, Keisuke Nakamura, Walter Daems, and Jan Steckel. Bio-inspired gesture recognition with baffled trans- ducers using temporal and spectral features. In 2023 IEEE SENSORS, pages 1–4. IEEE, 2023. [16] Dennis Laurijssen, Steven Truijen, Wim Saeys, Walter Daems, and Jan Steckel. A flexible embedded hardware platform supporting low-cost human pose estimation. In 2016 International Conference on Indoor Positioning and Indoor Navigation (IPIN), pages 1–8. IEEE, 2016. [17] Jacek Dmochowski, Jacob Benesty, and Sofiène Affès. On spatial aliasing in microphone arrays. IEEE Transactions on Signal Processing, 57(4):1383–1395, 2008. [18] Eric Konetzke, Matthias Rutsch, Maik Hoffmann, Alexan- der Unger, René Golinske, Dirk Killat, Sivaram Nishal Ramadas, Steve Dixon, and Mario Kupnik. Phased array transducer for emitting 40-khz air-coupled ultrasound with- out grating lobes. In 2015 IEEE International Ultrasonics Symposium (IUS), pages 1–4. IEEE, 2015. [19] Da Liang, Kaiyu Liu, Heng Zhang, Yunkai Deng, Dacheng Liu, Yafeng Chen, Chuang Li, Haixia Yue, and Robert Wang. A high-accuracy synchronization phase- compensation method based on kalman filter for bistatic synthetic aperture radar. IEEE Geoscience and Remote Sensing Letters, 17(10):1722–1726, 2019. [20] Xiaopeng Yang, Pilei Yin, and Tao Zeng. Time and phase synchronization for wideband distributed coherent aperture radar. 2013. [21] Shivansh Chaudhary and Abhay Samant. Characteriza- tion and calibration techniques for multi-channel phase- coherent systems. In 2015 IEEE AUTOTESTCON, pages 334–338. IEEE, 2015. [22] Jan Steckel, Andre Boen, and Herbert Peremans. Broad- band 3-d sonar system using a sparse array for indoor nav- igation. IEEE Transactions on Robotics, 29(1):161–171, 2012. [23] Jan Steckel and Herbert Peremans. Batslam: Simultaneous localization and mapping using biomimetic sonar. PloS one, 8(1):e54076, 2013. [24] Class definitions for communication devices 1.2. https://www.usb.org/document-library/ class-definitions-communication-devices-12 . [25] Navtej Singh, Lisa-Marie Browne, and Ray Butler. Parallel astronomical data processing with python: Recipes for multicore machines. Astronomy and Computing, 2:1–10, 2013. [26] Harry L Van Trees. Optimum array processing: Part IV of detection, estimation, and modulation theory. John Wiley & Sons, 2002. [27] Jonas Reijniers, Robin Kerstens, and Jan Steckel. An optimized spatial sampling strategy for wide-view planar array 3-d sonar sensors. IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control, 67(6):1236–1241, 2020. [28] Paul Leopardi. A partition of the unit sphere into regions of equal area and small diameter. Electronic Transactions on Numerical Analysis, 25(12):309–327, 2006. [29] Jan Steckel. Sonar system combining an emitter array with a sparse receiver array for air-coupled applications. IEEE Sensors Journal, 15(6):3446–3452, 2015. [30] Jack J Schuss, Thomas V Sikina, Joseph E Hilliard, Patrick J Makridakis, Je ffrey Upton, Joseph C Yeh, and Stephen M Sparagna. Large-scale phased array calibration. IEEE Transactions on Antennas and Propagation, 67(9): 5919–5933, 2019. [31] Guolong He, Xin Gao, and Rentian Zhang. Impact analysis and calibration methods of excitation errors for phased array antennas. IEEE Access, 9:59010–59026, 2021. [32] Ashok Agrawal and Allan Jablon. A calibration tech- nique for active phased array antennas. In IEEE Interna- tional Symposium on Phased Array Systems and Technol- ogy, 2003., pages 223–228. IEEE, 2003. [33] Dennis Laurijssen, Steven Truijen, Wim Saeys, and Jan Steckel. Three sources, three receivers, six degrees of free- dom: An ultrasonic sensor for pose estimation & motion capture. In 2015 IEEE SENSORS, pages 1–4. IEEE, 2015. [34] Elisabetta Sassaroli, Calum Crake, Andrea Scorza, Don- Soo Kim, and Mi-Ae Park. Image quality evaluation of ultrasound imaging systems: advanced b-modes. Journal of applied clinical medical physics, 20(3):115–124, 2019. [35] Giulia Matrone, Alessandro Stuart Savoia, Giosuè Caliano, and Giovanni Magenes. The delay multiply and sum beam- forming algorithm in ultrasound b-mode medical imaging. IEEE transactions on medical imaging, 34(4):940–949, 2014. [36] Anthony Schenck, Walter Daems, and Jan Steckel. Airleak- slam: Automated air leak detection. In Advances on P2P , Parallel, Grid, Cloud and Internet Computing: Proceed- ings of the 14th International Conference on P2P , Parallel, Grid, Cloud and Internet Computing (3PGCIC-2019) 14, pages 746–755. Springer, 2020. [37] Anthony Schenck, Walter Daems, and Jan Steckel. Air- leakslam: detection of pressurized air leaks using passive ultrasonic sensors. In 2019 IEEE SENSORS, pages 1–4. IEEE, 2019. [38] Robin Kerstens, Wouter Jansen, Gauthier de Borrekens, Stefaan Ides, and Jan Steckel. Tracking moored vessel movement in multiple dof using active sensing methods. IEEE Sensors Letters, 7(2):1–3, 2023. [39] Thomas Verellen, Florian Verbelen, Kurt Stockman, and Jan Steckel. Beamforming applied to ultrasound analysis in detection of bearing defects. Sensors, 21(20):6803, 2021. [40] Wouter Jansen, Dennis Laurijssen, and Jan Steckel. Real- time sonar fusion for layered navigation controller.Sensors, 22(9):3109, 2022.",
      "references": [
        "History of acoustic beamforming.",
        "Acoustics: an introduction to its physical principles and applications.",
        "ertis: A fully embedded real time 3d imaging sonar sensor for robotic applications.",
        "Low- cost one-bit mems microphone arrays for in-air acoustic imaging using fpga’s.",
        "An optimized planar mimo array approach to in-air synthetic aperture sonar.",
        "High- resolution ultrasound sensing for robotics using dense mi- crophone arrays.",
        "Real-time 3-d imaging using an air-coupled ultrasonic phased-array.",
        "Air-coupled ultrasonic spiral phased array for high-precision beamforming and imaging.",
        "Two-scale sparse spiral array design for 3d ultrasound imaging in air.",
        "Dis- tributed low-cost microphone array for 3d localization for bio-acoustic applications.",
        "Passive acoustic sound source tracking in 3d using distributed mi- crophone arrays.",
        "Bio-acoustic tracking and localization using heterogeneous, scalable microphone arrays.",
        "A flexible low-cost biolog- ically inspired sonar sensor platform for robotic applica- tions.",
        "Urtis: A small 3d imaging sonar sensor for robotic applications.",
        "Bio-inspired gesture recognition with baffled trans- ducers using temporal and spectral features.",
        "A flexible embedded hardware platform supporting low-cost human pose estimation.",
        "On spatial aliasing in microphone arrays.",
        "Phased array transducer for emitting 40-khz air-coupled ultrasound with- out grating lobes.",
        "A high-accuracy synchronization phase- compensation method based on kalman filter for bistatic synthetic aperture radar.",
        "Time and phase synchronization for wideband distributed coherent aperture radar.",
        "Characteriza- tion and calibration techniques for multi-channel phase- coherent systems.",
        "Broad- band 3-d sonar system using a sparse array for indoor nav- igation.",
        "Batslam: Simultaneous localization and mapping using biomimetic sonar.",
        "Class definitions for communication devices 1.2.",
        "Parallel astronomical data processing with python: Recipes for multicore machines.",
        "Optimum array processing: Part IV of detection, estimation, and modulation theory.",
        "An optimized spatial sampling strategy for wide-view planar array 3-d sonar sensors.",
        "A partition of the unit sphere into regions of equal area and small diameter.",
        "Sonar system combining an emitter array with a sparse receiver array for air-coupled applications.",
        "Large-scale phased array calibration.",
        "Impact analysis and calibration methods of excitation errors for phased array antennas.",
        "A calibration tech- nique for active phased array antennas.",
        "Three sources, three receivers, six degrees of free- dom: An ultrasonic sensor for pose estimation & motion capture.",
        "Image quality evaluation of ultrasound imaging systems: advanced b-modes.",
        "The delay multiply and sum beam- forming algorithm in ultrasound b-mode medical imaging.",
        "Airleak- slam: Automated air leak detection.",
        "Air- leakslam: detection of pressurized air leaks using passive ultrasonic sensors.",
        "Tracking moored vessel movement in multiple dof using active sensing methods.",
        "Beamforming applied to ultrasound analysis in detection of bearing defects.",
        "Real- time sonar fusion for layered navigation controller."
      ],
      "meta_data": {
        "arxiv_id": "2402.13110v1",
        "authors": [
          "Dennis Laurijssen",
          "Walter Daems",
          "Jan Steckel"
        ],
        "published_date": "2024-02-20T16:03:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces HiRIS, a high-resolution in-air ultrasonic imaging sensor using a dense 1024-channel (32×32) MEMS microphone array and distributed embedded DAQ hardware, addressing artifacts/dynamic-range limits of prior reduced-aperture arrays. Demonstrates near artifact-free 2D/3D imaging over the frontal hemisphere with high angular accuracy and up to ~70 dB mainlobe-to-sidelobe ratio using broadband MVDR beamforming with forward-backward spatial smoothing. Provides detailed hardware architecture, DAQ pipeline, and imaging signal-processing chain; validates via simulations (PSF) and real passive/active measurements.",
        "methodology": "Hardware: 1024 Knowles SPH0641LU4H-1 PDM MEMS mics in 3.9 mm-spaced URA; distributed architecture with 32 subordinate STM32F429 nodes (each samples 32 mics via 16 GPIO in stereo PDM) plus one primary STM32F429 node for 4.5 MHz clock fan-out (CDCLVC1108) and synchronized trigger; SDRAM buffering (64 MB/node); USB CDC-VCP to host; optional DAC output for active chirp emission. Signal processing: PDM demodulation (LPF to 100 kHz + decimation), optional matched filtering (emitted waveform or delta for passive), STFT, select operating frequency (e.g., 42 kHz), build observation matrix, estimate covariance via forward-backward spatial smoothing using 28×28 subarrays (25 virtual snapshots) with diagonal loading, compute MVDR weights and beamform; 2D scans by azimuth sampling, 3D by uniform spherical sampling via recursive zonal sphere partitioning.",
        "experimental_setup": "No public dataset/benchmark; validation through (1) simulated point-spread functions using array model (per Van Trees) comparing Bartlett vs MVDR+FB-SS (SNR 5 dB, diagonal loading 0.1, subarray 28×28) for sources at (θ,ϕ)=(0,0),(30,0),(-45,45). (2) Real-world passive measurement: single ~40 kHz tone source placed in front, ~70 dB SPL; process with delay-and-sum, Bartlett, and MVDR to obtain PSFs (log/linear scales). (3) Real-world active measurement: Senscomp 7000 transducer emits broadband hyperbolic chirp generated by primary-node DAC, amplified to ~200 Vpp with 200 V bias; reflections recorded and processed to produce 2D range–azimuth B-mode/energyscape image of a cluttered office. DAQ: 33 USB2 links aggregated via USB3 hubs; Python multiprocessing script enumerates VID/PID serial ports; typical capture 70 ms (~40 MB total).",
        "limitations": "High cost/complexity: ~4963 components, 33 MCUs, ~€3962 build cost, ~65 W power leading to thermal issues (needs heat-spreader). Data throughput/storage burden: ~40 MB per 70 ms capture; sustained high-rate acquisition limited by USB and practical duty cycle. Array spacing imposes alias-free steering limit ~44 kHz (grating lobes above). Real PSF deviates from simulation due to unmodeled per-microphone phase/transfer-function variations; requires calibration not yet implemented. MVDR relies on spatial smoothing/diagonal loading assumptions for single-snapshot regime; performance sensitive to parameter choices and coherence/model mismatch. Evaluation is mostly qualitative with limited scenarios; no standardized quantitative benchmarks, long-term robustness, or outdoor/industrial tests reported.",
        "future_research_directions": "Implement and validate per-channel phase/amplitude calibration and compensation; quantify imaging performance (angular/range resolution, dynamic range, sidelobe levels) across frequencies and environments. Release open-source high-resolution datasets and tools; study information content of airborne ultrasound, including diffraction echoes vs specular model validity. Use HiRIS as a reference to simulate/optimize reduced-aperture designs and derive cost–performance trade-offs for industrial sensors. Extend active sonar modalities (MIMO, coded waveforms), real-time/embedded processing, and multi-sensor fusion for robotics. Improve hardware integration: USB3 hub backplane, simplified cabling, programming infrastructure, and power/thermal optimization.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Most test-time prompt/self-refinement optimizers assume the available feedback signal is not only *noisy* but also *aligned* with the real deployment metric. In practice, feedback is often a *biased proxy* (e.g., lenient heuristics, flaky tests, or learned reward models) that can be systematically gamed: iterative refinement drifts toward outputs that score well under the proxy while regressing on the true objective (reward hacking / specification gaming). Existing verifier-gated self-refine and confidence-gated variants largely address random label noise, but do not provide a training-free, test-time mechanism to (i) detect proxy–true misalignment, (ii) adaptively spend a small “audit” budget on higher-fidelity checks, and (iii) enforce a conservative accept/stop rule that is robust to both stochastic noise and *systematic* proxy bias.\n\nKey gap: a lightweight, Python-verifiable algorithm for iterative prompt optimization that remains reliable when the only cheap signal is a biased/noisy proxy, while a small number of expensive/high-fidelity evaluations are available (human spot-checks, full integration tests, or stricter validators).",
    "method": "Audit-Calibrated Confidence Refinement (ACCR): a training-free refinement controller that couples (A) sequential confidence-gated selection under a *cheap proxy* verifier with (B) online calibration using a small number of *audits* from a higher-fidelity verifier.\n\nCore idea: treat the proxy as a stochastic, potentially biased classifier of “true success.” Instead of assuming proxy scores are directly comparable across candidates, ACCR continuously estimates the proxy’s *precision* and *negative predictive value* from audits and uses these to convert proxy observations into *conservative bounds* on true utility.\n\nAlgorithm (dataset-level online, test-time only):\n1) Maintain Beta posteriors over two calibration quantities estimated from audits:\n   - π = P(TrueSuccess=1 | Proxy=1)  (proxy precision)\n   - ν = P(TrueSuccess=0 | Proxy=0)  (proxy true-negative rate; gives P(TrueSuccess=1 | Proxy=0)=1−ν)\n   These are updated online across instances, requiring no training data.\n2) Per instance, per refinement round:\n   a) PROPOSE: generate critique + a batch of B candidate refinements via stochastic decoding.\n   b) PROXY SELECTION (sequential): allocate a proxy-query budget adaptively (Hoeffding-style UCB/LCB) to identify the best candidate under proxy uncertainty without over-spending.\n   c) AUDIT ALLOCATION (novel): spend a small audit budget on the most “proxy-promising” candidates and/or on candidates where the decision boundary is ambiguous. Audits use a stricter/high-fidelity verifier.\n   d) CALIBRATED ACCEPT/STOP (novel): for each candidate y, compute conservative bounds on *true* success probability using posterior quantiles of π and ν:\n      LCB_true(y) = q(y)*LCB(π) + (1−q(y))*(1−UCB(ν))\n      UCB_true(y) = q(y)*UCB(π) + (1−q(y))*(1−LCB(ν))\n      where q(y) is the estimated proxy-success probability from repeated proxy queries.\n      Define a regularized utility bound with a simplicity penalty to reduce verbosity hacking:\n      U(y)=P_true(y) − λ·Len(y).\n      Accept y_new only if LCB_U(y_new) > UCB_U(y_current)+τ; otherwise stop early (or use patience).\n\nWhy this is novel vs. Self-Refine / verifier gating / SCG-SR:\n- Moves beyond “proxy-as-truth” noise models to *explicit proxy misalignment* (reward hacking) and uses a principled audit-calibration layer at test time.\n- Introduces an online, training-free method to estimate and bound proxy reliability (π,ν) and propagate that uncertainty into accept/stop decisions.\n- Provides a concrete compute–risk trade-off: cheap proxy calls + rare audits, with conservative acceptance that reduces systematic regressions under biased verifiers.\n\nSocial value: this directly targets safety-critical failure modes where systems optimize for the wrong metric (misleading tests, miscalibrated reward models), supporting more trustworthy automated prompt optimization with limited human oversight.",
    "experimental_setup": "Goal: show ACCR prevents proxy-gaming and improves *true* performance under a biased/noisy feedback signal, using only small audit budgets.\n\nTasks (fully local, Python-verifiable):\n1) Arithmetic word problems (synthetic) with strict true scoring.\n\nTrue evaluator (high-fidelity / “audit”): STRICT exactness—prediction is correct iff the *first integer* in the output equals the gold answer. This represents real deployment constraints like parsers/validators requiring exact formats.\n\nProxy evaluator (cheap, biased): LENIENT match—returns 1 iff *any integer anywhere* in the output equals the gold answer, optionally with label flips p_flip (simulating flaky checks / reward-model noise). This proxy is systematically biased toward verbose “show your work” outputs that include many numbers, creating reward-hacking pressure.\n\nModels:\n- google/flan-t5-small via HuggingFace transformers.\n\nBaselines:\n1) Direct generation (answer-only prompt).\n2) SELF-REFINE (feedback+refine loop, no external selection).\n3) SCG-SR(Proxy): sequential confidence gating using only proxy feedback (no audits), analogous to the current hypothesis but vulnerable to proxy bias.\n4) ACCR (proposed): proxy sequential sampling + audit-calibrated acceptance bounds.\n\nBudgets (per instance, matched compute across methods):\n- Proxy calls per refinement round: e.g., 12.\n- Audit calls per refinement round: small, e.g., 1–2 (or a global audit budget per instance).\n\nData:\n- Train/dev/test generated with fixed seeds (e.g., dev 100, test 500). Dev only tunes λ and τ at one noise level.\n\nMetrics:\n- Primary: strict exact-match accuracy under the *true* evaluator.\n- Secondary: proxy-score (to show decoupling), average proxy calls, average audit calls, average iterations, and average output length.\n\nStress conditions:\n- Proxy noise p_flip ∈ {0.0, 0.1, 0.2}.\n- Proxy bias strength increased by using refinement prompts that sometimes elicit explanations (hence many numbers), amplifying the proxy/true mismatch.\n\nHypothesis to test:\nUnder proxy bias/noise, SCG-SR(Proxy) will over-optimize the lenient proxy and lose strict accuracy, while ACCR will maintain or improve strict accuracy with only a small audit budget.",
    "primary_metric": "accuracy",
    "experimental_code": "import re, random, math\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nMODEL = \"google/flan-t5-small\"\ntok = AutoTokenizer.from_pretrained(MODEL)\nmdl = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\nmdl.eval()\n\n# ------------------ data ------------------\n\ndef make_data(n=500, seed=0):\n    rng = random.Random(seed)\n    ops = [\"+\", \"-\", \"*\"]\n    data = []\n    for _ in range(n):\n        a, b = rng.randint(0, 50), rng.randint(0, 50)\n        op = rng.choice(ops)\n        ans = a + b if op == \"+\" else (a - b if op == \"-\" else a * b)\n        x = f\"What is {a} {op} {b}?\"\n        data.append((x, str(ans)))\n    return data\n\n_ints = re.compile(r\"-?\\d+\")\n\ndef ints_in(text: str):\n    return _ints.findall(text)\n\n# High-fidelity \"true\" verifier: strict formatting (first integer must be the answer)\n\ndef true_score_strict(pred_text: str, gold: str) -> float:\n    xs = ints_in(pred_text)\n    return 1.0 if (len(xs) > 0 and xs[0] == gold) else 0.0\n\n# Cheap proxy: lenient match (any integer match), plus optional stochastic flips\n\ndef proxy_score_lenient(pred_text: str, gold: str, p_flip: float, rng: random.Random) -> float:\n    xs = ints_in(pred_text)\n    s = 1.0 if (gold in xs) else 0.0\n    if rng.random() < p_flip:\n        s = 1.0 - s\n    return s\n\n# ------------------ generation helpers ------------------\n\ndef gen_one(prompt, max_new_tokens=80, do_sample=False, temperature=0.7):\n    inputs = tok(prompt, return_tensors=\"pt\")\n    with torch.no_grad():\n        out = mdl.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            temperature=temperature,\n        )\n    return tok.decode(out[0], skip_special_tokens=True).strip()\n\n\ndef gen_many(prompt, B=4, max_new_tokens=80, temperature=0.8):\n    inputs = tok(prompt, return_tensors=\"pt\")\n    with torch.no_grad():\n        outs = mdl.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            num_return_sequences=B,\n        )\n    return [tok.decode(o, skip_special_tokens=True).strip() for o in outs]\n\n# ------------------ baselines ------------------\n\ndef direct(x):\n    return gen_one(f\"Solve and output ONLY the number.\\nProblem: {x}\\nAnswer:\")\n\n\ndef self_refine(x, T=3):\n    y = direct(x)\n    for _ in range(T):\n        fb = gen_one(\n            \"You are a strict math checker. Given the problem and the proposed answer, \"\n            \"say what is wrong (if anything) and how to fix it.\\n\"\n            f\"Problem: {x}\\nProposed answer: {y}\\nFeedback:\" )\n        # Note: refine prompt can sometimes elicit verbosity, creating proxy-gaming risk\n        y = gen_one(\n            \"Revise the answer using the feedback.\\n\"\n            \"IMPORTANT: Put the final answer first, then you may add a brief explanation.\\n\"\n            f\"Problem: {x}\\nCurrent answer: {y}\\nFeedback: {fb}\\nRevised response:\" )\n    return y\n\n# ------------------ confidence utilities ------------------\n\ndef hoeffding_rad(n, delta):\n    return math.sqrt(math.log(2.0 / delta) / (2.0 * max(1, n)))\n\n\ndef beta_q(a, b, q):\n    # quantile of Beta(a,b) using torch\n    dist = torch.distributions.Beta(torch.tensor(float(a)), torch.tensor(float(b)))\n    return float(dist.icdf(torch.tensor(float(q))))\n\n# ------------------ SCG-SR with proxy only (no audits) ------------------\n\ndef scg_sr_proxy(\n    x,\n    gold,\n    T=3,\n    B=4,\n    lam=0.01,\n    p_flip=0.1,\n    delta=0.05,\n    tau=0.0,\n    per_round_proxy_budget=12,\n    patience=1,\n    seed=0,\n):\n    rng = random.Random(seed)\n\n    def util(mean_proxy, y_text):\n        return mean_proxy - lam * len(tok.encode(y_text))\n\n    y_star = direct(x)\n    n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n\n    bad = 0\n    for _ in range(T):\n        fb = gen_one(\n            \"You are a strict math checker. Given the problem and the proposed answer, \"\n            \"say what is wrong (if anything) and how to fix it.\\n\"\n            f\"Problem: {x}\\nProposed answer: {y_star}\\nFeedback:\" )\n        refine_prompt = (\n            \"Revise the answer using the feedback.\\n\"\n            \"IMPORTANT: Put the final answer first, then you may add a brief explanation.\\n\"\n            f\"Problem: {x}\\nCurrent answer: {y_star}\\nFeedback: {fb}\\nRevised response:\" )\n        cands = gen_many(refine_prompt, B=B)\n\n        n = [0]*B\n        s = [0.0]*B\n        evals = 0\n        for i in range(B):\n            if evals >= per_round_proxy_budget:\n                break\n            n[i] += 1\n            s[i] += proxy_score_lenient(cands[i], gold, p_flip, rng)\n            evals += 1\n        if evals < per_round_proxy_budget:\n            n_star += 1\n            sum_star += proxy_score_lenient(y_star, gold, p_flip, rng)\n            evals += 1\n\n        while evals < per_round_proxy_budget:\n            ucbs = []\n            lcbs = []\n            for i in range(B):\n                mean_i = s[i] / max(1, n[i])\n                r = hoeffding_rad(n[i], delta/(B+1))\n                ucbs.append(util(min(1.0, mean_i + r), cands[i]))\n                lcbs.append(util(max(0.0, mean_i - r), cands[i]))\n            order = sorted(range(B), key=lambda i: ucbs[i], reverse=True)\n            best_i = order[0]\n            second_i = order[1] if B > 1 else order[0]\n            if lcbs[best_i] > ucbs[second_i] + 1e-9:\n                break\n            # sample best UCB arm\n            i = best_i\n            n[i] += 1\n            s[i] += proxy_score_lenient(cands[i], gold, p_flip, rng)\n            evals += 1\n\n        # select by conservative proxy LCB\n        best_i, best_lcb = 0, -1e9\n        for i in range(B):\n            mean_i = s[i] / max(1, n[i])\n            r = hoeffding_rad(n[i], delta/(B+1))\n            lcb = util(max(0.0, mean_i - r), cands[i])\n            if lcb > best_lcb:\n                best_lcb, best_i = lcb, i\n\n        mean_star = sum_star / n_star\n        r_star = hoeffding_rad(n_star, delta/(B+1))\n        ucb_star = util(min(1.0, mean_star + r_star), y_star)\n\n        if best_lcb > ucb_star + tau:\n            y_star = cands[best_i]\n            n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                break\n\n    return y_star\n\n# ------------------ Proposed: ACCR (audit-calibrated) ------------------\n\nclass ProxyCalibrator:\n    \"\"\"Online Beta-posteriors for proxy precision and true-negative-rate.\"\"\"\n    def __init__(self, a_pi=1, b_pi=1, a_nu=1, b_nu=1):\n        # pi = P(true=1|proxy=1)\n        self.a_pi, self.b_pi = a_pi, b_pi\n        # nu = P(true=0|proxy=0)\n        self.a_nu, self.b_nu = a_nu, b_nu\n\n    def update(self, proxy_outcome: int, true_outcome: int):\n        if proxy_outcome == 1:\n            if true_outcome == 1:\n                self.a_pi += 1\n            else:\n                self.b_pi += 1\n        else:\n            if true_outcome == 0:\n                self.a_nu += 1\n            else:\n                self.b_nu += 1\n\n    def bounds(self, alpha=0.05):\n        l_pi = beta_q(self.a_pi, self.b_pi, alpha/2)\n        u_pi = beta_q(self.a_pi, self.b_pi, 1-alpha/2)\n        l_nu = beta_q(self.a_nu, self.b_nu, alpha/2)\n        u_nu = beta_q(self.a_nu, self.b_nu, 1-alpha/2)\n        return (l_pi, u_pi, l_nu, u_nu)\n\n\ndef accr(\n    x,\n    gold,\n    calibrator: ProxyCalibrator,\n    T=3,\n    B=4,\n    lam=0.01,\n    p_flip=0.1,\n    delta=0.05,\n    tau=0.0,\n    per_round_proxy_budget=12,\n    per_round_audit_budget=1,\n    alpha_cal=0.05,\n    patience=1,\n    seed=0,\n):\n    rng = random.Random(seed)\n\n    def util_from_trueprob(p_true, y_text):\n        return p_true - lam * len(tok.encode(y_text))\n\n    def calibrated_bounds_from_proxy(mean_proxy, y_text):\n        l_pi, u_pi, l_nu, u_nu = calibrator.bounds(alpha=alpha_cal)\n        # P(true=1|proxy=0) = 1 - nu\n        l_true = mean_proxy * l_pi + (1.0 - mean_proxy) * (1.0 - u_nu)\n        u_true = mean_proxy * u_pi + (1.0 - mean_proxy) * (1.0 - l_nu)\n        return util_from_trueprob(l_true, y_text), util_from_trueprob(u_true, y_text)\n\n    y_star = direct(x)\n    # track proxy estimate for current\n    n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n\n    bad = 0\n    for _ in range(T):\n        fb = gen_one(\n            \"You are a strict math checker. Given the problem and the proposed answer, \"\n            \"say what is wrong (if anything) and how to fix it.\\n\"\n            f\"Problem: {x}\\nProposed answer: {y_star}\\nFeedback:\" )\n        refine_prompt = (\n            \"Revise the answer using the feedback.\\n\"\n            \"IMPORTANT: Put the final answer first, then you may add a brief explanation.\\n\"\n            f\"Problem: {x}\\nCurrent answer: {y_star}\\nFeedback: {fb}\\nRevised response:\" )\n        cands = gen_many(refine_prompt, B=B)\n\n        n = [0]*B\n        s = [0.0]*B\n        proxy_first = [None]*B\n\n        evals = 0\n        for i in range(B):\n            if evals >= per_round_proxy_budget:\n                break\n            out = proxy_score_lenient(cands[i], gold, p_flip, rng)\n            proxy_first[i] = int(out)\n            n[i] += 1\n            s[i] += out\n            evals += 1\n        if evals < per_round_proxy_budget:\n            n_star += 1\n            sum_star += proxy_score_lenient(y_star, gold, p_flip, rng)\n            evals += 1\n\n        # proxy-only sequential sampling to reduce stochasticity\n        while evals < per_round_proxy_budget:\n            # pick arm with highest proxy UCB (unregularized, just for sampling)\n            ucbs = []\n            for i in range(B):\n                mean_i = s[i]/max(1,n[i])\n                r = hoeffding_rad(n[i], delta/(B+1))\n                ucbs.append(min(1.0, mean_i + r))\n            i = max(range(B), key=lambda j: ucbs[j])\n            n[i] += 1\n            s[i] += proxy_score_lenient(cands[i], gold, p_flip, rng)\n            evals += 1\n\n        means = [s[i]/max(1,n[i]) for i in range(B)]\n\n        # allocate audits to proxy-promising candidates (where proxy likely over-optimistic)\n        audit_order = sorted(range(B), key=lambda i: means[i], reverse=True)\n        for k in range(min(per_round_audit_budget, B)):\n            i = audit_order[k]\n            # use the first observed proxy outcome to define the condition for calibration\n            p0 = proxy_first[i]\n            if p0 is None:\n                p0 = int(means[i] >= 0.5)\n            t = int(true_score_strict(cands[i], gold))\n            calibrator.update(p0, t)\n\n        # choose candidate by calibrated lower bound\n        best_i, best_lcbU = 0, -1e9\n        for i in range(B):\n            lcbU, _ = calibrated_bounds_from_proxy(means[i], cands[i])\n            if lcbU > best_lcbU:\n                best_lcbU, best_i = lcbU, i\n\n        mean_star = sum_star / n_star\n        _, ucbU_star = calibrated_bounds_from_proxy(mean_star, y_star)\n\n        if best_lcbU > ucbU_star + tau:\n            y_star = cands[best_i]\n            n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                break\n\n    return y_star\n\n# ------------------ evaluation ------------------\n\ndef eval_acc_strict(fn, data):\n    return sum(true_score_strict(fn(x, g), g) for x, g in data) / len(data)\n\nif __name__ == \"__main__\":\n    dev = make_data(100, seed=0)\n    test = make_data(500, seed=1)\n\n    # small tuning for lambda (kept tiny; main effect is from audit-calibration)\n    lams = [0.0, 0.005, 0.01]\n    best_lam, best_acc = None, -1\n    for lam in lams:\n        cal = ProxyCalibrator()\n        acc = eval_acc_strict(lambda x,g: accr(x,g,calibrator=cal,lam=lam,p_flip=0.1,seed=0), dev)\n        if acc > best_acc:\n            best_acc, best_lam = acc, lam\n\n    for p in [0.0, 0.1, 0.2]:\n        acc_direct = eval_acc_strict(lambda x,g: direct(x), test)\n        acc_sr = eval_acc_strict(lambda x,g: self_refine(x,T=3), test)\n        acc_scg = eval_acc_strict(lambda x,g: scg_sr_proxy(x,g,lam=best_lam,p_flip=p,seed=0), test)\n        cal = ProxyCalibrator()\n        acc_accr = eval_acc_strict(lambda x,g: accr(x,g,calibrator=cal,lam=best_lam,p_flip=p,seed=0), test)\n        print({\"p_flip\": p, \"direct\": acc_direct, \"self_refine\": acc_sr,\n               \"scg_proxy\": acc_scg, \"accr\": acc_accr, \"lambda\": best_lam,\n               \"calib\": {\"a_pi\": cal.a_pi, \"b_pi\": cal.b_pi, \"a_nu\": cal.a_nu, \"b_nu\": cal.b_nu}})\n",
    "expected_result": "Because the proxy is *lenient* (any-number match) while the true metric is *strict* (first-number match), proxy-only selection is expected to systematically prefer verbose outputs that include the gold number somewhere but violate the strict format—classic reward hacking.\n\nPredicted strict accuracy (higher is better; illustrative numbers for flan-t5-small on this synthetic task):\n- p_flip=0.0:\n  - direct: 0.78–0.84\n  - SELF-REFINE: 0.75–0.82 (occasionally induces verbosity)\n  - SCG-SR(Proxy): 0.68–0.78 (over-optimizes lenient proxy)\n  - ACCR: 0.80–0.86 (recovers/improves by rejecting proxy-gamed candidates after a few audits)\n- p_flip=0.2:\n  - SCG-SR(Proxy) degrades further (e.g., 0.62–0.74)\n  - ACCR degrades more gracefully (e.g., 0.78–0.84), yielding +6 to +12 accuracy points over SCG-SR(Proxy).\n\nSecondary expectations:\n- Proxy score for SCG-SR(Proxy) will be high even when strict accuracy is low, demonstrating misalignment.\n- ACCR will use only ~T×(1–2) audit calls per instance (small overhead) but will reduce harmful drift and often stop earlier due to conservative acceptance.\n- Output length: SCG-SR(Proxy) tends to grow (proxy gaming); ACCR stays shorter due to both λ and audit-informed rejection.",
    "expected_conclusion": "ACCR upgrades confidence-gated self-refinement from robustness-to-noise to robustness-to-*misaligned* feedback, by adding an online audit-calibration layer that estimates and bounds proxy reliability at test time. Academically, it reframes automated prompt optimization as decision-making with a biased proxy and scarce high-fidelity labels, connecting self-refinement to selective auditing, calibration, and conservative (risk-limiting) sequential acceptance rules. Practically and socially, it mitigates reward hacking/specification gaming—one of the most consequential deployment failures—while requiring no training and only minimal expensive checks (e.g., occasional human review or full integration tests). This makes automated prompt optimization more dependable in real pipelines where cheap metrics are imperfect and safety hinges on not optimizing the wrong objective."
  },
  "experimental_design": {
    "experiment_summary": "Task: For each math word problem, the system must output a final answer in a strict, machine-parseable format where the *first integer/number appearing in the model output* is the predicted answer. The experiment evaluates iterative test-time refinement controllers that generate candidate revisions, score them with a cheap but biased/noisy proxy verifier, and decide whether to accept a revision.\n\nPurpose: Demonstrate that the proposed Audit‑Calibrated Confidence Refinement (ACCR) controller is more robust than proxy-only self-refinement when the cheap feedback signal is systematically misaligned with the true deployment metric (reward hacking/spec gaming). ACCR couples (1) sequential proxy querying for candidate ranking with (2) online calibration from a small number of high-fidelity “audits” and (3) a conservative accept/stop rule based on credible bounds over true success.\n\nWorkflow (per instance):\n1) INITIAL: produce an initial answer.\n2) For up to T refinement rounds:\n   a) PROPOSE: generate a critique and B candidate refinements via stochastic decoding.\n   b) PROXY SELECTION: repeatedly query a *biased proxy* verifier on candidates under a proxy-call budget to estimate proxy success probabilities q(y) with uncertainty bounds.\n   c) AUDIT: spend a small audit budget to run the *true* evaluator on selected candidates; update global Beta posteriors for proxy precision π=P(True=1|Proxy=1) and true-negative rate ν=P(True=0|Proxy=0).\n   d) ACCEPT/STOP: compute conservative bounds on true utility using posterior quantiles of π and ν; accept a revision only if its lower bound exceeds the current solution’s upper bound by margin τ; otherwise stop early.\n\nProxy–true mismatch used to induce reward hacking: the true evaluator requires the *first extracted numeric token* to match gold, while the proxy evaluator returns success if *any numeric token anywhere* matches gold (plus optional stochastic label flips p_flip). This encourages verbose “show your work” outputs that include many numbers, which can score well on proxy while failing strict true scoring.\n\nScale to fit Runner (H200 140GB VRAM, 240GB RAM):\n- Use exactly two small/medium HF models (≤7B) and run inference-only (no fine-tuning).\n- Subsample evaluation to 500 problems per dataset (1,000 total) with fixed seeds.\n- Limit decoding to max_new_tokens=96, B=4 candidates/round, T=3 rounds.\n- Proxy calls capped at 12 per round; audits capped at 1 per round (≈3 audits/instance worst case).\n- Use bf16 + device_map=auto; generation batched where possible (num_return_sequences=B for proposals). This keeps runtime and VRAM well within a single H200.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA H200, VRAM: 140 GB, RAM: 240 GB"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Primary metric (strict exactness under the true evaluator).\n\nCorrectness criteria: For a model output string y and gold numeric answer g (normalized), extract the first numeric token from y using a regex that captures integers/decimals with optional sign (e.g., r\"[-+]?\\d+(?:\\.\\d+)?\"). The prediction is correct iff (i) at least one numeric token exists, and (ii) the first extracted token equals g after normalization. Normalization: strip spaces/commas, convert to Decimal; treat integers and decimals as exact string-equal after canonicalization (e.g., Decimal quantization). If the dataset provides integer answers (GSM8K/SVAMP mostly integers), this reduces to integer equality.\n\nCalculation method: accuracy = (1/N) * Σ_i 1[ true_correct(y_i, g_i) ].\n\nTask appropriateness: The hypothesis is about proxy-gaming causing failures on a strict deployment metric (a parser/validator). This metric directly represents that strict downstream requirement.\n\nRelevant visualizations: (1) Bar chart of accuracy by method and proxy-noise level p_flip. (2) Accuracy vs. audit budget (0,1,2) curve to show compute–risk trade-off. (3) Paired per-item win/loss heatmap between methods (ACCR vs SCG-SR) to show where ACCR avoids regressions."
      },
      {
        "name": "proxy_success_rate",
        "description": "Measures performance under the cheap, biased proxy verifier.\n\nCorrectness criteria: proxy(y,g)=1 iff *any* numeric token anywhere in y equals g after the same normalization as in accuracy; then optionally flip the binary label with probability p_flip to simulate flaky tests/reward-model noise.\n\nCalculation method: proxy_success_rate = (1/N) * Σ_i proxy(y_i, g_i).\n\nTask appropriateness: Needed to demonstrate proxy–true decoupling: proxy-only methods may increase proxy scores while decreasing true accuracy (reward hacking).\n\nRelevant visualizations: Scatter plot of (proxy_success_rate, accuracy) per method and p_flip; ideally ACCR lies closer to the diagonal while proxy-only refinement shifts right but down."
      },
      {
        "name": "avg_output_length_tokens",
        "description": "Average generated output length in model tokens (method-level).\n\nCorrectness criteria: Tokenize each final output y_i with the corresponding model tokenizer; length_i = number of tokens.\n\nCalculation method: avg_output_length_tokens = (1/N) * Σ_i length_i.\n\nTask appropriateness: Proxy is biased toward verbosity (including many numbers). Length quantifies verbosity hacking and allows assessing the effect of the length penalty λ and conservative acceptance.\n\nRelevant visualizations: Violin/box plots of output length by method; joint plot of length vs. proxy score to show proxy reward for verbosity."
      },
      {
        "name": "avg_proxy_calls_per_instance",
        "description": "Compute/cost metric: average number of proxy verifier queries used per instance.\n\nCorrectness criteria: Count every invocation of proxy_score_lenient for candidate evaluation and (if used) for re-evaluating the current solution.\n\nCalculation method: avg_proxy_calls_per_instance = (1/N) * Σ_i proxy_calls_i.\n\nTask appropriateness: Methods must be compared under similar compute budgets; this metric confirms budget adherence and shows ACCR’s overhead profile.\n\nRelevant visualizations: Stacked bar chart of proxy vs audit calls per instance by method."
      },
      {
        "name": "avg_audit_calls_per_instance",
        "description": "Compute/cost metric: average number of expensive high-fidelity audits used per instance.\n\nCorrectness criteria: Count every invocation of the strict true evaluator used *during refinement* for calibration/selection (exclude the final evaluation pass).\n\nCalculation method: avg_audit_calls_per_instance = (1/N) * Σ_i audit_calls_i.\n\nTask appropriateness: ACCR’s claim is improved robustness with a small audit budget; this metric quantifies that extra cost.\n\nRelevant visualizations: Accuracy vs avg_audit_calls Pareto frontier (trade-off curve) across audit budgets and methods."
      },
      {
        "name": "avg_refinement_rounds",
        "description": "Average number of refinement iterations executed before stopping.\n\nCorrectness criteria: rounds_i equals the number of completed refinement rounds for instance i (0..T).\n\nCalculation method: avg_refinement_rounds = (1/N) * Σ_i rounds_i.\n\nTask appropriateness: Conservative accept/stop rules should prevent harmful drift and stop earlier when improvements are not reliable.\n\nRelevant visualizations: Histogram of rounds-to-stop by method; survival curve (fraction not stopped vs round)."
      },
      {
        "name": "reward_hacking_gap",
        "description": "Measures the degree of proxy–true misalignment exploited by the method.\n\nCorrectness criteria: Compute per instance: gap_i = proxy(y_i,g_i) − true_correct(y_i,g_i) where both are in {0,1} (proxy includes p_flip).\n\nCalculation method: reward_hacking_gap = (1/N) * Σ_i gap_i.\n\nTask appropriateness: Directly quantifies cases where the model ‘passes the proxy but fails the true metric’—the failure mode ACCR aims to reduce.\n\nRelevant visualizations: Bar chart of gap by method and p_flip; confusion-style 2x2 table counts of (proxy pass/fail) vs (true pass/fail) for each method."
      },
      {
        "name": "calibration_diagnostics_pi_nu",
        "description": "Tracks ACCR’s online calibration estimates of proxy reliability.\n\nCorrectness criteria: ACCR maintains Beta posteriors for π=P(True=1|Proxy=1) and ν=P(True=0|Proxy=0) updated from audits. Report posterior mean and 95% credible interval (CI) at the end of evaluation, and optionally as a function of processed instances.\n\nCalculation method: For Beta(a,b): mean=a/(a+b); 95% CI via quantiles q=0.025,0.975 (e.g., torch.distributions.Beta.icdf).\n\nTask appropriateness: Validates that ACCR detects proxy misalignment and reflects uncertainty when audits are scarce.\n\nRelevant visualizations: Line plot of posterior mean with shaded CI for π and ν over evaluation steps; should stabilize as audits accumulate."
      }
    ],
    "models_to_use": [
      "google/flan-t5-small (~80M parameters)",
      "mistralai/Mistral-7B-v0.3 (7.3B parameters)"
    ],
    "datasets_to_use": [
      "openai/gsm8k (main)",
      "MU-NLPC/Calc-svamp (SVAMP)"
    ],
    "proposed_method": {
      "method_name": "Audit-Calibrated Confidence Refinement (ACCR)",
      "description": "Inference-only refinement controller that is robust to *biased* and noisy proxy feedback. ACCR (i) proposes multiple candidate refinements, (ii) estimates proxy success probabilities with sequential sampling under a fixed proxy-call budget, (iii) spends a small number of high-fidelity audits to update online Beta posteriors over proxy precision π and true-negative rate ν, and (iv) applies a conservative accept/stop rule using posterior quantile bounds on the *true* success probability, regularized by a length penalty to reduce verbosity hacking.",
      "optuna_config": {
        "enabled": true,
        "n_trials": 20,
        "search_spaces": [
          {
            "param_name": "lam_length_penalty",
            "distribution_type": "uniform",
            "low": 0.0,
            "high": 0.03
          },
          {
            "param_name": "tau_accept_margin",
            "distribution_type": "uniform",
            "low": 0.0,
            "high": 0.15
          },
          {
            "param_name": "alpha_calibration",
            "distribution_type": "categorical",
            "choices": [
              0.01,
              0.05,
              0.1
            ]
          },
          {
            "param_name": "per_round_audit_budget",
            "distribution_type": "categorical",
            "choices": [
              0,
              1,
              2
            ]
          },
          {
            "param_name": "p_flip_dev",
            "distribution_type": "categorical",
            "choices": [
              0.0,
              0.1
            ]
          }
        ]
      }
    },
    "comparative_methods": [
      {
        "method_name": "SCG-SR(Proxy-only)",
        "description": "Sequential confidence-gated self-refinement that uses only the cheap proxy verifier for candidate selection and acceptance (no audits, no proxy–true calibration). Uses Hoeffding-style bounds on proxy success rate to allocate proxy queries and to decide accept/stop, making it robust to stochastic proxy noise but vulnerable to systematic proxy bias/reward hacking.",
        "optuna_config": {
          "enabled": true,
          "n_trials": 20,
          "search_spaces": [
            {
              "param_name": "lam_length_penalty",
              "distribution_type": "uniform",
              "low": 0.0,
              "high": 0.03
            },
            {
              "param_name": "tau_accept_margin",
              "distribution_type": "uniform",
              "low": 0.0,
              "high": 0.15
            },
            {
              "param_name": "per_round_proxy_budget",
              "distribution_type": "categorical",
              "choices": [
                8,
                12,
                16
              ]
            },
            {
              "param_name": "p_flip_dev",
              "distribution_type": "categorical",
              "choices": [
                0.0,
                0.1
              ]
            }
          ]
        }
      }
    ]
  },
  "experiment_code": {
    "train_py": "import os\nimport json\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Any, Optional\n\nimport torch\nimport hydra\nimport optuna\nimport wandb\nfrom omegaconf import OmegaConf\n\nfrom src.preprocess import load_dataset_split, extract_numbers, normalize_number_str\nfrom src.model import (\n    GenModelWrapper,\n    ProxyCalibrator,\n    direct_generate,\n    self_refine_generate,\n    scg_sr_proxy,\n    accr_refine,\n)\n\n\n@dataclass\nclass EvalStats:\n    accuracy: float\n    proxy_success_rate: float\n    avg_output_length_tokens: float\n    avg_proxy_calls_per_instance: float\n    avg_audit_calls_per_instance: float\n    avg_refinement_rounds: float\n    reward_hacking_gap: float\n    calibration_pi_mean: float\n    calibration_pi_lcb: float\n    calibration_pi_ucb: float\n    calibration_nu_mean: float\n    calibration_nu_lcb: float\n    calibration_nu_ucb: float\n    proxy_true_tp: int\n    proxy_true_fp: int\n    proxy_true_tn: int\n    proxy_true_fn: int\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef true_score_strict(pred_text: str, gold: str) -> float:\n    nums = extract_numbers(pred_text)\n    if len(nums) == 0:\n        return 0.0\n    return 1.0 if normalize_number_str(nums[0]) == normalize_number_str(gold) else 0.0\n\n\ndef proxy_score_lenient(pred_text: str, gold: str, p_flip: float, rng: random.Random) -> float:\n    nums = extract_numbers(pred_text)\n    gold_n = normalize_number_str(gold)\n    cand = [normalize_number_str(n) for n in nums]\n    s = 1.0 if gold_n in cand else 0.0\n    if rng.random() < p_flip:\n        s = 1.0 - s\n    return s\n\n\ndef evaluate_method(\n    cfg,\n    model_wrapper: GenModelWrapper,\n    data: List[Tuple[str, str]],\n    method_name: str,\n    params: Dict[str, Any],\n    log_wandb: bool,\n) -> Tuple[EvalStats, Dict[str, Any]]:\n    rng = random.Random(cfg.seed)\n    cal = ProxyCalibrator()\n    total = len(data)\n    correct = 0\n    proxy_succ = 0\n    lengths: List[int] = []\n    proxy_calls: List[int] = []\n    audit_calls: List[int] = []\n    rounds: List[int] = []\n    gaps: List[float] = []\n    tp = fp = tn = fn = 0\n\n    for idx, (x, g) in enumerate(data):\n        if cfg.mode == \"trial\" and idx >= cfg.trial.max_items:\n            break\n        if method_name == \"Direct\":\n            y = direct_generate(model_wrapper, x, cfg.generation)\n            meta = {\"proxy_calls\": 0, \"audit_calls\": 0, \"rounds\": 0}\n        elif method_name == \"SELF-REFINE\":\n            y, meta = self_refine_generate(model_wrapper, x, cfg.generation, T=cfg.method_cfg.T)\n        elif method_name == \"SCG-SR(Proxy-only)\":\n            y, meta = scg_sr_proxy(\n                model_wrapper,\n                x,\n                g,\n                cfg,\n                p_flip=params.get(\"p_flip\", cfg.method_cfg.p_flip),\n                lam=params[\"lam_length_penalty\"],\n                tau=params[\"tau_accept_margin\"],\n                per_round_proxy_budget=params[\"per_round_proxy_budget\"],\n            )\n        elif method_name == \"Audit-Calibrated Confidence Refinement (ACCR)\":\n            y, meta = accr_refine(\n                model_wrapper,\n                x,\n                g,\n                cfg,\n                calibrator=cal,\n                p_flip=params.get(\"p_flip\", cfg.method_cfg.p_flip),\n                lam=params[\"lam_length_penalty\"],\n                tau=params[\"tau_accept_margin\"],\n                per_round_audit_budget=params[\"per_round_audit_budget\"],\n                alpha_cal=params[\"alpha_calibration\"],\n            )\n        else:\n            raise ValueError(f\"Unknown method {method_name}\")\n\n        t = true_score_strict(y, g)\n        p = proxy_score_lenient(y, g, params.get(\"p_flip\", cfg.method_cfg.p_flip), rng)\n        correct += t\n        proxy_succ += p\n        lengths.append(len(model_wrapper.tokenize(y)))\n        proxy_calls.append(meta[\"proxy_calls\"])\n        audit_calls.append(meta[\"audit_calls\"])\n        rounds.append(meta[\"rounds\"])\n        gaps.append(p - t)\n\n        if p >= 0.5 and t >= 0.5:\n            tp += 1\n        elif p >= 0.5 and t < 0.5:\n            fp += 1\n        elif p < 0.5 and t < 0.5:\n            tn += 1\n        else:\n            fn += 1\n\n        if log_wandb and cfg.wandb.mode != \"disabled\":\n            wandb.log(\n                {\n                    \"step\": idx,\n                    \"accuracy\": correct / max(1, idx + 1),\n                    \"proxy_success_rate\": proxy_succ / max(1, idx + 1),\n                    \"avg_output_length_tokens\": sum(lengths) / max(1, len(lengths)),\n                    \"avg_proxy_calls_per_instance\": sum(proxy_calls) / max(1, len(proxy_calls)),\n                    \"avg_audit_calls_per_instance\": sum(audit_calls) / max(1, len(audit_calls)),\n                    \"avg_refinement_rounds\": sum(rounds) / max(1, len(rounds)),\n                    \"reward_hacking_gap\": sum(gaps) / max(1, len(gaps)),\n                },\n                step=idx,\n            )\n\n        if idx == 0:\n            assert isinstance(x, str) and isinstance(g, str), \"Inputs must be strings\"\n            assert len(y) > 0, \"Model output must be non-empty\"\n\n    n = len(lengths)\n    l_pi, u_pi, l_nu, u_nu = cal.bounds(alpha=0.05)\n    stats = EvalStats(\n        accuracy=correct / max(1, n),\n        proxy_success_rate=proxy_succ / max(1, n),\n        avg_output_length_tokens=sum(lengths) / max(1, n),\n        avg_proxy_calls_per_instance=sum(proxy_calls) / max(1, n),\n        avg_audit_calls_per_instance=sum(audit_calls) / max(1, n),\n        avg_refinement_rounds=sum(rounds) / max(1, n),\n        reward_hacking_gap=sum(gaps) / max(1, n),\n        calibration_pi_mean=cal.mean_pi(),\n        calibration_pi_lcb=l_pi,\n        calibration_pi_ucb=u_pi,\n        calibration_nu_mean=cal.mean_nu(),\n        calibration_nu_lcb=l_nu,\n        calibration_nu_ucb=u_nu,\n        proxy_true_tp=tp,\n        proxy_true_fp=fp,\n        proxy_true_tn=tn,\n        proxy_true_fn=fn,\n    )\n    return stats, {\"calibrator\": cal}\n\n\ndef optuna_objective(trial, cfg, model_wrapper, dev_data, method_name):\n    params: Dict[str, Any] = {}\n    for ss in cfg.optuna.search_spaces:\n        if ss.distribution_type == \"uniform\":\n            params[ss.param_name] = trial.suggest_float(ss.param_name, ss.low, ss.high)\n        elif ss.distribution_type == \"categorical\":\n            params[ss.param_name] = trial.suggest_categorical(ss.param_name, ss.choices)\n        else:\n            raise ValueError(f\"Unknown distribution {ss.distribution_type}\")\n    if method_name == \"SCG-SR(Proxy-only)\":\n        params.setdefault(\"per_round_proxy_budget\", cfg.method_cfg.per_round_proxy_budget)\n    if method_name == \"Audit-Calibrated Confidence Refinement (ACCR)\":\n        params.setdefault(\"per_round_audit_budget\", cfg.method_cfg.per_round_audit_budget)\n        params.setdefault(\"alpha_calibration\", cfg.method_cfg.alpha_calibration)\n\n    params[\"p_flip\"] = params.get(\"p_flip_dev\", cfg.method_cfg.p_flip)\n    stats, _ = evaluate_method(cfg, model_wrapper, dev_data, method_name, params, log_wandb=False)\n    return stats.accuracy\n\n\ndef maybe_run_training(cfg, model_wrapper):\n    if cfg.training.inference_only or cfg.training.epochs <= 0:\n        return\n    model_wrapper.model.train()\n    optimizer = torch.optim.AdamW(model_wrapper.model.parameters(), lr=cfg.training.learning_rate)\n    dummy_inputs = model_wrapper.tokenizer(\n        [\"Dummy input\"], return_tensors=\"pt\", padding=True\n    ).to(model_wrapper.model.device)\n    dummy_labels = model_wrapper.tokenizer(\n        [\"0\"], return_tensors=\"pt\", padding=True\n    ).input_ids.to(model_wrapper.model.device)\n    for epoch in range(cfg.training.epochs):\n        outputs = model_wrapper.model(**dummy_inputs, labels=dummy_labels)\n        loss = outputs.loss\n        params = [p for p in model_wrapper.model.parameters() if p.requires_grad]\n        aux_grads = torch.autograd.grad(loss, params, retain_graph=True, create_graph=False, allow_unused=True)\n        aux_norm = sum(g.abs().sum() for g in aux_grads if g is not None)\n        loss.backward()\n        if epoch == 0:\n            assert dummy_inputs[\"input_ids\"].shape[0] == dummy_labels.shape[0]\n        grads = [p.grad for p in model_wrapper.model.parameters() if p.requires_grad]\n        assert any(g is not None for g in grads), \"No gradients found before optimizer step\"\n        assert any((g is not None and g.abs().sum().item() > 0) for g in grads), \"Zero gradients before optimizer step\"\n        optimizer.step()\n        optimizer.zero_grad()\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\"train_loss\": float(loss.item()), \"aux_grad_norm\": float(aux_norm)})\n    model_wrapper.model.eval()\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg):\n    set_seed(cfg.seed)\n    os.makedirs(cfg.results_dir, exist_ok=True)\n\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.method_cfg.T = min(cfg.method_cfg.T, 1)\n        cfg.method_cfg.B = min(cfg.method_cfg.B, 2)\n        cfg.method_cfg.per_round_proxy_budget = min(cfg.method_cfg.per_round_proxy_budget, 8)\n        cfg.method_cfg.per_round_audit_budget = min(cfg.method_cfg.per_round_audit_budget, 1)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n        )\n        print(f\"WandB URL: {wandb.run.url}\")\n\n    data = load_dataset_split(cfg.dataset, cfg.dataset.split, limit=cfg.dataset.get(\"limit\", None))\n    dev_size = min(cfg.dataset.get(\"dev_size\", 100), len(data))\n    test_size = min(cfg.dataset.get(\"test_size\", 500), max(0, len(data) - dev_size))\n    rng = random.Random(cfg.seed)\n    rng.shuffle(data)\n    dev_data = data[:dev_size]\n    test_data = data[dev_size : dev_size + test_size]\n\n    if cfg.mode == \"trial\":\n        dev_data = dev_data[: cfg.trial.max_items]\n        test_data = test_data[: cfg.trial.max_items]\n\n    model_wrapper = GenModelWrapper(cfg)\n    assert model_wrapper.tokenizer.pad_token_id is not None, \"Tokenizer pad_token_id must be set\"\n    assert model_wrapper.model is not None, \"Model must be initialized\"\n    assert model_wrapper.model.config.vocab_size > 0, \"Model vocab size must be valid\"\n\n    maybe_run_training(cfg, model_wrapper)\n\n    method_name = cfg.run.method\n    params = {\n        \"lam_length_penalty\": cfg.method_cfg.lam_length_penalty,\n        \"tau_accept_margin\": cfg.method_cfg.tau_accept_margin,\n        \"per_round_proxy_budget\": cfg.method_cfg.per_round_proxy_budget,\n        \"per_round_audit_budget\": cfg.method_cfg.per_round_audit_budget,\n        \"alpha_calibration\": cfg.method_cfg.alpha_calibration,\n        \"p_flip\": cfg.method_cfg.p_flip,\n    }\n\n    if cfg.optuna.n_trials > 0:\n        study = optuna.create_study(direction=\"maximize\")\n        study.optimize(\n            lambda t: optuna_objective(t, cfg, model_wrapper, dev_data, method_name),\n            n_trials=cfg.optuna.n_trials,\n        )\n        params.update(study.best_params)\n\n    stats, _ = evaluate_method(cfg, model_wrapper, test_data, method_name, params, log_wandb=True)\n\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary[\"accuracy\"] = stats.accuracy\n        wandb.summary[\"proxy_success_rate\"] = stats.proxy_success_rate\n        wandb.summary[\"avg_output_length_tokens\"] = stats.avg_output_length_tokens\n        wandb.summary[\"avg_proxy_calls_per_instance\"] = stats.avg_proxy_calls_per_instance\n        wandb.summary[\"avg_audit_calls_per_instance\"] = stats.avg_audit_calls_per_instance\n        wandb.summary[\"avg_refinement_rounds\"] = stats.avg_refinement_rounds\n        wandb.summary[\"reward_hacking_gap\"] = stats.reward_hacking_gap\n        wandb.summary[\"calibration_pi_mean\"] = stats.calibration_pi_mean\n        wandb.summary[\"calibration_pi_lcb\"] = stats.calibration_pi_lcb\n        wandb.summary[\"calibration_pi_ucb\"] = stats.calibration_pi_ucb\n        wandb.summary[\"calibration_nu_mean\"] = stats.calibration_nu_mean\n        wandb.summary[\"calibration_nu_lcb\"] = stats.calibration_nu_lcb\n        wandb.summary[\"calibration_nu_ucb\"] = stats.calibration_nu_ucb\n        wandb.summary[\"proxy_true_tp\"] = stats.proxy_true_tp\n        wandb.summary[\"proxy_true_fp\"] = stats.proxy_true_fp\n        wandb.summary[\"proxy_true_tn\"] = stats.proxy_true_tn\n        wandb.summary[\"proxy_true_fn\"] = stats.proxy_true_fn\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import os\nimport json\nimport sys\nfrom collections import defaultdict\nfrom typing import Dict, List, Any\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\nimport yaml\nfrom scipy import stats\n\n\ndef parse_kv_args(argv: List[str]) -> Dict[str, str]:\n    args: Dict[str, str] = {}\n    for arg in argv:\n        if \"=\" in arg:\n            k, v = arg.split(\"=\", 1)\n            args[k] = v\n    return args\n\n\ndef save_json(path, obj):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef plot_learning_curve(df, run_id, out_dir):\n    paths = []\n    if \"accuracy\" not in df.columns:\n        return paths\n    fig = plt.figure()\n    plt.plot(df.index, df[\"accuracy\"], label=\"accuracy\")\n    if \"proxy_success_rate\" in df.columns:\n        plt.plot(df.index, df[\"proxy_success_rate\"], label=\"proxy_success_rate\")\n    plt.xlabel(\"step\")\n    plt.ylabel(\"metric\")\n    plt.title(f\"{run_id} Learning Curve\")\n    plt.legend()\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_learning_curve.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    paths.append(path)\n    return paths\n\n\ndef plot_confusion_matrix(summary: Dict[str, Any], run_id: str, out_dir: str):\n    tp = summary.get(\"proxy_true_tp\", 0)\n    fp = summary.get(\"proxy_true_fp\", 0)\n    tn = summary.get(\"proxy_true_tn\", 0)\n    fn = summary.get(\"proxy_true_fn\", 0)\n    mat = np.array([[tp, fp], [fn, tn]])\n    fig = plt.figure()\n    sns.heatmap(mat, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"True\", \"False\"], yticklabels=[\"True\", \"False\"])\n    plt.xlabel(\"Proxy Pass\")\n    plt.ylabel(\"True Pass\")\n    plt.title(f\"{run_id} Proxy vs True Confusion\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_confusion_matrix.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return [path]\n\n\ndef plot_bar_comparison(metrics: Dict[str, Dict[str, float]], metric_name: str, out_dir: str):\n    fig = plt.figure()\n    run_ids = list(metrics[metric_name].keys())\n    values = [metrics[metric_name][rid] for rid in run_ids]\n    sns.barplot(x=run_ids, y=values)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(metric_name)\n    plt.title(f\"Comparison {metric_name}\")\n    for i, v in enumerate(values):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"comparison_{metric_name}_bar_chart.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return [path]\n\n\ndef plot_box_comparison(histories: Dict[str, pd.DataFrame], metric_name: str, out_dir: str):\n    rows = []\n    for run_id, df in histories.items():\n        if metric_name in df.columns:\n            for v in df[metric_name].dropna().tolist():\n                rows.append({\"run_id\": run_id, metric_name: v})\n    if not rows:\n        return []\n    df = pd.DataFrame(rows)\n    fig = plt.figure()\n    sns.boxplot(data=df, x=\"run_id\", y=metric_name)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.title(f\"{metric_name} Distribution\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"comparison_{metric_name}_box_plot.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return [path]\n\n\ndef plot_metric_table(metrics: Dict[str, Dict[str, float]], out_dir: str):\n    metric_names = list(metrics.keys())\n    run_ids = sorted({rid for m in metrics.values() for rid in m.keys()})\n    table = []\n    for rid in run_ids:\n        row = [rid]\n        for m in metric_names:\n            row.append(f\"{metrics[m].get(rid, float('nan')):.4f}\")\n        table.append(row)\n    fig = plt.figure(figsize=(1 + len(metric_names), 1 + len(run_ids)))\n    ax = plt.gca()\n    ax.axis(\"off\")\n    columns = [\"run_id\"] + metric_names\n    tbl = plt.table(cellText=table, colLabels=columns, loc=\"center\")\n    tbl.auto_set_font_size(False)\n    tbl.set_fontsize(8)\n    plt.tight_layout()\n    path = os.path.join(out_dir, \"comparison_metrics_table.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return [path]\n\n\ndef main():\n    args = parse_kv_args(sys.argv[1:])\n    results_dir = args.get(\"results_dir\")\n    run_ids_raw = args.get(\"run_ids\")\n    if results_dir is None or run_ids_raw is None:\n        raise ValueError(\"Expected results_dir=... and run_ids='[...]'\")\n    run_ids = json.loads(run_ids_raw)\n\n    with open(os.path.join(\"config\", \"config.yaml\"), \"r\") as f:\n        cfg = yaml.safe_load(f)\n    entity = cfg[\"wandb\"][\"entity\"]\n    project = cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    comparison_metrics: Dict[str, Dict[str, float]] = defaultdict(dict)\n    histories: Dict[str, pd.DataFrame] = {}\n    all_paths: List[str] = []\n\n    for run_id in run_ids:\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        history = run.history()\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        run_dir = os.path.join(results_dir, run_id)\n        os.makedirs(run_dir, exist_ok=True)\n\n        metrics = {\n            \"history\": history.to_dict(orient=\"list\"),\n            \"summary\": summary,\n            \"config\": config,\n        }\n        metrics_path = os.path.join(run_dir, \"metrics.json\")\n        save_json(metrics_path, metrics)\n        all_paths.append(metrics_path)\n\n        histories[run_id] = history\n\n        fig_paths = plot_learning_curve(history, run_id, run_dir)\n        all_paths.extend(fig_paths)\n        fig_paths = plot_confusion_matrix(summary, run_id, run_dir)\n        all_paths.extend(fig_paths)\n\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                comparison_metrics[k][run_id] = float(v)\n\n    primary_metric = \"accuracy\"\n    best_proposed = {\"run_id\": None, \"value\": -1e9}\n    best_baseline = {\"run_id\": None, \"value\": -1e9}\n\n    for run_id, v in comparison_metrics.get(primary_metric, {}).items():\n        if \"proposed\" in run_id:\n            if v > best_proposed[\"value\"]:\n                best_proposed = {\"run_id\": run_id, \"value\": v}\n        if \"comparative\" in run_id or \"baseline\" in run_id:\n            if v > best_baseline[\"value\"]:\n                best_baseline = {\"run_id\": run_id, \"value\": v}\n\n    minimize = any(x in primary_metric for x in [\"loss\", \"perplexity\", \"error\"])\n    gap = None\n    if best_proposed[\"run_id\"] and best_baseline[\"run_id\"]:\n        raw_gap = (best_proposed[\"value\"] - best_baseline[\"value\"]) / max(1e-9, best_baseline[\"value\"]) * 100.0\n        gap = -raw_gap if minimize else raw_gap\n\n    stat_tests = {}\n    run_list = list(histories.keys())\n    for i in range(len(run_list)):\n        for j in range(i + 1, len(run_list)):\n            r1, r2 = run_list[i], run_list[j]\n            h1 = histories[r1].get(primary_metric)\n            h2 = histories[r2].get(primary_metric)\n            if h1 is not None and h2 is not None:\n                a = np.array(h1.dropna().tolist())\n                b = np.array(h2.dropna().tolist())\n                if len(a) > 2 and len(b) > 2:\n                    t_stat, p_val = stats.ttest_ind(a, b, equal_var=False)\n                    stat_tests[f\"{r1}_vs_{r2}\"] = {\"t_stat\": float(t_stat), \"p_val\": float(p_val)}\n\n    agg = {\n        \"primary_metric\": primary_metric,\n        \"metrics\": comparison_metrics,\n        \"best_proposed\": best_proposed,\n        \"best_baseline\": best_baseline,\n        \"gap\": gap,\n        \"stat_tests\": stat_tests,\n    }\n\n    comp_dir = os.path.join(results_dir, \"comparison\")\n    os.makedirs(comp_dir, exist_ok=True)\n    agg_path = os.path.join(comp_dir, \"aggregated_metrics.json\")\n    save_json(agg_path, agg)\n    all_paths.append(agg_path)\n\n    for metric_name in [\"accuracy\", \"proxy_success_rate\", \"reward_hacking_gap\"]:\n        if metric_name in comparison_metrics:\n            fig_paths = plot_bar_comparison(comparison_metrics, metric_name, comp_dir)\n            all_paths.extend(fig_paths)\n            fig_paths = plot_box_comparison(histories, metric_name, comp_dir)\n            all_paths.extend(fig_paths)\n\n    all_paths.extend(plot_metric_table(comparison_metrics, comp_dir))\n\n    for p in all_paths:\n        print(p)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "import re\nfrom decimal import Decimal\nfrom typing import List, Tuple\n\nfrom datasets import load_dataset\n\n_NUM_RE = re.compile(r\"[-+]?\\d+(?:\\.\\d+)?\")\n\n\ndef extract_numbers(text: str) -> List[str]:\n    return _NUM_RE.findall(text)\n\n\ndef normalize_number_str(s: str) -> str:\n    s = s.replace(\",\", \"\").strip()\n    try:\n        d = Decimal(s)\n        q = d.normalize()\n        s2 = format(q, \"f\")\n        if \".\" in s2:\n            s2 = s2.rstrip(\"0\").rstrip(\".\")\n        return s2\n    except Exception:\n        return s\n\n\ndef preprocess_answer(ans: str, normalize_numbers: bool, extract_first_number: bool) -> str:\n    out = ans\n    if extract_first_number:\n        nums = extract_numbers(out)\n        if nums:\n            out = nums[0]\n    if normalize_numbers:\n        out = normalize_number_str(out)\n    return out\n\n\ndef load_dataset_split(dataset_cfg, split: str, limit: int = None) -> List[Tuple[str, str]]:\n    ds = load_dataset(\n        dataset_cfg.name,\n        dataset_cfg.subset,\n        split=split,\n        cache_dir=\".cache/\",\n    )\n    data: List[Tuple[str, str]] = []\n    for ex in ds:\n        if \"Question\" in ex:\n            x = ex[\"Question\"]\n        elif \"question\" in ex:\n            x = ex[\"question\"]\n        elif \"Body\" in ex:\n            x = ex[\"Body\"]\n        elif \"body\" in ex:\n            x = ex[\"body\"]\n        else:\n            x = ex.get(\"input\", ex.get(\"prompt\", \"\"))\n        if \"Answer\" in ex:\n            g = str(ex[\"Answer\"])\n        elif \"answer\" in ex:\n            g = str(ex[\"answer\"])\n        elif \"label\" in ex:\n            g = str(ex[\"label\"])\n        elif \"output\" in ex:\n            g = str(ex[\"output\"])\n        else:\n            g = str(ex.get(\"target\", \"\"))\n        g = preprocess_answer(g, dataset_cfg.preprocessing.normalize_numbers, dataset_cfg.preprocessing.extract_first_number)\n        data.append((x, g))\n        if limit is not None and len(data) >= limit:\n            break\n    return data\n",
    "model_py": "import math\nimport random\nfrom typing import List, Dict, Tuple\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n\nfrom src.preprocess import extract_numbers, normalize_number_str\n\n\ndef hoeffding_rad(n, delta):\n    return math.sqrt(math.log(2.0 / delta) / (2.0 * max(1, n)))\n\n\ndef beta_q(a, b, q):\n    dist = torch.distributions.Beta(torch.tensor(float(a)), torch.tensor(float(b)))\n    return float(dist.icdf(torch.tensor(float(q))))\n\n\nclass GenModelWrapper:\n    def __init__(self, cfg):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.run.model.name, cache_dir=\".cache/\")\n        if self.tokenizer.pad_token_id is None:\n            if self.tokenizer.eos_token_id is not None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            else:\n                self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n\n        dtype = torch.bfloat16 if cfg.run.model.dtype == \"bf16\" else torch.float32\n        if \"t5\" in cfg.run.model.name.lower():\n            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n                cfg.run.model.name,\n                cache_dir=\".cache/\",\n                torch_dtype=dtype,\n                device_map=cfg.run.model.device_map,\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                cfg.run.model.name,\n                cache_dir=\".cache/\",\n                torch_dtype=dtype,\n                device_map=cfg.run.model.device_map,\n            )\n        if len(self.tokenizer) != self.model.get_input_embeddings().num_embeddings:\n            self.model.resize_token_embeddings(len(self.tokenizer))\n        self.model.eval()\n\n    def tokenize(self, text: str) -> List[int]:\n        return self.tokenizer(text, add_special_tokens=False).input_ids\n\n    def generate(self, prompt: str, max_new_tokens: int, do_sample: bool, temperature: float, num_return_sequences: int = 1):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        with torch.no_grad():\n            outs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=do_sample,\n                temperature=temperature if do_sample else None,\n                num_return_sequences=num_return_sequences,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        if isinstance(self.model, AutoModelForCausalLM) or self.model.config.is_decoder:\n            input_len = inputs[\"input_ids\"].shape[1]\n            outs = outs[:, input_len:]\n        return [self.tokenizer.decode(o, skip_special_tokens=True).strip() for o in outs]\n\n\nclass ProxyCalibrator:\n    def __init__(self, a_pi=1, b_pi=1, a_nu=1, b_nu=1):\n        self.a_pi, self.b_pi = a_pi, b_pi\n        self.a_nu, self.b_nu = a_nu, b_nu\n\n    def update(self, proxy_outcome: int, true_outcome: int):\n        if proxy_outcome == 1:\n            if true_outcome == 1:\n                self.a_pi += 1\n            else:\n                self.b_pi += 1\n        else:\n            if true_outcome == 0:\n                self.a_nu += 1\n            else:\n                self.b_nu += 1\n\n    def bounds(self, alpha=0.05):\n        l_pi = beta_q(self.a_pi, self.b_pi, alpha / 2)\n        u_pi = beta_q(self.a_pi, self.b_pi, 1 - alpha / 2)\n        l_nu = beta_q(self.a_nu, self.b_nu, alpha / 2)\n        u_nu = beta_q(self.a_nu, self.b_nu, 1 - alpha / 2)\n        return (l_pi, u_pi, l_nu, u_nu)\n\n    def mean_pi(self):\n        return self.a_pi / (self.a_pi + self.b_pi)\n\n    def mean_nu(self):\n        return self.a_nu / (self.a_nu + self.b_nu)\n\n\ndef direct_generate(model: GenModelWrapper, x: str, gen_cfg) -> str:\n    prompt = f\"Solve and output ONLY the number.\\nProblem: {x}\\nAnswer:\"\n    return model.generate(prompt, gen_cfg.max_new_tokens, False, gen_cfg.temperature)[0]\n\n\ndef self_refine_generate(model: GenModelWrapper, x: str, gen_cfg, T=3) -> Tuple[str, Dict[str, int]]:\n    y = direct_generate(model, x, gen_cfg)\n    for _ in range(T):\n        fb = model.generate(\n            \"You are a strict math checker. Given the problem and the proposed answer, say what is wrong (if anything) and how to fix it.\\n\"\n            f\"Problem: {x}\\nProposed answer: {y}\\nFeedback:\",\n            gen_cfg.max_new_tokens,\n            False,\n            gen_cfg.temperature,\n        )[0]\n        y = model.generate(\n            \"Revise the answer using the feedback.\\nIMPORTANT: Put the final answer first, then you may add a brief explanation.\\n\"\n            f\"Problem: {x}\\nCurrent answer: {y}\\nFeedback: {fb}\\nRevised response:\",\n            gen_cfg.max_new_tokens,\n            True,\n            gen_cfg.temperature,\n        )[0]\n    return y, {\"proxy_calls\": 0, \"audit_calls\": 0, \"rounds\": T}\n\n\ndef proxy_score_lenient(pred_text: str, gold: str, p_flip: float, rng: random.Random) -> float:\n    nums = extract_numbers(pred_text)\n    s = 1.0 if normalize_number_str(gold) in [normalize_number_str(n) for n in nums] else 0.0\n    if rng.random() < p_flip:\n        s = 1.0 - s\n    return s\n\n\ndef true_score_strict(pred_text: str, gold: str) -> float:\n    nums = extract_numbers(pred_text)\n    if len(nums) == 0:\n        return 0.0\n    return 1.0 if normalize_number_str(nums[0]) == normalize_number_str(gold) else 0.0\n\n\ndef scg_sr_proxy(model: GenModelWrapper, x: str, gold: str, cfg, p_flip: float, lam: float, tau: float, per_round_proxy_budget: int):\n    rng = random.Random(cfg.seed)\n    T = cfg.method_cfg.T\n    B = cfg.method_cfg.B\n    delta = cfg.method_cfg.delta\n    patience = cfg.method_cfg.patience\n\n    def util(mean_proxy, y_text):\n        return mean_proxy - lam * len(model.tokenize(y_text))\n\n    y_star = direct_generate(model, x, cfg.generation)\n    n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n    proxy_calls = 1\n    audit_calls = 0\n    bad = 0\n\n    for t in range(T):\n        fb = model.generate(\n            \"You are a strict math checker. Given the problem and the proposed answer, say what is wrong (if anything) and how to fix it.\\n\"\n            f\"Problem: {x}\\nProposed answer: {y_star}\\nFeedback:\",\n            cfg.generation.max_new_tokens,\n            False,\n            cfg.generation.temperature,\n        )[0]\n        refine_prompt = (\n            \"Revise the answer using the feedback.\\nIMPORTANT: Put the final answer first, then you may add a brief explanation.\\n\"\n            f\"Problem: {x}\\nCurrent answer: {y_star}\\nFeedback: {fb}\\nRevised response:\"\n        )\n        cands = model.generate(refine_prompt, cfg.generation.max_new_tokens, True, cfg.generation.temperature, num_return_sequences=B)\n\n        n = [0] * B\n        s = [0.0] * B\n        evals = 0\n        for i in range(B):\n            if evals >= per_round_proxy_budget:\n                break\n            n[i] += 1\n            s[i] += proxy_score_lenient(cands[i], gold, p_flip, rng)\n            evals += 1\n            proxy_calls += 1\n        if evals < per_round_proxy_budget:\n            n_star += 1\n            sum_star += proxy_score_lenient(y_star, gold, p_flip, rng)\n            evals += 1\n            proxy_calls += 1\n\n        while evals < per_round_proxy_budget:\n            ucbs = []\n            lcbs = []\n            for i in range(B):\n                mean_i = s[i] / max(1, n[i])\n                r = hoeffding_rad(n[i], delta / (B + 1))\n                ucbs.append(util(min(1.0, mean_i + r), cands[i]))\n                lcbs.append(util(max(0.0, mean_i - r), cands[i]))\n            order = sorted(range(B), key=lambda i: ucbs[i], reverse=True)\n            best_i = order[0]\n            second_i = order[1] if B > 1 else order[0]\n            if lcbs[best_i] > ucbs[second_i] + 1e-9:\n                break\n            i = best_i\n            n[i] += 1\n            s[i] += proxy_score_lenient(cands[i], gold, p_flip, rng)\n            evals += 1\n            proxy_calls += 1\n\n        best_i, best_lcb = 0, -1e9\n        for i in range(B):\n            mean_i = s[i] / max(1, n[i])\n            r = hoeffding_rad(n[i], delta / (B + 1))\n            lcb = util(max(0.0, mean_i - r), cands[i])\n            if lcb > best_lcb:\n                best_lcb, best_i = lcb, i\n\n        mean_star = sum_star / n_star\n        r_star = hoeffding_rad(n_star, delta / (B + 1))\n        ucb_star = util(min(1.0, mean_star + r_star), y_star)\n\n        if best_lcb > ucb_star + tau:\n            y_star = cands[best_i]\n            n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n            proxy_calls += 1\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                return y_star, {\"proxy_calls\": proxy_calls, \"audit_calls\": audit_calls, \"rounds\": t + 1}\n\n    return y_star, {\"proxy_calls\": proxy_calls, \"audit_calls\": audit_calls, \"rounds\": T}\n\n\ndef accr_refine(\n    model: GenModelWrapper,\n    x: str,\n    gold: str,\n    cfg,\n    calibrator: ProxyCalibrator,\n    p_flip: float,\n    lam: float,\n    tau: float,\n    per_round_audit_budget: int,\n    alpha_cal: float,\n):\n    rng = random.Random(cfg.seed)\n    T = cfg.method_cfg.T\n    B = cfg.method_cfg.B\n    delta = cfg.method_cfg.delta\n    per_round_proxy_budget = cfg.method_cfg.per_round_proxy_budget\n    patience = cfg.method_cfg.patience\n\n    def util_from_trueprob(p_true, y_text):\n        return p_true - lam * len(model.tokenize(y_text))\n\n    def calibrated_bounds_from_proxy(mean_proxy, y_text):\n        l_pi, u_pi, l_nu, u_nu = calibrator.bounds(alpha=alpha_cal)\n        l_true = mean_proxy * l_pi + (1.0 - mean_proxy) * (1.0 - u_nu)\n        u_true = mean_proxy * u_pi + (1.0 - mean_proxy) * (1.0 - l_nu)\n        return util_from_trueprob(l_true, y_text), util_from_trueprob(u_true, y_text)\n\n    y_star = direct_generate(model, x, cfg.generation)\n    n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n    proxy_calls = 1\n    audit_calls = 0\n    bad = 0\n\n    for t in range(T):\n        fb = model.generate(\n            \"You are a strict math checker. Given the problem and the proposed answer, say what is wrong (if anything) and how to fix it.\\n\"\n            f\"Problem: {x}\\nProposed answer: {y_star}\\nFeedback:\",\n            cfg.generation.max_new_tokens,\n            False,\n            cfg.generation.temperature,\n        )[0]\n        refine_prompt = (\n            \"Revise the answer using the feedback.\\nIMPORTANT: Put the final answer first, then you may add a brief explanation.\\n\"\n            f\"Problem: {x}\\nCurrent answer: {y_star}\\nFeedback: {fb}\\nRevised response:\"\n        )\n        cands = model.generate(refine_prompt, cfg.generation.max_new_tokens, True, cfg.generation.temperature, num_return_sequences=B)\n\n        n = [0] * B\n        s = [0.0] * B\n        proxy_first = [None] * B\n\n        evals = 0\n        for i in range(B):\n            if evals >= per_round_proxy_budget:\n                break\n            out = proxy_score_lenient(cands[i], gold, p_flip, rng)\n            proxy_first[i] = int(out)\n            n[i] += 1\n            s[i] += out\n            evals += 1\n            proxy_calls += 1\n        if evals < per_round_proxy_budget:\n            n_star += 1\n            sum_star += proxy_score_lenient(y_star, gold, p_flip, rng)\n            evals += 1\n            proxy_calls += 1\n\n        while evals < per_round_proxy_budget:\n            ucbs = []\n            for i in range(B):\n                mean_i = s[i] / max(1, n[i])\n                r = hoeffding_rad(n[i], delta / (B + 1))\n                ucbs.append(min(1.0, mean_i + r))\n            i = max(range(B), key=lambda j: ucbs[j])\n            n[i] += 1\n            s[i] += proxy_score_lenient(cands[i], gold, p_flip, rng)\n            evals += 1\n            proxy_calls += 1\n\n        means = [s[i] / max(1, n[i]) for i in range(B)]\n        audit_order = sorted(range(B), key=lambda i: means[i], reverse=True)\n        for k in range(min(per_round_audit_budget, B)):\n            i = audit_order[k]\n            p0 = proxy_first[i]\n            if p0 is None:\n                p0 = int(means[i] >= 0.5)\n            t_true = int(true_score_strict(cands[i], gold))\n            calibrator.update(p0, t_true)\n            audit_calls += 1\n\n        best_i, best_lcbU = 0, -1e9\n        for i in range(B):\n            lcbU, _ = calibrated_bounds_from_proxy(means[i], cands[i])\n            if lcbU > best_lcbU:\n                best_lcbU, best_i = lcbU, i\n\n        mean_star = sum_star / n_star\n        _, ucbU_star = calibrated_bounds_from_proxy(mean_star, y_star)\n\n        if best_lcbU > ucbU_star + tau:\n            y_star = cands[best_i]\n            n_star, sum_star = 1, proxy_score_lenient(y_star, gold, p_flip, rng)\n            proxy_calls += 1\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                return y_star, {\"proxy_calls\": proxy_calls, \"audit_calls\": audit_calls, \"rounds\": t + 1}\n\n    return y_star, {\"proxy_calls\": proxy_calls, \"audit_calls\": audit_calls, \"rounds\": T}\n",
    "main_py": "import os\nimport subprocess\nimport sys\nimport hydra\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg):\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.method_cfg.T = min(cfg.method_cfg.T, 1)\n        cfg.method_cfg.B = min(cfg.method_cfg.B, 2)\n        cfg.method_cfg.per_round_proxy_budget = min(cfg.method_cfg.per_round_proxy_budget, 8)\n        cfg.method_cfg.per_round_audit_budget = min(cfg.method_cfg.per_round_audit_budget, 1)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n\n    run_id = cfg.run.run_id\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"accr-experiments\"\nversion = \"0.1.0\"\ndescription = \"ACCR proxy calibration experiments\"\nrequires-python = \">=3.10\"\n\n[project.dependencies]\ntorch = \"*\"\ntransformers = \"*\"\ndatasets = \"*\"\nhydra-core = \"*\"\nwandb = \"*\"\noptuna = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\nscipy = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n",
    "config_yaml": "defaults:\n  - run: proposed-flan-t5-small-calc-svamp\n\nmode: full\nresults_dir: ./results\nseed: 0\n\nwandb:\n  entity: airas\n  project: 2026-02-05-1\n  mode: online\n\ntrial:\n  max_items: 2\n\nmethod_cfg:\n  T: 3\n  B: 4\n  delta: 0.05\n  patience: 1\n  lam_length_penalty: 0.01\n  tau_accept_margin: 0.02\n  per_round_proxy_budget: 12\n  per_round_audit_budget: 1\n  alpha_calibration: 0.05\n  p_flip: 0.1\n\ngeneration:\n  max_new_tokens: 96\n  temperature: 0.8\n\noptuna:\n  n_trials: 20\n  search_spaces: []\n",
    "run_configs": {
      "proposed-flan-t5-small-calc-svamp": "run_id: proposed-flan-t5-small-calc-svamp\nmethod: Audit-Calibrated Confidence Refinement (ACCR)\nmodel:\n  name: google/flan-t5-small\n  dtype: bf16\n  device_map: auto\ndataset:\n  name: MU-NLPC/Calc-svamp\n  subset: SVAMP\n  split: test\n  max_length: 512\n  preprocessing:\n    normalize_numbers: true\n    extract_first_number: true\ntraining:\n  learning_rate: 0.0\n  batch_size: 1\n  epochs: 0\n  optimizer: adamw\n  warmup_steps: 0\n  gradient_accumulation_steps: 1\n  inference_only: true\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: lam_length_penalty\n      distribution_type: uniform\n      low: 0.0\n      high: 0.03\n    - param_name: tau_accept_margin\n      distribution_type: uniform\n      low: 0.0\n      high: 0.15\n    - param_name: alpha_calibration\n      distribution_type: categorical\n      choices: [0.01, 0.05, 0.1]\n    - param_name: per_round_audit_budget\n      distribution_type: categorical\n      choices: [0, 1, 2]\n    - param_name: p_flip_dev\n      distribution_type: categorical\n      choices: [0.0, 0.1]\n",
      "comparative-1-flan-t5-small-calc-svamp": "run_id: comparative-1-flan-t5-small-calc-svamp\nmethod: SCG-SR(Proxy-only)\nmodel:\n  name: google/flan-t5-small\n  dtype: bf16\n  device_map: auto\ndataset:\n  name: MU-NLPC/Calc-svamp\n  subset: SVAMP\n  split: test\n  max_length: 512\n  preprocessing:\n    normalize_numbers: true\n    extract_first_number: true\ntraining:\n  learning_rate: 0.0\n  batch_size: 1\n  epochs: 0\n  optimizer: adamw\n  warmup_steps: 0\n  gradient_accumulation_steps: 1\n  inference_only: true\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: lam_length_penalty\n      distribution_type: uniform\n      low: 0.0\n      high: 0.03\n    - param_name: tau_accept_margin\n      distribution_type: uniform\n      low: 0.0\n      high: 0.15\n    - param_name: per_round_proxy_budget\n      distribution_type: categorical\n      choices: [8, 12, 16]\n    - param_name: p_flip_dev\n      distribution_type: categorical\n      choices: [0.0, 0.1]\n",
      "proposed-mistral-7b-v0.3-calc-svamp": "run_id: proposed-mistral-7b-v0.3-calc-svamp\nmethod: Audit-Calibrated Confidence Refinement (ACCR)\nmodel:\n  name: mistralai/Mistral-7B-v0.3\n  dtype: bf16\n  device_map: auto\ndataset:\n  name: MU-NLPC/Calc-svamp\n  subset: SVAMP\n  split: test\n  max_length: 512\n  preprocessing:\n    normalize_numbers: true\n    extract_first_number: true\ntraining:\n  learning_rate: 0.0\n  batch_size: 1\n  epochs: 0\n  optimizer: adamw\n  warmup_steps: 0\n  gradient_accumulation_steps: 1\n  inference_only: true\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: lam_length_penalty\n      distribution_type: uniform\n      low: 0.0\n      high: 0.03\n    - param_name: tau_accept_margin\n      distribution_type: uniform\n      low: 0.0\n      high: 0.15\n    - param_name: alpha_calibration\n      distribution_type: categorical\n      choices: [0.01, 0.05, 0.1]\n    - param_name: per_round_audit_budget\n      distribution_type: categorical\n      choices: [0, 1, 2]\n    - param_name: p_flip_dev\n      distribution_type: categorical\n      choices: [0.0, 0.1]\n",
      "comparative-1-mistral-7b-v0.3-calc-svamp": "run_id: comparative-1-mistral-7b-v0.3-calc-svamp\nmethod: SCG-SR(Proxy-only)\nmodel:\n  name: mistralai/Mistral-7B-v0.3\n  dtype: bf16\n  device_map: auto\ndataset:\n  name: MU-NLPC/Calc-svamp\n  subset: SVAMP\n  split: test\n  max_length: 512\n  preprocessing:\n    normalize_numbers: true\n    extract_first_number: true\ntraining:\n  learning_rate: 0.0\n  batch_size: 1\n  epochs: 0\n  optimizer: adamw\n  warmup_steps: 0\n  gradient_accumulation_steps: 1\n  inference_only: true\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: lam_length_penalty\n      distribution_type: uniform\n      low: 0.0\n      high: 0.03\n    - param_name: tau_accept_margin\n      distribution_type: uniform\n      low: 0.0\n      high: 0.15\n    - param_name: per_round_proxy_budget\n      distribution_type: categorical\n      choices: [8, 12, 16]\n    - param_name: p_flip_dev\n      distribution_type: categorical\n      choices: [0.0, 0.1]\n"
    }
  }
}